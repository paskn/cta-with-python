{
  "hash": "7c93b9f07b69abecdad6cdb55c91eb55",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Lab 02: Words as data points II\"\nsubtitle: \"Comparing corpora, lemmatization, and statistical significance\"\nformat:\n  html:\n    number-sections: true\n    fig-format: svg\n    toc: true\n    toc-depth: 4\ndate: now\ndate-format: \"YYYY-MM-DD HH:mm:ss\"\nexecute:\n  error: true\n---\n\n\n\n## Learning objectives\n\nBy the end of this lab, you will understand:\n\n- How to compare word usage across different groups or corpora\n- What lemmatization is and why it matters for text analysis\n- The difference between stemming and lemmatization\n- Why simple frequency comparisons can be misleading\n- How to measure statistical significance with log-likelihood (G²)\n- How to quantify effect size with log odds ratio\n- What named entities are and how to extract them\n- How to use spaCy for advanced NLP tasks in Python\n\n---\n\n## Introduction: Why compare corpora?\n\nIn social and political science, texts often serve as proxies for social phenomena, sentiments, ideas, or discourses. A common research design involves collecting texts from different institutions, groups, or actors to create **contrasting corpora**. By comparing word usage across these corpora, we can infer something about the underlying social or political features of the entities they represent.\n\n### The research question\n\nConsider this scenario: Do Democratic and Republican presidents talk differently? Not just in terms of political positions, but in the actual *words* they use?\n\nIn Lab 01, we compared authors based on *pre-selected* words (stop words, personal pronouns). This worked well for stylometry because function words are a closed class - we know all of them in advance.\n\nBut what about **content words**? If we want to compare the *substance* of what different groups talk about, how do we:\n\n1. Avoid arbitrary word selection?\n2. Distinguish meaningful differences from random variation?\n3. Quantify both the *significance* and *magnitude* of differences?\n\nThis is where corpus comparison methods come in.\n\n### Our approach today\n\nWe'll create two contrasting corpora:\n\n- **Corpus A**: State of the Union addresses by Democratic presidents (since 1917)\n- **Corpus B**: State of the Union addresses by Republican presidents (since 1917)\n\nThen we'll use statistical measures to identify which words are significantly over- or under-used in one corpus compared to the other.\n\n**Key insight**: We're not just looking for *different* words - we're looking for *statistically significant* differences that reveal meaningful patterns.\n\n---\n\n## Setup: Loading packages\n\n::: {#2722dd5a .cell execution_count=1}\n``` {.python .cell-code}\n# Data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Text processing\nimport spacy\nfrom collections import Counter\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Statistical functions\nfrom scipy.stats import chi2\n\n# Set visualization style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\nprint(\"✓ Packages loaded successfully\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n✓ Packages loaded successfully\n```\n:::\n:::\n\n\n::: {.callout-note}\n## About these packages\n\nNew packages in this lab:\n\n- **numpy**: Numerical computing (for log calculations)\n- **scipy.stats**: Statistical functions (for significance testing)\n\nWe'll continue using pandas, spaCy, and visualization libraries from Lab 01.\n:::\n\n### Loading spaCy model\n\n::: {#a6acd272 .cell execution_count=2}\n``` {.python .cell-code}\n# Load English language model\nnlp = spacy.load(\"en_core_web_sm\")\n\nnlp.max_length = 1530000        # https://github.com/explosion/spaCy/issues/13207#issuecomment-1865973378\n\nprint(f\"✓ spaCy model loaded: {nlp.meta['name']}\")\nprint(f\"  Language: {nlp.meta['lang']}\")\nprint(f\"  Components: {nlp.pipe_names}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n✓ spaCy model loaded: core_web_sm\n  Language: en\n  Components: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n```\n:::\n:::\n\n\n---\n\n## Loading and preparing the data\n\nLet's load our State of the Union dataset:\n\n::: {#945359a9 .cell execution_count=3}\n``` {.python .cell-code}\n# Load the data\nspeeches = pd.read_csv(\"data/transcripts.csv\")\n\nprint(f\"Total speeches: {len(speeches)}\")\nprint(f\"Date range: {speeches['date'].min()} to {speeches['date'].max()}\")\nprint(f\"\\nFirst few rows:\")\nspeeches.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal speeches: 244\nDate range: 1790-01-08 to 2018-01-30\n\nFirst few rows:\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>president</th>\n      <th>title</th>\n      <th>url</th>\n      <th>transcript</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-01-30</td>\n      <td>Donald J. Trump</td>\n      <td>Address Before a Joint Session of the Congress...</td>\n      <td>https://www.cnn.com/2018/01/30/politics/2018-s...</td>\n      <td>\\nMr. Speaker, Mr. Vice President, Members of ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-02-28</td>\n      <td>Donald J. Trump</td>\n      <td>Address Before a Joint Session of the Congress</td>\n      <td>http://www.presidency.ucsb.edu/ws/index.php?pi...</td>\n      <td>Thank you very much. Mr. Speaker, Mr. Vice Pre...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2016-01-12</td>\n      <td>Barack Obama</td>\n      <td>Address Before a Joint Session of the Congress...</td>\n      <td>http://www.presidency.ucsb.edu/ws/index.php?pi...</td>\n      <td>Thank you. Mr. Speaker, Mr. Vice President, Me...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2015-01-20</td>\n      <td>Barack Obama</td>\n      <td>Address Before a Joint Session of the Congress...</td>\n      <td>http://www.presidency.ucsb.edu/ws/index.php?pi...</td>\n      <td>The President. Mr. Speaker, Mr. Vice President...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2014-01-28</td>\n      <td>Barack Obama</td>\n      <td>Address Before a Joint Session of the Congress...</td>\n      <td>http://www.presidency.ucsb.edu/ws/index.php?pi...</td>\n      <td>The President. Mr. Speaker, Mr. Vice President...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Creating contrasting corpora\n\nWe'll split speeches by party affiliation. First, let's define which presidents belong to which party (since 1917):\n\n::: {#26e08712 .cell execution_count=4}\n``` {.python .cell-code}\n# Democratic presidents since 1917\ndemocrats = [\n    \"Woodrow Wilson\", \n    \"Franklin D. Roosevelt\", \n    \"Harry S. Truman\", \n    \"John F. Kennedy\", \n    \"Lyndon B. Johnson\", \n    \"Jimmy Carter\",\n    \"William J. Clinton\", \n    \"Barack Obama\"\n]\n\n# Filter speeches\nspeeches_after_1917 = speeches[speeches['date'] > '1917-10-25'].copy()\n\n# Create party labels\nspeeches_after_1917['party'] = speeches_after_1917['president'].apply(\n    lambda x: 'Democrat' if x in democrats else 'Republican'\n)\n\n# Split into two corpora\ndem_speeches = speeches_after_1917[speeches_after_1917['party'] == 'Democrat']\nrep_speeches = speeches_after_1917[speeches_after_1917['party'] == 'Republican']\n\nprint(\"Democratic presidents:\")\nprint(dem_speeches['president'].value_counts())\nprint(f\"\\nTotal Democratic speeches: {len(dem_speeches)}\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"\\nRepublican presidents:\")\nprint(rep_speeches['president'].value_counts())\nprint(f\"\\nTotal Republican speeches: {len(rep_speeches)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDemocratic presidents:\npresident\nFranklin D. Roosevelt    13\nBarack Obama              8\nHarry S. Truman           8\nWilliam J. Clinton        8\nJimmy Carter              7\nLyndon B. Johnson         6\nWoodrow Wilson            4\nJohn F. Kennedy           3\nName: count, dtype: int64\n\nTotal Democratic speeches: 57\n\n==================================================\n\nRepublican presidents:\npresident\nRichard Nixon           12\nDwight D. Eisenhower    10\nRonald Reagan            8\nGeorge W. Bush           8\nCalvin Coolidge          6\nHerbert Hoover           4\nGeorge Bush              4\nGerald R. Ford           3\nDonald J. Trump          2\nWarren G. Harding        2\nName: count, dtype: int64\n\nTotal Republican speeches: 59\n```\n:::\n:::\n\n\n::: {.callout-tip}\n## Why start in 1917?\n\nWe chose 1917 as a cutoff to:\n\n- Ensure both parties have substantial representation\n- Focus on relatively modern political language\n- Avoid complications from 19th century political realignments\n\nIn your own research, such choices should be **explicit** and **justified**.\n:::\n\n### Combining texts by party\n\nFor corpus comparison, we'll combine all speeches from each party into two large text collections:\n\n::: {#70ed1cb9 .cell execution_count=5}\n``` {.python .cell-code}\n# Combine all speeches by party\ndem_corpus = \" \".join(dem_speeches['transcript'].tolist())\nrep_corpus = \" \".join(rep_speeches['transcript'].tolist())\n\nprint(f\"Democratic corpus: {len(dem_corpus):,} characters\")\nprint(f\"Republican corpus: {len(rep_corpus):,} characters\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDemocratic corpus: 4,874,617 characters\nRepublican corpus: 4,176,938 characters\n```\n:::\n:::\n\n\n---\n\n## From wordforms to lemmas: Introduction to lemmatization\n\n### The problem with raw words\n\nConsider these sentences:\n\n- \"The government **regulates** industry.\"\n- \"These **regulations** affect small businesses.\"\n- \"The **regulatory** framework is complex.\"\n\nThese three words - *regulates*, *regulations*, *regulatory* - are clearly related. They share the same root concept of \"regulation.\" But if we count them separately, we miss this connection.\n\nThis problem is especially acute in languages with rich inflection (Russian, German, Finnish), but it exists in English too:\n\n- Verbs: walk, walks, walked, walking\n- Nouns: cat, cats, mouse, mice\n- Adjectives: big, bigger, biggest\n\n### Two solutions: Stemming vs lemmatization\n\n**Stemming**: Crudely chop off word endings\n\n- running → run\n- better → bet (⚠️ wrong!)\n- organization → organ (⚠️ wrong!)\n- Fast but imprecise\n\n**Lemmatization**: Reduce words to their dictionary form (*lemma*)\n\n- running → run\n- better → good\n- mice → mouse\n- Slower but accurate\n\n### How lemmatization works\n\nLemmatization requires:\n\n1. **Part-of-speech information**: Is \"running\" a verb or a noun?\n2. **Morphological dictionary**: What are all the forms of \"run\"?\n3. **Linguistic rules**: How do irregular forms work?\n\nFortunately, spaCy does all this for us!\n\n### Lemmatization with spaCy\n\nLet's see lemmatization in action:\n\n::: {#3de60c2c .cell execution_count=6}\n``` {.python .cell-code}\n# Example text\nexample = \"The regulations are regulating industries more effectively than previous regulatory frameworks.\"\n\n# Process with spaCy\ndoc = nlp(example)\n\n# Show original word, lemma, and part of speech\nprint(\"Word → Lemma (Part of Speech)\")\nprint(\"-\" * 40)\nfor token in doc:\n    if not token.is_punct:\n        print(f\"{token.text:15} → {token.lemma_:15} ({token.pos_})\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWord → Lemma (Part of Speech)\n----------------------------------------\nThe             → the             (DET)\nregulations     → regulation      (NOUN)\nare             → be              (AUX)\nregulating      → regulate        (VERB)\nindustries      → industry        (NOUN)\nmore            → more            (ADV)\neffectively     → effectively     (ADV)\nthan            → than            (ADP)\nprevious        → previous        (ADJ)\nregulatory      → regulatory      (ADJ)\nframeworks      → framework       (NOUN)\n```\n:::\n:::\n\n\nNotice how:\n\n- \"regulations\" → \"regulation\"\n- \"are regulating\" → \"be regulate\" (splits auxiliary verb)\n- \"regulatory\" → \"regulatory\" (already base form)\n\n::: {.callout-important}\n## When to lemmatize?\n\n**Use lemmatization when:**\n\n- Comparing content across documents\n- Building topic models\n- Working with inflected languages\n- Vocabulary is too large\n\n**Don't lemmatize when:**\n\n- Doing stylometry (exact forms matter)\n- Studying syntax or grammar\n- Tense/number/person is important\n- Training neural language models\n:::\n\n---\n\n## Processing our corpora with spaCy\n\nNow let's lemmatize both of our political corpora. This will take a few minutes as spaCy processes all the text.\n\n::: {.callout-note}\n## Processing time\n\nProcessing large texts with spaCy is computationally intensive. For very large corpora (millions of words), you might want to:\n\n- Use spaCy's `nlp.pipe()` for batch processing\n- Disable unnecessary components (`nlp.disable_pipes()`)\n- Save processed results to disk\n\nFor this lab, the processing should take 2-5 minutes.\n:::\n\n::: {#954a7ab5 .cell execution_count=7}\n``` {.python .cell-code}\n# Process Democratic speeches (this takes time!)\nprint(\"Processing Democratic speeches...\")\ndem_doc = nlp(dem_corpus)\nprint(\"✓ Democratic corpus processed\")\n\n# Process Republican speeches\nprint(\"Processing Republican speeches...\")\nrep_doc = nlp(rep_corpus)\nprint(\"✓ Republican corpus processed\")\n```\n:::\n\n\nFor the purposes of this lab, let's work with a sample to speed things up:\n\n::: {#208a2e5f .cell execution_count=8}\n``` {.python .cell-code}\n# Take a sample of each corpus for faster processing\ndem_sample = \" \".join(dem_speeches.sample(n=min(20, len(dem_speeches)), random_state=42)['transcript'].tolist())\nrep_sample = \" \".join(rep_speeches.sample(n=min(20, len(rep_speeches)), random_state=42)['transcript'].tolist())\n\n# Process samples\nprint(\"Processing samples...\")\ndem_doc = nlp(dem_sample)\nrep_doc = nlp(rep_sample)\nprint(\"✓ Processing complete\")\n\nprint(f\"\\nDemocratic sample: {len(dem_doc)} tokens\")\nprint(f\"Republican sample: {len(rep_doc)} tokens\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProcessing samples...\n✓ Processing complete\n\nDemocratic sample: 295013 tokens\nRepublican sample: 267382 tokens\n```\n:::\n:::\n\n\n### Extracting lemmas and filtering\n\nWe want to keep only content-bearing words. Let's filter out:\n\n- Punctuation (`.`, `,`, `!`, etc.)\n- Numbers (`1`, `2020`, `million`)\n- Symbols (`$`, `%`, `@`)\n- Proper nouns (specific names of people and places)\n- Stop words (optional - let's keep them for now to see what happens)\n\n::: {#7654376d .cell execution_count=9}\n``` {.python .cell-code}\n# Extract lemmas from Democratic speeches\ndem_lemmas = []\nfor token in dem_doc:\n    if not token.is_punct and not token.is_space and token.pos_ not in ['NUM', 'SYM', 'PROPN']:\n        dem_lemmas.append({\n            'lemma': token.lemma_.lower(),\n            'pos': token.pos_,\n            'party': 'Democrat'\n        })\n\n# Extract lemmas from Republican speeches\nrep_lemmas = []\nfor token in rep_doc:\n    if not token.is_punct and not token.is_space and token.pos_ not in ['NUM', 'SYM', 'PROPN']:\n        rep_lemmas.append({\n            'lemma': token.lemma_.lower(),\n            'pos': token.pos_,\n            'party': 'Republican'\n        })\n\n# Combine into DataFrames\ndem_df = pd.DataFrame(dem_lemmas)\nrep_df = pd.DataFrame(rep_lemmas)\n\nprint(f\"Democratic lemmas: {len(dem_df):,}\")\nprint(f\"Republican lemmas: {len(rep_df):,}\")\nprint(f\"\\nSample of Democratic lemmas:\")\nprint(dem_df.head(10))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDemocratic lemmas: 247,919\nRepublican lemmas: 225,161\n\nSample of Democratic lemmas:\n     lemma   pos     party\n0    thank  VERB  Democrat\n1      you  PRON  Democrat\n2       of   ADP  Democrat\n3       my  PRON  Democrat\n4   fellow   ADJ  Democrat\n5  tonight  NOUN  Democrat\n6     mark  VERB  Democrat\n7      the   DET  Democrat\n8   eighth   ADJ  Democrat\n9     year  NOUN  Democrat\n```\n:::\n:::\n\n\n### Creating frequency tables\n\nNow let's count how often each lemma appears in each corpus:\n\n::: {#2f074465 .cell execution_count=10}\n``` {.python .cell-code}\n# Count frequencies by party\ndem_counts = dem_df['lemma'].value_counts().reset_index()\ndem_counts.columns = ['lemma', 'democrat']\n\nrep_counts = rep_df['lemma'].value_counts().reset_index()\nrep_counts.columns = ['lemma', 'republican']\n\n# Merge into one table\nfreq_table = dem_counts.merge(rep_counts, on='lemma', how='outer').fillna(0)\n\n# Convert to integers\nfreq_table['democrat'] = freq_table['democrat'].astype(int)\nfreq_table['republican'] = freq_table['republican'].astype(int)\n\n# Filter out very rare words (appears less than 10 times in both corpora)\nfreq_table = freq_table[(freq_table['democrat'] > 10) | (freq_table['republican'] > 10)].copy()\n\nprint(f\"Unique lemmas (after filtering): {len(freq_table):,}\")\nprint(f\"\\nTop 20 by total frequency:\")\nfreq_table['total'] = freq_table['democrat'] + freq_table['republican']\nprint(freq_table.sort_values('total', ascending=False).head(20))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnique lemmas (after filtering): 2,072\n\nTop 20 by total frequency:\n     lemma  democrat  republican  total\n6817   the     14722       14995  29717\n4617    of      8965        9654  18619\n395    and      9610        8446  18056\n6915    to      9536        7944  17480\n708     be      8286        8421  16707\n3462    in      5361        5390  10751\n7507    we      5757        4214   9971\n94       a      4410        4106   8516\n4700   our      4423        3595   8018\n6816  that      3850        2843   6693\n3187  have      3386        3183   6569\n2805   for      2936        2765   5701\n3364     i      2742        1921   4663\n7583  will      2390        1964   4354\n3797    it      1995        1740   3735\n6849  this      1952        1738   3690\n4548   not      1780        1372   3152\n7604  with      1626        1383   3009\n4640    on      1419        1445   2864\n6842  they      1658        1065   2723\n```\n:::\n:::\n\n\n---\n\n## The problem with simple frequency comparisons\n\nLooking at raw frequencies is tempting, but it can be misleading. Let's see why.\n\n### Corpus size matters\n\n::: {#18e74124 .cell execution_count=11}\n``` {.python .cell-code}\n# Total tokens per party\ntotal_dem = freq_table['democrat'].sum()\ntotal_rep = freq_table['republican'].sum()\n\nprint(f\"Total Democratic tokens: {total_dem:,}\")\nprint(f\"Total Republican tokens: {total_rep:,}\")\nprint(f\"Ratio (Dem/Rep): {total_dem/total_rep:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal Democratic tokens: 234,922\nTotal Republican tokens: 212,286\nRatio (Dem/Rep): 1.11\n```\n:::\n:::\n\n\nIf one corpus is larger, it will naturally have higher raw counts for most words. We need to account for this.\n\n### Example: The word \"people\"\n\n::: {#5d566fcb .cell execution_count=12}\n``` {.python .cell-code}\n# Look at a specific word\npeople_row = freq_table[freq_table['lemma'] == 'people']\n\nif len(people_row) > 0:\n    dem_count = people_row['democrat'].values[0]\n    rep_count = people_row['republican'].values[0]\n    \n    # Raw counts\n    print(f\"Raw counts for 'people':\")\n    print(f\"  Democrats: {dem_count}\")\n    print(f\"  Republicans: {rep_count}\")\n    print(f\"  Difference: {dem_count - rep_count}\")\n    \n    # Normalized (per 1000 words)\n    dem_rate = (dem_count / total_dem) * 1000\n    rep_rate = (rep_count / total_rep) * 1000\n    \n    print(f\"\\nNormalized rates (per 1,000 words):\")\n    print(f\"  Democrats: {dem_rate:.2f}\")\n    print(f\"  Republicans: {rep_rate:.2f}\")\n    print(f\"  Difference: {dem_rate - rep_rate:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRaw counts for 'people':\n  Democrats: 972\n  Republicans: 687\n  Difference: 285\n\nNormalized rates (per 1,000 words):\n  Democrats: 4.14\n  Republicans: 3.24\n  Difference: 0.90\n```\n:::\n:::\n\n\nThe raw difference might be large just because one corpus is bigger!\n\n### Two questions we need to answer\n\n1. **Is the difference statistically significant?**\n   - Could this difference occur by chance?\n   - How confident can we be that it's a real pattern?\n   - → We'll use **log-likelihood (G²)** for this\n\n2. **How large is the effect?**\n   - Is it a huge difference or a tiny one?\n   - Which words show the strongest contrast?\n   - → We'll use **log odds ratio** for this\n\n::: {.callout-note}\n## Statistical significance ≠ practical importance\n\nA difference can be:\n\n- **Statistically significant** but **tiny** (large sample)\n- **Large** but **not significant** (small sample)\n\nWe need *both* measures to draw meaningful conclusions.\n:::\n\n---\n\n## Measuring significance: Log-likelihood (G²)\n\n### The problem: When is a difference real?\n\nLet's say we're comparing Republican and Democratic speeches, and we find that the word \"freedom\" appears:\n\n- 100 times in Republican speeches\n- 50 times in Democratic speeches\n\nShould we conclude that Republicans talk twice as much about freedom?\n\nNot necessarily. Here's why: What if the Republican corpus contains 1,000,000 words total, while the Democratic corpus contains 500,000 words? Then both parties use \"freedom\" at exactly the same rate (100 per million words). The difference in raw counts is simply because we have more Republican text.\n\nThis is why we need a statistical test that accounts for corpus size.\n\n### What is log-likelihood (G²)?\n\nLog-likelihood, abbreviated as **G²**, is a statistical test that answers one simple question:\n\n> *\"Given the sizes of my two corpora, how surprising is this word's distribution?\"*\n\n**The logic:**\n\n- If a word is distributed just as we'd expect (proportional to corpus size), G² is close to 0\n- If the distribution is very different from what we'd expect, G² is large\n- The larger G², the more confident we can be that the difference is real, not just random variation\n\nThink of G² as a \"surprise meter\" - it measures how surprised we should be by what we observe.\n\n### How to read G² values\n\nG² follows a well-known statistical distribution, which means we have standard thresholds for interpretation:\n\n| G² value | Confidence level | What it means |\n|----------|------------------|---------------|\n| < 3.84   | Not significant  | Difference might be random chance |\n| > 3.84   | 95% confident    | Probably a real pattern (p < 0.05) |\n| > 6.63   | 99% confident    | Very likely a real pattern (p < 0.01) |\n| > 10.83  | 99.9% confident  | Almost certainly a real pattern (p < 0.001) |\n\n**Rule of thumb**: We typically use **G² > 6.63** as our cutoff for trusting a difference.\n\n::: {.callout-note}\n## What does \"99% confident\" mean?\n\nIt means: \"If there were actually no real difference, we'd see a result this extreme less than 1% of the time.\" In other words, we're very confident the pattern is real, not just luck.\n:::\n\n::: {.callout-note collapse=\"true\"}\n## For the mathematically curious: How G² is calculated\n\nG² compares observed frequencies (what we actually see) to expected frequencies (what we'd see if words were distributed proportionally to corpus size).\n\nThe formula is:\n\n$$G^2 = 2 \\sum O \\times \\ln\\left(\\frac{O}{E}\\right)$$\n\nWhere:\n\n- $O$ = observed frequency\n- $E$ = expected frequency\n- $\\ln$ = natural logarithm\n\nFor two corpora, this expands to:\n\n$$G^2 = 2 \\times \\left[ a \\times \\ln\\left(\\frac{a}{E_1}\\right) + b \\times \\ln\\left(\\frac{b}{E_2}\\right) \\right]$$\n\nWhere:\n\n- $a$ = word count in Corpus A\n- $b$ = word count in Corpus B\n- $E_1$ = expected count in Corpus A\n- $E_2$ = expected count in Corpus B\n\nThe expected frequencies account for corpus size:\n\n$$E_1 = C \\times \\frac{a + b}{C + D}$$\n$$E_2 = D \\times \\frac{a + b}{C + D}$$\n\nWhere:\n\n- $C$ = total size of Corpus A\n- $D$ = total size of Corpus B\n\nThis test is based on Dunning (1993), a foundational paper in corpus linguistics. It's preferred over chi-squared for text data because it handles sparse data (rare words) more reliably.\n:::\n\n### Calculating G² in Python\n\nWe'll create a function that does all the mathematical work for us:\n\n::: {#6dd32828 .cell execution_count=13}\n``` {.python .cell-code}\ndef log_likelihood(a, b):\n    \"\"\"\n    Calculate log-likelihood (G²) for word frequencies in two corpora.\n\n    This function compares observed word frequencies to expected frequencies\n    (based on corpus size) and returns a G² value indicating how surprising\n    the observed distribution is.\n\n    Parameters:\n    -----------\n    a : array-like\n        Word counts in corpus A (e.g., Democratic speeches)\n    b : array-like\n        Word counts in corpus B (e.g., Republican speeches)\n\n    Returns:\n    --------\n    array-like\n        G² values for each word (higher = more surprising/significant)\n    \"\"\"\n    # Total corpus sizes\n    C = np.sum(a)  # Total tokens in corpus A\n    D = np.sum(b)  # Total tokens in corpus B\n\n    # Calculate expected frequencies (what we'd expect if words were distributed proportionally)\n    E1 = C * ((a + b) / (C + D))\n    E2 = D * ((a + b) / (C + D))\n\n    # Calculate G² statistic\n    # Note: We add a tiny constant (1e-10) to avoid mathematical errors when counts are zero\n    g2 = 2 * ((a * np.log(a / E1 + 1e-10)) + (b * np.log(b / E2 + 1e-10)))\n\n    return g2\n```\n:::\n\n\n### Using G² to find significant differences\n\n::: {#8730a6a4 .cell execution_count=14}\n``` {.python .cell-code}\n# Calculate log-likelihood for all words\nfreq_table['g2'] = log_likelihood(\n    freq_table['democrat'].values, \n    freq_table['republican'].values\n)\n\n# Sort by G² (most significant differences)\nfreq_table_sorted = freq_table.sort_values('g2', ascending=False).copy()\n\nprint(\"Words with highest G² (most significant differences):\")\nprint(freq_table_sorted[['lemma', 'democrat', 'republican', 'g2']].head(20))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWords with highest G² (most significant differences):\n            lemma  democrat  republican          g2\n2095           do      1380         612  231.203248\n2986          get       421         122  145.273635\n4617           of      8965        9654  143.074502\n7675          you       941         443  136.369324\n7625         work      1034         505  135.944566\n7507           we      5757        4214  108.982754\n5829        right       503         198  108.139684\n6817          the     14722       14995  106.400966\n7343           us       174          28  103.171606\n1226      college       174          29  100.740012\n3123          gun        78           0  100.428162\n3813          job       544         232   99.441736\n1061         cent         6          86   91.520368\n5163      present        86         238   90.417812\n7555        which       805        1117   87.414119\n7560          who       837         443   86.633621\n1278      company       120          14   85.637924\n4673           or       930         514   83.115776\n2546  expenditure        28         130   82.156755\n6803    terrorist        42         156   81.910170\n```\n:::\n:::\n\n\nLook at the G² values. Many are well above 6.63, meaning we can be very confident these differences are real.\n\n### How many significant differences did we find?\n\nLet's count how many words show statistically significant differences at different confidence levels:\n\n::: {#e074f198 .cell execution_count=15}\n``` {.python .cell-code}\n# Count significant differences\nsig_05 = (freq_table['g2'] > 3.84).sum()\nsig_01 = (freq_table['g2'] > 6.63).sum()\nsig_001 = (freq_table['g2'] > 10.83).sum()\n\nprint(f\"Significant differences:\")\nprint(f\"  95% confident (G² > 3.84):   {sig_05} words\")\nprint(f\"  99% confident (G² > 6.63):   {sig_01} words\")\nprint(f\"  99.9% confident (G² > 10.83): {sig_001} words\")\nprint(f\"\\nTotal words tested: {len(freq_table)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSignificant differences:\n  95% confident (G² > 3.84):   994 words\n  99% confident (G² > 6.63):   737 words\n  99.9% confident (G² > 10.83): 487 words\n\nTotal words tested: 2072\n```\n:::\n:::\n\n\nSo we have hundreds of words with statistically significant differences. But are they all interesting?\n\n### The problem: Stop words dominate\n\nNot all statistically significant differences are interesting. Let's check what kinds of words have the highest G² values:\n\n::: {#075dc515 .cell execution_count=16}\n``` {.python .cell-code}\n# Load stop words from spaCy\nstop_words = nlp.Defaults.stop_words\n\n# Check if top G² words are stop words\nfreq_table_sorted['is_stopword'] = freq_table_sorted['lemma'].isin(stop_words)\n\nprint(\"Top 20 by G² - are they stop words?\")\nprint(freq_table_sorted[['lemma', 'g2', 'is_stopword']].head(20))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTop 20 by G² - are they stop words?\n            lemma          g2  is_stopword\n2095           do  231.203248         True\n2986          get  145.273635         True\n4617           of  143.074502         True\n7675          you  136.369324         True\n7625         work  135.944566        False\n7507           we  108.982754         True\n5829        right  108.139684        False\n6817          the  106.400966         True\n7343           us  103.171606         True\n1226      college  100.740012        False\n3123          gun  100.428162        False\n3813          job   99.441736        False\n1061         cent   91.520368        False\n5163      present   90.417812        False\n7555        which   87.414119         True\n7560          who   86.633621         True\n1278      company   85.637924        False\n4673           or   83.115776         True\n2546  expenditure   82.156755        False\n6803    terrorist   81.910170        False\n```\n:::\n:::\n\n\nNotice that many high-G² words are stop words (words like \"the\", \"and\", \"of\").\n\n**Why does this happen?**\n\n- Stop words appear thousands of times in our corpora\n- G² is sensitive to absolute frequencies - when a word appears 5,000 times, even a small proportional difference produces high G²\n- A word that's 51% vs 49% between corpora can have higher G² than a word that's 90% vs 10%, just because the first word is more common overall\n\n**The solution**: Filter to focus on content words (nouns, verbs, adjectives) by removing stop words.\n\n::: {#a4c45f7b .cell execution_count=17}\n``` {.python .cell-code}\n# Focus on content words by removing stop words\ncontent_words = freq_table_sorted[~freq_table_sorted['is_stopword']].copy()\n\nprint(\"Top 20 content words by G²:\")\nprint(content_words[['lemma', 'democrat', 'republican', 'g2']].head(20))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTop 20 content words by G²:\n            lemma  democrat  republican          g2\n7625         work      1034         505  135.944566\n5829        right       503         198  108.139684\n1226      college       174          29  100.740012\n3123          gun        78           0  100.428162\n3813          job       544         232   99.441736\n1061         cent         6          86   91.520368\n5163      present        86         238   90.417812\n1278      company       120          14   85.637924\n2546  expenditure        28         130   82.156755\n6803    terrorist        42         156   81.910170\n7477         want       326         119   80.271861\n3164         hard       196          51   76.786385\n1582        court        18         103   74.890387\n3866         know       423         186   72.262874\n5282     property         6          67   66.090068\n776           big       120          25   58.448358\n6891         tile         0          38   56.626794\n5806      revenue        22          94   55.715059\n4211      measure        66         166   55.275584\n1830    dependent         2          46   54.495605\n```\n:::\n:::\n\n\nMuch better! Now we're seeing substantive words about policy, governance, and political issues.\n\n### What G² doesn't tell us\n\nG² tells us **that** a difference exists and how confident we can be about it. But it doesn't tell us:\n\n1. **Which corpus** uses the word more\n2. **How much more** it's used\n\nFor that, we need another measure: log odds ratio.\n\n---\n\n## Measuring effect size: Log odds ratio\n\n### The problem: G² doesn't tell us everything\n\nLook back at the content words with high G² values. Can you quickly tell which party uses each word more? Is \"health\" more Democratic or Republican? What about \"security\"?\n\nG² told us that differences exist and that they're statistically significant. But it doesn't tell us:\n\n1. **Direction**: Which corpus uses the word more?\n2. **Magnitude**: Is it slightly more common, or dramatically more common?\n\nFor this, we need a different measure: **log odds ratio**.\n\n### What is log odds ratio?\n\nLog odds ratio is a measure of **effect size** that answers:\n\n> *\"How much more is this word used in one corpus compared to the other?\"*\n\nIt gives us two pieces of information:\n\n- **The sign** (+ or -) tells us which corpus uses the word more\n- **The number** tells us how much more it's used\n\n### How to read log odds values\n\nIn our analysis, we calculate log odds where:\n\n- **Positive values** = word is more common in Democratic speeches\n- **Negative values** = word is more common in Republican speeches\n- **Zero** = word is equally common in both\n\nThe magnitude tells us how big the difference is:\n\n| Log Odds | Meaning |\n|----------|---------|\n| +1.0     | Word is 2× more common in Democratic speeches |\n| +2.0     | Word is 4× more common in Democratic speeches |\n| +3.0     | Word is 8× more common in Democratic speeches |\n| -1.0     | Word is 2× more common in Republican speeches |\n| -2.0     | Word is 4× more common in Republican speeches |\n| 0.0      | Word is equally common in both |\n\n### A concrete example\n\nLet's say the word \"healthcare\" appears:\n\n- 200 times in Democratic speeches (out of 100,000 total Democratic words)\n- 50 times in Republican speeches (out of 100,000 total Republican words)\n\nThe proportions are:\n\n- Democratic: 200/100,000 = 0.002 (0.2%)\n- Republican: 50/100,000 = 0.0005 (0.05%)\n\nThe ratio is 0.002/0.0005 = 4.0 (Democrats use it 4× more often).\n\nThe log₂(4.0) = 2.0\n\nSo this word would have a **log odds ratio of +2.0**, meaning Democrats use it 4× more than Republicans.\n\n::: {.callout-note collapse=\"true\"}\n## Why use logarithm?\n\nRaw ratios are asymmetric and hard to interpret:\n\n- \"2× more common\" = ratio of 2.0\n- \"2× less common\" = ratio of 0.5\n\nThese don't look symmetric even though they represent the same magnitude of difference.\n\nTaking the logarithm makes them symmetric:\n\n- 2× more common: log₂(2.0) = +1.0\n- 2× less common: log₂(0.5) = -1.0\n\nWe use **base-2 logarithm** (log₂) because it's easy to interpret:\n\n- Each +1 means \"doubled\"\n- Each -1 means \"halved\"\n\nThis makes effect sizes comparable across different words.\n:::\n\n::: {.callout-note collapse=\"true\"}\n## The mathematical formula\n\nLog odds ratio is calculated as:\n\n$$\\text{Log Odds Ratio} = \\log_2\\left(\\frac{a/C}{b/D}\\right)$$\n\nWhere:\n\n- $a$ = word count in Corpus A (Democrats)\n- $b$ = word count in Corpus B (Republicans)\n- $C$ = total size of Corpus A\n- $D$ = total size of Corpus B\n\nThis simplifies to comparing the proportions (a/C vs b/D) of how often each corpus uses the word.\n:::\n\n### Calculating log odds ratio in Python\n\nLet's create a function to calculate log odds ratio for all our words:\n\n::: {#68318a34 .cell execution_count=18}\n``` {.python .cell-code}\ndef log_odds_ratio(a, b):\n    \"\"\"\n    Calculate log odds ratio for word frequencies in two corpora.\n\n    This function compares how often words appear in each corpus\n    (accounting for corpus size) and returns a number telling us\n    which corpus uses each word more and by how much.\n\n    Positive values = more common in corpus A (Democrats)\n    Negative values = more common in corpus B (Republicans)\n    Magnitude = how much more (1 = 2×, 2 = 4×, 3 = 8×, etc.)\n\n    Parameters:\n    -----------\n    a : array-like\n        Word counts in corpus A (e.g., Democratic speeches)\n    b : array-like\n        Word counts in corpus B (e.g., Republican speeches)\n\n    Returns:\n    --------\n    array-like\n        Log odds ratios (base 2) for each word\n    \"\"\"\n    # Total corpus sizes\n    C = np.sum(a)  # Total words in corpus A\n    D = np.sum(b)  # Total words in corpus B\n\n    # Calculate proportions (what percentage of each corpus is this word?)\n    prop_a = a / C\n    prop_b = b / D\n\n    # Calculate log odds ratio\n    # Note: We add a tiny constant (1e-10) to avoid mathematical errors when counts are zero\n    lor = np.log2((prop_a + 1e-10) / (prop_b + 1e-10))\n\n    return lor\n```\n:::\n\n\n### Using log odds ratio to see which party uses each word\n\n::: {#76d2fcb0 .cell execution_count=19}\n``` {.python .cell-code}\n# Calculate log odds ratio\nfreq_table['log_odds'] = log_odds_ratio(\n    freq_table['democrat'].values,\n    freq_table['republican'].values\n)\n\n# Add to our content words table too\ncontent_words['log_odds'] = log_odds_ratio(\n    content_words['democrat'].values,\n    content_words['republican'].values\n)\n\nprint(\"Words most strongly associated with Democrats (positive log odds):\")\nprint(content_words.nlargest(15, 'log_odds')[['lemma', 'democrat', 'republican', 'log_odds', 'g2']])\n\nprint(\"\\nWords most strongly associated with Republicans (negative log odds):\")\nprint(content_words.nsmallest(15, 'log_odds')[['lemma', 'democrat', 'republican', 'log_odds', 'g2']])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWords most strongly associated with Democrats (positive log odds):\n          lemma  democrat  republican   log_odds          g2\n3123        gun        78           0  23.022742  100.428162\n4042   lobbyist        32           0  21.737340   41.201297\n3706   internet        30           0  21.644231   38.626216\n2085  diversity        28           0  21.544695   36.051135\n6756       tech        24           0  21.322303   30.900973\n6152     shrink        24           0  21.322303   30.900973\n1397   conquest        22           0  21.196772   28.325892\n4817  paperwork        22           0  21.196772   28.325892\n4238     mental        20           0  21.059268   25.750811\n7081         tv        20           0  21.059268   25.750811\n6763       teen        18           0  20.907265   23.175730\n91         96th        18           0  20.907265   23.175730\n6734       tank        18           0  20.907265   23.175730\n5617   reinvent        18           0  20.907265   23.175730\n2887     french        18           0  20.907265   23.175730\n\nWords most strongly associated with Republicans (negative log odds):\n            lemma  democrat  republican   log_odds         g2\n6891         tile         0          38 -22.100187  56.626794\n53           11th         0          28 -21.659614  41.725006\n3090        gross         0          26 -21.552699  38.744648\n7036     tribunal         0          22 -21.311691  32.783933\n3517        index         0          20 -21.174187  29.803576\n244      advisory         0          20 -21.174187  29.803576\n5256  prohibition         0          19 -21.100187  28.313397\n3028     governor         0          16 -20.852259  23.842860\n6279       solely         0          16 -20.852259  23.842860\n6808      testing         0          16 -20.852259  23.842860\n5564        refer         0          16 -20.852259  23.842860\n5442     reaction         0          16 -20.852259  23.842860\n6082      seventy         0          16 -20.852259  23.842860\n7342     urgently         0          16 -20.852259  23.842860\n5455      realism         0          15 -20.759150  22.352682\n```\n:::\n:::\n\n\nNow we can see the full picture! Look at the output:\n\n- **Positive log odds** (e.g., +2.5) means Democrats use this word more (roughly 2^2.5 ≈ 5-6× more often)\n- **Negative log odds** (e.g., -1.8) means Republicans use this word more (roughly 2^1.8 ≈ 3-4× more often)\n\n### Reading the results: Putting it all together\n\nFor each word, we now have **three** key numbers:\n\n1. **Democrat count / Republican count**: Raw frequencies (affected by corpus size)\n2. **Log odds ratio**: Effect size - which party uses it more and by how much\n3. **G² value**: Statistical significance - how confident we can be\n\n**Example interpretation**:\n\nIf you see a word with:\n- Log odds = +2.0\n- G² = 45.3\n\nThis means: \"Democrats use this word about 4× more often than Republicans, and we're extremely confident (p < 0.001) this is a real pattern, not chance.\"\n\n::: {.callout-tip}\n## Best practice: Filter for both significance AND effect size\n\nNot every statistically significant difference is interesting. And not every large difference is reliable.\n\nThe most meaningful words are those that pass **three** tests:\n\n1. **Statistically significant**: G² > 6.63 (we're 99% confident it's real)\n2. **Large effect**: |log odds| > 0.5 (at least 40% more frequent in one corpus)\n3. **Not too rare**: Appears at least 5 times in both corpora (reliable measurement)\n\nOnly words that pass all three tests are truly distinctive and reliable.\n:::\n\n### Finding the most meaningful differences\n\nLet's filter our results to find words that are both statistically significant AND show large effects:\n\n::: {#0538d856 .cell execution_count=20}\n``` {.python .cell-code}\n# Find meaningful differences - must pass all three tests\nmeaningful = content_words[\n    (content_words['g2'] > 6.63) &                    # Test 1: Statistically significant\n    (np.abs(content_words['log_odds']) > 0.5) &       # Test 2: Large effect size\n    (content_words['democrat'] > 5) &                 # Test 3: Not too rare\n    (content_words['republican'] > 5)\n].copy()\n\nprint(f\"Words with significant AND large differences: {len(meaningful)}\")\nprint(\"\\nTop 10 most distinctively Democratic words:\")\nprint(meaningful.nlargest(10, 'log_odds')[['lemma', 'democrat', 'republican', 'log_odds', 'g2']])\n\nprint(\"\\nTop 10 most distinctively Republican words:\")\nprint(meaningful.nsmallest(10, 'log_odds')[['lemma', 'democrat', 'republican', 'log_odds', 'g2']])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWords with significant AND large differences: 376\n\nTop 10 most distinctively Democratic words:\n           lemma  democrat  republican  log_odds          g2\n1278     company       120          14  2.984616   85.637924\n4296     minimum        56           8  2.692435   35.797119\n2469   everybody        37           6  2.509570   21.825902\n1226     college       174          29  2.470043  100.740012\n3605  innovation        54          10  2.318039   28.953921\n4071         lot        78          15  2.263592   40.605421\n282   aggression        80          16  2.207008   40.338351\n2955         gas        60          12  2.207008   30.253763\n776          big       120          25  2.148115   58.448358\n5008      planet        28           6  2.107472   13.304258\n\nTop 10 most distinctively Republican words:\n          lemma  democrat  republican  log_odds         g2\n1061       cent         6          86 -3.956219  91.520368\n5282   property         6          67 -3.596044  66.090068\n3765      iraqi         6          48 -3.114917  41.579958\n1263  commodity         6          42 -2.922272  34.142816\n1743   decrease         6          40 -2.851883  31.708856\n826       board         6          40 -2.851883  31.708856\n5588     regime        10          61 -2.723727  46.054082\n2921   function         8          48 -2.699880  35.895878\n4907      pende         6          35 -2.659238  25.744069\n1582      court        18         103 -2.631494  74.890387\n```\n:::\n:::\n\n\nThese are the words that truly distinguish Democratic from Republican political rhetoric - they're both statistically reliable and substantively important.\n\n---\n\n## Visualizing differences\n\n### Bar chart of log odds ratios\n\n::: {#b49184f0 .cell execution_count=21}\n``` {.python .cell-code}\n# Get top 15 for each party\ntop_dem = meaningful.nlargest(15, 'log_odds')\ntop_rep = meaningful.nsmallest(15, 'log_odds')\ntop_both = pd.concat([top_dem, top_rep])\n\n# Sort by log odds for plotting\ntop_both = top_both.sort_values('log_odds')\n\n# Create plot\nfig, ax = plt.subplots(figsize=(10, 8))\n\ncolors = ['#0015BC' if x > 0 else '#E81B23' for x in top_both['log_odds']]\nax.barh(range(len(top_both)), top_both['log_odds'], color=colors, alpha=0.7)\nax.set_yticks(range(len(top_both)))\nax.set_yticklabels(top_both['lemma'])\nax.axvline(0, color='black', linewidth=0.8, linestyle='--')\nax.set_xlabel('Log Odds Ratio (negative = Republican, positive = Democrat)', fontsize=12)\nax.set_title('Most Distinctive Words by Party', fontsize=14, fontweight='bold')\n\n# Add legend\nfrom matplotlib.patches import Patch\nlegend_elements = [\n    Patch(facecolor='#0015BC', alpha=0.7, label='More Democratic'),\n    Patch(facecolor='#E81B23', alpha=0.7, label='More Republican')\n]\nax.legend(handles=legend_elements, loc='lower right')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](lab_02_files/figure-html/cell-22-output-1.svg){}\n:::\n:::\n\n\n### Scatter plot: Significance vs effect size\n\nA scatter plot helps us visualize the relationship between effect size (log odds ratio) and statistical significance (G²).\n\nHowever, we need to be careful about very rare words. Words that appear only once or twice in one corpus but zero times in the other create extreme log odds ratios (dividing by near-zero) with low statistical significance. These are statistical artifacts, not meaningful patterns.\n\nTo avoid misleading visualizations, we'll filter out words that don't appear at least 5 times in both corpora:\n\n::: {#30f9b329 .cell execution_count=22}\n``` {.python .cell-code}\n# Create scatter plot\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Filter for plotting - remove very rare words that create artifacts\nplot_data = content_words[\n    (content_words['democrat'] >= 5) &\n    (content_words['republican'] >= 5)\n].copy()\n\n# Color by which party uses word more\ncolors = ['#0015BC' if x > 0 else '#E81B23' for x in plot_data['log_odds']]\n\nax.scatter(plot_data['log_odds'], plot_data['g2'],\n           c=colors, alpha=0.5, s=30)\n\n# Add significance threshold line\nax.axhline(6.63, color='gray', linestyle='--', linewidth=1, alpha=0.7, label='p < 0.01')\n\n# Add effect size threshold lines\nax.axvline(-0.5, color='gray', linestyle=':', linewidth=1, alpha=0.5)\nax.axvline(0.5, color='gray', linestyle=':', linewidth=1, alpha=0.5)\n\nax.set_xlabel('Log Odds Ratio (negative = Republican, positive = Democrat)', fontsize=12)\nax.set_ylabel('Log-Likelihood (G²)', fontsize=12)\nax.set_title('Effect Size vs Statistical Significance', fontsize=14, fontweight='bold')\n\n# Annotate some interesting words\nfor _, row in meaningful.head(10).iterrows():\n    ax.annotate(row['lemma'], \n                (row['log_odds'], row['g2']),\n                fontsize=8, alpha=0.7,\n                xytext=(5, 5), textcoords='offset points')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](lab_02_files/figure-html/cell-23-output-1.svg){}\n:::\n:::\n\n\nThis scatter plot shows the relationship between:\n\n- **X-axis**: Effect size (how different?)\n- **Y-axis**: Statistical significance (how confident?)\n\nThe most interesting words are in the **upper left** and **upper right** corners - both statistically significant (high G²) and distinctive (large absolute log odds ratio). These are the words that show strong, reliable differences between the two parties.\n\nWords near the bottom (low G²) may have large log odds ratios but aren't statistically reliable - often because they're too rare. The horizontal line at G² = 6.63 marks the p < 0.01 significance threshold.\n\n---\n\n## Named entity recognition\n\nSo far we've analyzed individual words (lemmas). But sometimes we're interested in references to real-world entities:\n\n- **PERSON**: Barack Obama, Hillary Clinton\n- **ORG**: United Nations, Department of Defense\n- **GPE**: America, Iraq, New York\n- **DATE**: tomorrow, 2020, next year\n- **MONEY**: $1 billion, five dollars\n\nThis is called **Named Entity Recognition (NER)**, and spaCy does it automatically!\n\n### How NER works\n\nNER is a classification task:\n\n1. Identify spans of text that might be entities\n2. Classify each span into entity types\n3. Use machine learning models trained on annotated data\n\nModern NER systems use neural networks trained on large corpora of hand-labeled examples.\n\n### Extracting entities with spaCy\n\nLet's look at entities in a sample speech:\n\n::: {#31e9b4cb .cell execution_count=23}\n``` {.python .cell-code}\n# Get one speech\nsample_speech = dem_speeches.iloc[0]['transcript'][:1000]  # First 1000 chars\n\n# Process it\nsample_doc = nlp(sample_speech)\n\n# Display entities\nprint(\"Named entities found:\\n\")\nprint(f\"{'Entity':<25} {'Type':<15} {'Explanation'}\")\nprint(\"-\" * 65)\n\nfor ent in sample_doc.ents:\n    print(f\"{ent.text:<25} {ent.label_:<15} {spacy.explain(ent.label_)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNamed entities found:\n\nEntity                    Type            Explanation\n-----------------------------------------------------------------\nSpeaker                   PERSON          People, including fictional\nCongress                  ORG             Companies, agencies, institutions, etc.\nAmericans                 NORP            Nationalities or religious or political groups\nTonight                   TIME            Times smaller than a day\nthe eighth year           DATE            Absolute or relative dates or periods\nthe State of the Union    ORG             Companies, agencies, institutions, etc.\nIowa                      GPE             Countries, cities, states\nan election season        DATE            Absolute or relative dates or periods\nthis year                 DATE            Absolute or relative dates or periods\nSpeaker                   PERSON          People, including fictional\nthe end of last year      DATE            Absolute or relative dates or periods\nthis year                 DATE            Absolute or relative dates or periods\ntonight                   TIME            Times smaller than a day\nthe year ahead            DATE            Absolute or relative dates or periods\nDon                       PERSON          People, including fictional\n```\n:::\n:::\n\n\n### Comparing entity usage across parties\n\nLet's extract all location entities (GPE = Geo-Political Entity) from both corpora:\n\n::: {#265fb32a .cell execution_count=24}\n``` {.python .cell-code}\n# Extract GPE entities from both corpora\ndem_locations = [ent.text.lower() for ent in dem_doc.ents if ent.label_ == 'GPE']\nrep_locations = [ent.text.lower() for ent in rep_doc.ents if ent.label_ == 'GPE']\n\nprint(f\"Democratic location mentions: {len(dem_locations)}\")\nprint(f\"Republican location mentions: {len(rep_locations)}\")\n\n# Count frequencies\ndem_loc_counts = pd.Series(dem_locations).value_counts().reset_index()\ndem_loc_counts.columns = ['location', 'democrat']\n\nrep_loc_counts = pd.Series(rep_locations).value_counts().reset_index()\nrep_loc_counts.columns = ['location', 'republican']\n\n# Merge\nlocation_freq = dem_loc_counts.merge(rep_loc_counts, on='location', how='outer').fillna(0)\nlocation_freq['democrat'] = location_freq['democrat'].astype(int)\nlocation_freq['republican'] = location_freq['republican'].astype(int)\n\n# Filter for locations mentioned at least 5 times\nlocation_freq = location_freq[\n    (location_freq['democrat'] >= 5) | (location_freq['republican'] >= 5)\n].copy()\n\nprint(f\"\\nLocations mentioned frequently:\")\nprint(location_freq.head(15))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDemocratic location mentions: 2133\nRepublican location mentions: 2007\n\nLocations mentioned frequently:\n       location  democrat  republican\n3   afghanistan        40          57\n5        alaska        10           2\n7       america       554         701\n19    australia         0           6\n21      baghdad         0          18\n28      belgium         6           2\n29       berlin        20           2\n38       brazil         4           6\n42        burma         6           4\n44   california        18          13\n45       canada         2          16\n52      chicago         4           6\n54        china        68          31\n57     colombia        14           0\n61        congo         6           2\n```\n:::\n:::\n\n\n### Statistical comparison of locations\n\n::: {#87fc4e54 .cell execution_count=25}\n``` {.python .cell-code}\n# Calculate G² and log odds for locations\nlocation_freq['g2'] = log_likelihood(\n    location_freq['democrat'].values,\n    location_freq['republican'].values\n)\n\nlocation_freq['log_odds'] = log_odds_ratio(\n    location_freq['democrat'].values,\n    location_freq['republican'].values\n)\n\n# Find significant differences\nsig_locations = location_freq[\n    (location_freq['g2'] > 6.63) &\n    (location_freq['democrat'] >= 3) &\n    (location_freq['republican'] >= 3)\n].copy()\n\nprint(\"Locations with significant usage differences:\\n\")\nprint(\"Most Democratic:\")\nprint(sig_locations.nlargest(10, 'log_odds')[['location', 'democrat', 'republican', 'log_odds', 'g2']])\n\nprint(\"\\nMost Republican:\")\nprint(sig_locations.nsmallest(10, 'log_odds')[['location', 'democrat', 'republican', 'log_odds', 'g2']])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLocations with significant usage differences:\n\nMost Democratic:\n                         location  democrat  republican  log_odds         g2\n96                        germany        24           4  2.539343  15.224309\n62                           cuba        16           3  2.369418   9.359102\n158                        mexico        16           3  2.369418   9.359102\n113                         india        16           4  1.954381   7.335339\n238                        russia        36          11  1.664874  13.230245\n290  the united states of america        48          20  1.217415  11.011168\n284              the soviet union        64          28  1.147026  13.355114\n54                          china        68          31  1.087647  13.024438\n314                       vietnam        48          22  1.079912   9.087790\n7                         america       554         701 -0.385148  22.219891\n\nMost Republican:\n                         location  democrat  republican  log_odds         g2\n119                          iraq        19         119 -2.692510  83.903263\n261                        states        59          88 -0.622408   6.712539\n288             the united states       116         163 -0.536366   9.511364\n7                         america       554         701 -0.385148  22.219891\n314                       vietnam        48          22  1.079912   9.087790\n54                          china        68          31  1.087647  13.024438\n284              the soviet union        64          28  1.147026  13.355114\n290  the united states of america        48          20  1.217415  11.011168\n238                        russia        36          11  1.664874  13.230245\n113                         india        16           4  1.954381   7.335339\n```\n:::\n:::\n\n\n### Visualizing location mentions\n\n::: {#93177c3a .cell execution_count=26}\n``` {.python .cell-code}\nif len(sig_locations) > 0:\n    # Get top locations for each party\n    top_dem_loc = sig_locations.nlargest(10, 'log_odds')\n    top_rep_loc = sig_locations.nsmallest(10, 'log_odds')\n    top_loc = pd.concat([top_dem_loc, top_rep_loc]).drop_duplicates().sort_values('log_odds')\n    \n    # Plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    colors = ['#0015BC' if x > 0 else '#E81B23' for x in top_loc['log_odds']]\n    \n    ax.barh(range(len(top_loc)), top_loc['log_odds'], color=colors, alpha=0.7)\n    ax.set_yticks(range(len(top_loc)))\n    ax.set_yticklabels(top_loc['location'])\n    ax.axvline(0, color='black', linewidth=0.8, linestyle='--')\n    ax.set_xlabel('Log Odds Ratio (negative = Republican, positive = Democrat)', fontsize=12)\n    ax.set_title('Geographic Focus: Location Mentions by Party', fontsize=14, fontweight='bold')\n    \n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Not enough significant location differences in our sample.\")\n```\n\n::: {.cell-output .cell-output-display}\n![](lab_02_files/figure-html/cell-27-output-1.svg){}\n:::\n:::\n\n\n::: {.callout-note}\n## Other entity types\n\nYou can analyze other entity types the same way:\n\n- **PERSON**: Which individuals are mentioned?\n- **ORG**: What organizations are discussed?\n- **DATE**: How are temporal references used?\n- **MONEY**: How are financial amounts discussed?\n\nTry exploring these in the exercises!\n:::\n\n---\n\n## Summary and key takeaways\n\n### What we learned\n\nToday we covered methods for statistically comparing corpora:\n\n1. **Lemmatization** → Reducing words to dictionary forms\n2. **Corpus preparation** → Creating contrasting text collections\n3. **Log-likelihood (G²)** → Testing statistical significance\n4. **Log odds ratio** → Measuring effect size\n5. **Named entity recognition** → Extracting references to real-world entities\n\n### Key concepts\n\n**Lemmatization**\n: Reducing wordforms to their base dictionary form (lemma)\n\n**Contrasting corpora**\n: Collections of texts from different sources for comparison\n\n**Log-likelihood (G²)**\n: Statistical test for significance of frequency differences\n\n**Log odds ratio**\n: Measure of effect size (how much more frequent)\n\n**Named entity**\n: Reference to a real-world entity (person, place, organization)\n\n**Effect size vs significance**\n: Significance = confidence; effect size = magnitude\n\n### Critical insights\n\n::: {.callout-important}\n## Don't trust p-values alone!\n\nA word can be:\n\n- **Highly significant** but barely different (large sample)\n- **Highly different** but not significant (rare word)\n\nAlways report *both* significance and effect size.\n:::\n\n::: {.callout-important}\n## Preprocessing choices matter\n\n- Lemmatize or not?\n- Remove stop words or not?\n- Filter by part of speech or not?\n\nEach choice affects your results. Make them **explicit** and **justified**.\n:::\n\n### Statistical comparison workflow\n\n1. **Prepare corpora** → Split data into contrasting groups\n2. **Lemmatize** → Reduce morphological variation (if appropriate)\n3. **Count frequencies** → Create frequency table\n4. **Filter** → Remove very rare words, stop words (if appropriate)\n5. **Calculate G²** → Test significance\n6. **Calculate log odds** → Measure effect size\n7. **Filter meaningful differences** → Both significant AND large\n8. **Interpret** → What do the differences tell us?\n\n---\n\n## Exercises\n\n### Exercise 1: Full corpus analysis\n\nWe used samples for speed in this lab. Now process the *full* corpora:\n\n1. Process all Democratic and all Republican speeches (not just samples)\n2. Calculate G² and log odds ratio for all lemmas\n3. Identify the 20 most distinctive content words for each party\n4. Create visualizations\n\n**Note**: This will take 10-15 minutes to process!\n\n### Exercise 2: Stop word investigation\n\nInvestigate whether stop words show political patterns:\n\n1. Filter for *only* stop words in your frequency table\n2. Calculate G² and log odds ratio\n3. Which stop words differ most between parties?\n4. Can you interpret why? (Think about formality, rhetorical style)\n\n### Exercise 3: Temporal comparison\n\nInstead of comparing parties, compare time periods:\n\n1. Split speeches into before/after 1970 (or another meaningful date)\n2. Calculate distinctive words for each period\n3. What changes in American political discourse can you observe?\n\n### Exercise 4: Named entity deep dive\n\nChoose one entity type (PERSON, ORG, or DATE) and:\n\n1. Extract all entities of that type from both corpora\n2. Calculate frequency differences\n3. Identify significant patterns\n4. Interpret: What do these patterns reveal about political priorities?\n\n### Exercise 5: Part-of-speech patterns (Advanced)\n\nCompare parts of speech:\n\n1. Count how often each POS tag appears in each corpus\n2. Do Democrats use more adjectives? Republicans more verbs?\n3. Calculate significance and effect size\n4. What might linguistic differences reveal about rhetorical style?\n\n### Exercise 6: Creating your own contrasting corpora\n\nThink of another comparison that interests you in the State of the Union data:\n\n- War vs peace time presidents\n- First term vs second term speeches\n- 19th vs 20th vs 21st century\n- High vs low approval ratings (you'd need to add this data)\n\nDesign and execute your own corpus comparison study.\n\n---\n\n## References and further reading\n\n### Academic papers\n\n- Dunning, T. (1993). Accurate methods for the statistics of surprise and coincidence. *Computational Linguistics*, 19(1), 61-74. [https://aclanthology.org/J93-1003.pdf](https://aclanthology.org/J93-1003.pdf)\n- Rayson, P., & Garside, R. (2000). Comparing corpora using frequency profiling. *Proceedings of the Workshop on Comparing Corpora*, 1-6. [https://doi.org/10.3115/1117729.1117730](https://doi.org/10.3115/1117729.1117730)\n- Monroe, B. L., Colaresi, M. P., & Quinn, K. M. (2008). Fightin' words: Lexical feature selection and evaluation for identifying the content of political conflict. *Political Analysis*, 16(4), 372-403. [https://doi.org/10.1093/pan/mpn018](https://doi.org/10.1093/pan/mpn018) \n\n### Textbooks\n\n- Jurafsky, D., & Martin, J. H. (2023). *Speech and Language Processing* (3rd ed., draft). Chapter 2 (Regular Expressions, Text Normalization, Edit Distance). [https://web.stanford.edu/~jurafsky/slp3/](https://web.stanford.edu/~jurafsky/slp3/)\n- Silge, J., & Robinson, D. (2017). *Text Mining with R*. Chapter 4 (Relationships between words). [https://www.tidytextmining.com/ngrams.html](https://www.tidytextmining.com/ngrams.html)\n\n### Tutorials\n\n- spaCy documentation on lemmatization: [https://spacy.io/usage/linguistic-features#lemmatization](https://spacy.io/usage/linguistic-features#lemmatization)\n- spaCy documentation on NER: [https://spacy.io/usage/linguistic-features#named-entities](https://spacy.io/usage/linguistic-features#named-entities)\n- Log-likelihood calculator and explanation: [http://ucrel.lancs.ac.uk/llwizard.html](http://ucrel.lancs.ac.uk/llwizard.html)\n\n### Tools\n\n- **spaCy**: Industrial-strength NLP library - [https://spacy.io](https://spacy.io)\n- **NLTK**: Classic Python NLP toolkit - [https://www.nltk.org](https://www.nltk.org)\n- **Lancaster Stats Tools**: Log-likelihood calculator - [http://ucrel.lancs.ac.uk/llwizard.html](http://ucrel.lancs.ac.uk/llwizard.html)\n\n---\n\n**End of Lab 02**\n\n",
    "supporting": [
      "lab_02_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}