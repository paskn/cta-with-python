{
  "hash": "e9296d4bf66c9630670c39cd6e109291",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Lab 01: Words as data points I\"\nsubtitle: \"Zipf's law, stop words and stylometry\"\nformat:\n  html:\n    number-sections: true\n    fig-format: svg\n    toc: true\n    toc-depth: 4\ndate: now\ndate-format: \"YYYY-MM-DD HH:mm:ss\"\nexecute:\n  error: true\n---\n\n## Learning Objectives\n\nBy the end of this lab, you will understand:\n\n- How to transform unstructured text into structured data\n- What tokenization is and why it matters\n- How word frequencies reveal patterns in text\n- Zipf's Law and its implications for text analysis\n- What stop words are and when to remove them\n- The basics of text preprocessing\n- What stylometry is and how function words reveal authorship\n- How to use Python and pandas for basic text analysis\n\n---\n\n## Introduction: Why Text as Data?\n\nText is everywhere: social media posts, news articles, scientific papers, government documents, customer reviews. But text in its raw form is **unstructured** - it's just sequences of characters. To analyze text computationally, we need to transform it into **structured data** that computers can process mathematically.\n\nThink of it this way: if you wanted to compare two novels, you could read both and form an impression. But what if you had 1,000 novels? Or 100,000 tweets? This is where computational text analysis becomes essential.\n\n### Our Dataset: State of the Union Addresses\n\nToday we'll work with a collection of State of the Union addresses - speeches given by US presidents from the 18th century to 2018. These speeches are:\n\n- **Historical**: Spanning over 200 years\n- **Political**: Reflecting different eras and priorities\n- **Comparable**: Same genre, same occasion, different authors\n\nThis makes them perfect for learning text analysis techniques.\n\n**Data source**: [Brian Weinstein's State of the Union corpus](https://github.com/BrianWeinstein/state-of-the-union)\n\n---\n\n## Setup: Loading Packages\n\nFirst, let's load the Python packages we'll need:\n\n```bash\n# you will need to run it once after installing spaCy\n!python -m spacy download en_core_web_sm\n```\n\n::: {#61887a1b .cell execution_count=1}\n``` {.python .cell-code}\n# Data manipulation\nimport pandas as pd\n\n# Text processing\nimport spacy\nfrom collections import Counter\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# For word clouds\nfrom wordcloud import WordCloud\n\n# Set visualization style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\nprint(\"âœ“ Packages loaded successfully\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nâœ“ Packages loaded successfully\n```\n:::\n:::\n\n\n::: {.callout-note}\n## ðŸ“Œ About these packages\n\n- **pandas**: Works with tabular data (like spreadsheets)\n- **spaCy**: Processes natural language text\n- **matplotlib/seaborn**: Create visualizations\n- **wordcloud**: Generate word cloud visualizations\n:::\n\n---\n\n## Loading and Exploring the Data\n\nLet's load our dataset:\n\n::: {#c0192f4b .cell execution_count=2}\n``` {.python .cell-code}\n# Load the data\nspeeches = pd.read_csv(\"data/transcripts.csv\")\n\n# Display first few rows\nprint(\"Dataset shape:\", speeches.shape)\nprint(\"\\nFirst few rows:\")\nspeeches.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDataset shape: (244, 5)\n\nFirst few rows:\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>president</th>\n      <th>title</th>\n      <th>url</th>\n      <th>transcript</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-01-30</td>\n      <td>Donald J. Trump</td>\n      <td>Address Before a Joint Session of the Congress...</td>\n      <td>https://www.cnn.com/2018/01/30/politics/2018-s...</td>\n      <td>\\nMr. Speaker, Mr. Vice President, Members of ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-02-28</td>\n      <td>Donald J. Trump</td>\n      <td>Address Before a Joint Session of the Congress</td>\n      <td>http://www.presidency.ucsb.edu/ws/index.php?pi...</td>\n      <td>Thank you very much. Mr. Speaker, Mr. Vice Pre...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2016-01-12</td>\n      <td>Barack Obama</td>\n      <td>Address Before a Joint Session of the Congress...</td>\n      <td>http://www.presidency.ucsb.edu/ws/index.php?pi...</td>\n      <td>Thank you. Mr. Speaker, Mr. Vice President, Me...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2015-01-20</td>\n      <td>Barack Obama</td>\n      <td>Address Before a Joint Session of the Congress...</td>\n      <td>http://www.presidency.ucsb.edu/ws/index.php?pi...</td>\n      <td>The President. Mr. Speaker, Mr. Vice President...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2014-01-28</td>\n      <td>Barack Obama</td>\n      <td>Address Before a Joint Session of the Congress...</td>\n      <td>http://www.presidency.ucsb.edu/ws/index.php?pi...</td>\n      <td>The President. Mr. Speaker, Mr. Vice President...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Understanding the Data Structure\n\nOur data is in **tabular format** - like a spreadsheet with rows and columns:\n\n::: {#8470b483 .cell execution_count=3}\n``` {.python .cell-code}\n# Check column names and types\nprint(\"Columns:\")\nprint(speeches.dtypes)\n\n# Basic statistics\nprint(\"\\nNumber of speeches:\", len(speeches))\nprint(\"Number of presidents:\", speeches['president'].nunique())\nprint(\"\\nDate range:\")\nprint(\"  Earliest:\", speeches['date'].min())\nprint(\"  Latest:\", speeches['date'].max())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nColumns:\ndate          object\npresident     object\ntitle         object\nurl           object\ntranscript    object\ndtype: object\n\nNumber of speeches: 244\nNumber of presidents: 42\n\nDate range:\n  Earliest: 1790-01-08\n  Latest: 2018-01-30\n```\n:::\n:::\n\n\nEach row represents **one speech**. The columns contain:\n\n- **date**: When the speech was given\n- **president**: Who gave it\n- **title**: The speech's official title\n- **url**: Where the transcript came from\n- **transcript**: The actual text of the speech\n\nLet's look at a short excerpt:\n\n::: {#fe241d9c .cell execution_count=4}\n``` {.python .cell-code}\n# Display a snippet of one speech\nsample_text = speeches.loc[0, 'transcript'][:500]  # First 500 characters\nprint(\"Sample from first speech:\")\nprint(sample_text)\nprint(\"...\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSample from first speech:\n\nMr. Speaker, Mr. Vice President, Members of Congress, the First Lady of the United States, and my fellow Americans:\nLess than 1 year has passed since I first stood at this podium, in this majestic chamber, to speak on behalf of the American People -- and to address their concerns, their hopes, and their dreams.  That night, our new Administration had already taken swift action.  A new tide of optimism was already sweeping across our land.\nEach day since, we have gone forward with a clear vision\n...\n```\n:::\n:::\n\n\n::: {.callout-tip}\n## ðŸ’¡ Why This Structure?\n\nText data commonly comes with **metadata** (data about data):\n\n- Author, date, source, title, etc.\n- This metadata helps us compare and analyze texts\n- We keep text and metadata together in a table\n:::\n\n---\n\n## From Text to Tokens - The Bag of Words Model\n\n### The Challenge: Text is Unstructured\n\nRight now, our `transcript` column contains long strings of text. How do we measure or compare them? We can't easily do math on text!\n\n### The Solution: Tokenization\n\n**Tokenization** is the process of splitting text into smaller units called **tokens**. Usually, tokens are words, but they could also be:\n\n- Characters (letters)\n- Sentences\n- N-grams (sequences of N words)\n\nLet's see this in action:\n\n::: {#42579c25 .cell execution_count=5}\n``` {.python .cell-code}\n# Simple example: split text into words\nexample = \"Python is great for text analysis\"\nwords = example.split()\nprint(\"Original text:\", example)\nprint(\"Tokens (words):\", words)\nprint(\"Number of tokens:\", len(words))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal text: Python is great for text analysis\nTokens (words): ['Python', 'is', 'great', 'for', 'text', 'analysis']\nNumber of tokens: 6\n```\n:::\n:::\n\n\n### Tokenizing Our Speeches\n\nNow let's tokenize one full speech:\n\n::: {#35614060 .cell execution_count=6}\n``` {.python .cell-code}\n# Get one speech\nspeech_text = speeches.loc[0, 'transcript']\n\n# Simple tokenization (just splitting on spaces)\nsimple_tokens = speech_text.split()\n\nprint(f\"Speech length: {len(speech_text)} characters\")\nprint(f\"Number of tokens: {len(simple_tokens)}\")\nprint(f\"\\nFirst 20 tokens:\")\nprint(simple_tokens[:20])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSpeech length: 30354 characters\nNumber of tokens: 5190\n\nFirst 20 tokens:\n['Mr.', 'Speaker,', 'Mr.', 'Vice', 'President,', 'Members', 'of', 'Congress,', 'the', 'First', 'Lady', 'of', 'the', 'United', 'States,', 'and', 'my', 'fellow', 'Americans:', 'Less']\n```\n:::\n:::\n\n\n### The Bag of Words Model\n\nOnce we have tokens, we can treat text as a \"**bag of words**\" - we:\n\n1. **Ignore word order** (\"cat dog\" = \"dog cat\")\n2. **Count word frequencies**\n3. **Represent text as numbers**\n\nThis might seem like we're throwing away information (word order matters!), but this simple model is surprisingly powerful for many tasks.\n\nLet's create a bag of words for one speech:\n\n::: {#d87db820 .cell execution_count=7}\n``` {.python .cell-code}\n# Count word frequencies\nword_counts = Counter(simple_tokens)\n\n# Show most common words\nprint(\"Top 15 most frequent words:\")\nfor word, count in word_counts.most_common(15):\n    print(f\"  {word}: {count}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTop 15 most frequent words:\n  the: 215\n  and: 184\n  to: 175\n  of: 121\n  our: 95\n  we: 88\n  a: 79\n  in: 72\n  is: 61\n  are: 48\n  that: 45\n  --: 44\n  have: 40\n  for: 34\n  will: 34\n```\n:::\n:::\n\n\n### Two Data Formats: Long vs Wide\n\nWe can represent tokenized text in two ways:\n\n**Long format**: One row per word occurrence\n```\npresident    word\nTrump        the\nTrump        economy\nTrump        is\nTrump        strong\n```\n\n**Wide format**: One row per document, one column per unique word\n```\npresident    the    economy    is    strong\nTrump        150    12         89    5\n```\n\nFor now, we'll work with long format because it's easier to understand and manipulate.\n\n---\n\n## Tokenizing All Speeches\n\nLet's tokenize all speeches and create a long-format dataset:\n\n::: {#b941d6c3 .cell execution_count=8}\n``` {.python .cell-code}\n# Initialize empty list to store results\nall_tokens = []\n\n# Process each speech\nfor idx, row in speeches.iterrows():\n    president = row['president']\n    text = row['transcript']\n    \n    # Simple tokenization\n    tokens = text.lower().split()  # Convert to lowercase\n    \n    # Add each token to our list\n    for token in tokens:\n        all_tokens.append({\n            'president': president,\n            'word': token\n        })\n\n# Convert to DataFrame\ntokens_df = pd.DataFrame(all_tokens)\n\nprint(f\"Total number of tokens: {len(tokens_df):,}\")\nprint(f\"\\nFirst few rows:\")\ntokens_df.head(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal number of tokens: 3,947,946\n\nFirst few rows:\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>president</th>\n      <th>word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Donald J. Trump</td>\n      <td>mr.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Donald J. Trump</td>\n      <td>speaker,</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Donald J. Trump</td>\n      <td>mr.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Donald J. Trump</td>\n      <td>vice</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Donald J. Trump</td>\n      <td>president,</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Donald J. Trump</td>\n      <td>members</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Donald J. Trump</td>\n      <td>of</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Donald J. Trump</td>\n      <td>congress,</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Donald J. Trump</td>\n      <td>the</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Donald J. Trump</td>\n      <td>first</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### How many unique words?\n\n::: {#dc2db946 .cell execution_count=9}\n``` {.python .cell-code}\nunique_words = tokens_df['word'].nunique()\nprint(f\"Number of unique words: {unique_words:,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of unique words: 68,356\n```\n:::\n:::\n\n\nThat's a lot of unique words! But are they all useful?\n\n---\n\n## Word Frequencies\n\nNow let's count how often each word appears:\n\n::: {#c4597f1a .cell execution_count=10}\n``` {.python .cell-code}\n# Count word frequencies\nword_freq = tokens_df['word'].value_counts().reset_index()\nword_freq.columns = ['word', 'count']\n\nprint(\"Top 20 most frequent words:\")\nprint(word_freq.head(20))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTop 20 most frequent words:\n     word   count\n0     the  326862\n1      of  212531\n2      to  135329\n3     and  133944\n4      in   85772\n5       a   61992\n6    that   47130\n7     for   43079\n8      be   40521\n9     our   38759\n10     is   36752\n11     by   32429\n12     it   29271\n13     we   27276\n14   have   26784\n15   this   26594\n16     as   26510\n17   with   26400\n18  which   26237\n19   will   21913\n```\n:::\n:::\n\n\n### Visualizing Frequency\n\n::: {#4557387e .cell execution_count=11}\n``` {.python .cell-code}\n# Plot top 15 words\ntop_15 = word_freq.head(15)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(data=top_15, y='word', x='count', palette='viridis')\nplt.title('Top 15 Most Frequent Words in State of the Union Addresses', fontsize=14, fontweight='bold')\nplt.xlabel('Frequency (count)', fontsize=12)\nplt.ylabel('Word', fontsize=12)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_1012969/2171022789.py:5: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=top_15, y='word', x='count', palette='viridis')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](lab_01_files/figure-html/cell-12-output-2.svg){}\n:::\n:::\n\n\n### What do we notice?\n\nThe most frequent words are:\n- Articles: \"the\", \"a\", \"an\"\n- Prepositions: \"of\", \"to\", \"in\", \"for\"\n- Conjunctions: \"and\", \"but\", \"or\"\n\nThese are **grammatical words** that appear in almost any English text. They don't tell us much about the *content* of the speeches!\n\nThis observation leads us to an important concept...\n\n---\n\n## Zipf's Law - A Fundamental Pattern\n\n### What is Zipf's Law?\n\nIf you count word frequencies in *any* large collection of text and rank words by frequency, you'll observe something remarkable:\n\n**The most common word appears roughly twice as often as the second most common word, three times as often as the third most common, and so on.**\n\nThis is called **Zipf's Law** (named after linguist George Kingsley Zipf).\n\n### Why Does This Matter?\n\nZipf's Law tells us that:\n\n1. **A few words are very common** (\"the\", \"of\", \"and\")\n2. **Most words are very rare** (appear only once or twice)\n3. **This pattern is universal** - it appears in all languages and genres\n\nThis has practical implications:\n- Most words provide little statistical power (they're too rare)\n- A small vocabulary covers most of any text\n- We need strategies to deal with this imbalance\n\n### Visualizing Zipf's Law\n\nLet's see if our data follows Zipf's Law:\n\n::: {#4d7ccbad .cell execution_count=12}\n``` {.python .cell-code}\n# Add rank to our frequency table\nword_freq['rank'] = range(1, len(word_freq) + 1)\n\n# Create log-log plot\nplt.figure(figsize=(10, 6))\nplt.loglog(word_freq['rank'], word_freq['count'], 'b.')\nplt.xlabel('Rank (log scale)', fontsize=12)\nplt.ylabel('Frequency (log scale)', fontsize=12)\nplt.title(\"Zipf's Law in State of the Union Addresses\", fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](lab_01_files/figure-html/cell-13-output-1.svg){}\n:::\n:::\n\n\nThe nearly straight line on a log-log plot confirms Zipf's Law!\n\n### The Long Tail\n\nLet's look at how many words appear only once or twice:\n\n::: {#aa0997ef .cell execution_count=13}\n``` {.python .cell-code}\n# Count words by frequency\nfreq_distribution = word_freq['count'].value_counts().sort_index()\n\nprint(\"Distribution of word frequencies:\")\nprint(f\"  Words appearing once: {freq_distribution.get(1, 0):,}\")\nprint(f\"  Words appearing twice: {freq_distribution.get(2, 0):,}\")\nprint(f\"  Words appearing 3-5 times: {freq_distribution.loc[3:5].sum():,}\")\nprint(f\"  Words appearing 6-10 times: {freq_distribution.loc[6:10].sum():,}\")\nprint(f\"  Words appearing > 100 times: {(word_freq['count'] > 100).sum():,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDistribution of word frequencies:\n  Words appearing once: 9,835\n  Words appearing twice: 27,310\n  Words appearing 3-5 times: 8,157\n  Words appearing 6-10 times: 7,957\n  Words appearing > 100 times: 3,484\n```\n:::\n:::\n\n\nThis is the \"**long tail**\" - many rare words, few common words.\n\n::: {.callout-important}\n## ðŸŽ¯ Key Insight\n\nZipf's Law means that word frequencies are **highly skewed**. This affects how we:\n\n- Build statistical models\n- Select features for machine learning\n- Preprocess text\n- Interpret results\n:::\n\n---\n\n## Stop Words and Preprocessing\n\n### What are Stop Words?\n\n**Stop words** are extremely common words that appear frequently in almost any text:\n\n- Articles: the, a, an\n- Prepositions: in, on, at, to, from\n- Pronouns: I, you, he, she, it\n- Conjunctions: and, but, or\n- Auxiliary verbs: is, are, was, were\n\n### Why Remove Stop Words?\n\nThere are two main perspectives:\n\n**Reasons to remove stop words:**\n\n1. **Content analysis**: They don't tell us about the topic\n2. **Computational efficiency**: Fewer words = faster processing\n3. **Signal-to-noise**: They can overwhelm more informative words\n\n**Reasons to keep stop words:**\n\n1. **Stylometry**: They reveal personal writing style\n2. **Syntax**: Needed for parsing sentence structure\n3. **Meaning**: Sometimes they matter (\"not good\" â‰  \"good\")\n\n### Stop Words in Our Data\n\nLet's see what Python's spaCy library considers stop words:\n\n::: {#ec0d33f0 .cell execution_count=14}\n``` {.python .cell-code}\n# Load English language model (small version)\ntry:\n    nlp = spacy.load(\"en_core_web_sm\")\nexcept:\n    # If not installed, show installation instructions\n    print(\"Please install spaCy model:\")\n    print(\"!python -m spacy download en_core_web_sm\")\n    # For now, use a simple stopword list\n    from spacy.lang.en.stop_words import STOP_WORDS\n    stopwords_set = STOP_WORDS\nelse:\n    stopwords_set = nlp.Defaults.stop_words\n\nprint(f\"Number of stop words: {len(stopwords_set)}\")\nprint(f\"\\nFirst 30 stop words (alphabetically):\")\nprint(sorted(list(stopwords_set))[:30])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of stop words: 326\n\nFirst 30 stop words (alphabetically):\n[\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amount', 'an', 'and', 'another', 'any']\n```\n:::\n:::\n\n\n### Comparing with Our Most Frequent Words\n\n::: {#9e5beffe .cell execution_count=15}\n``` {.python .cell-code}\n# Check which top words are stop words\ntop_30 = word_freq.head(30).copy()\ntop_30['is_stopword'] = top_30['word'].isin(stopwords_set)\n\nprint(\"Top 30 words with stop word labels:\")\nprint(top_30[['word', 'count', 'is_stopword']])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTop 30 words with stop word labels:\n     word   count  is_stopword\n0     the  326862         True\n1      of  212531         True\n2      to  135329         True\n3     and  133944         True\n4      in   85772         True\n5       a   61992         True\n6    that   47130         True\n7     for   43079         True\n8      be   40521         True\n9     our   38759         True\n10     is   36752         True\n11     by   32429         True\n12     it   29271         True\n13     we   27276         True\n14   have   26784         True\n15   this   26594         True\n16     as   26510         True\n17   with   26400         True\n18  which   26237         True\n19   will   21913         True\n20     on   20689         True\n21      i   20687         True\n22    has   19896         True\n23    are   19619         True\n24   been   19135         True\n25    not   18597         True\n26  their   16784         True\n27   from   16055         True\n28     at   14991         True\n29    all   13608         True\n```\n:::\n:::\n\n\nNotice how most of the top frequent words are stop words!\n\n### Removing Stop Words\n\n::: {#78bea96f .cell execution_count=16}\n``` {.python .cell-code}\n# Filter out stop words\ncontent_words = word_freq[~word_freq['word'].isin(stopwords_set)].copy()\n\nprint(f\"Words before removing stop words: {len(word_freq):,}\")\nprint(f\"Words after removing stop words: {len(content_words):,}\")\nprint(f\"\\nTop 30 content words:\")\nprint(content_words.head(30))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWords before removing stop words: 68,356\nWords after removing stop words: 68,058\n\nTop 30 content words:\n            word  count  rank\n35    government  11209    36\n38        united  10158    39\n42        states   9524    43\n44      congress   8597    45\n52           new   7120    53\n56         great   6714    57\n59        public   6520    60\n62        people   6229    63\n66          year   5918    67\n69      american   5575    70\n72          time   5162    73\n76      national   4888    77\n81       country   4367    82\n82       present   4331    83\n88       federal   4093    89\n89         state   4073    90\n90         shall   4006    91\n91           war   3987    92\n99          work   3501   100\n100          act   3485   101\n102      foreign   3369   103\n103        years   3319   104\n105        power   3206   106\n106      general   3191   107\n107          law   3171   108\n108        world   3168   109\n112       system   2931   113\n116    necessary   2878   117\n117     increase   2818   118\n118  legislation   2790   119\n```\n:::\n:::\n\n\nNow we see more **content-bearing words**: government, states, congress, country, people, etc.\n\n### Visualizing Content Words\n\n::: {#1fd1092f .cell execution_count=17}\n``` {.python .cell-code}\n# Create word cloud from content words\nwordcloud_dict = dict(zip(content_words['word'].head(100), \n                          content_words['count'].head(100)))\n\nwordcloud = WordCloud(width=800, height=400, \n                      background_color='white',\n                      colormap='viridis').generate_from_frequencies(wordcloud_dict)\n\nplt.figure(figsize=(14, 7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Most Frequent Content Words in State of the Union Addresses', \n          fontsize=16, fontweight='bold', pad=20)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](lab_01_files/figure-html/cell-18-output-1.svg){}\n:::\n:::\n\n\n::: {.callout-tip}\n## ðŸ’¡ Preprocessing is a Choice\n\nWhether to remove stop words depends on your research question:\n\n- **Topic modeling**: Usually remove\n- **Sentiment analysis**: Keep (negations matter!)\n- **Stylometry**: Keep (they reveal style!)\n- **Document classification**: Test both\n:::\n\n---\n\n## Introduction to Stylometry\n\n### What is Stylometry?\n\n**Stylometry** is the statistical analysis of writing style. It's used to:\n\n- **Attribute authorship**: Who wrote this anonymous text?\n- **Detect plagiarism**: Did someone copy another's style?\n- **Study literary history**: How did an author's style evolve?\n- **Forensics**: Analyze threatening letters or disputed documents\n\n### The Surprising Power of Function Words\n\nYou might think content words (nouns, verbs) reveal writing style. But actually, **function words** (the stop words we just removed!) are more revealing because:\n\n1. **Unconscious use**: Authors don't think about them\n2. **High frequency**: Provide strong statistical signal  \n3. **Stable patterns**: Less affected by topic\n4. **Individual variation**: People use them differently\n\n### A Simple Example\n\nDifferent presidents might discuss the same topics but use different grammatical structures:\n\n- \"**We** must ensure...\" vs \"**I** believe we must...\"\n- \"This **is** important\" vs \"This **has been** important\"\n- \"**The** people\" vs \"**Our** people\"\n\nThese tiny differences add up to distinctive \"stylistic fingerprints.\"\n\n### Analyzing Presidential Style\n\nLet's look at how often different presidents use personal pronouns:\n\n::: {#27e319a0 .cell execution_count=18}\n``` {.python .cell-code}\n# Define personal pronouns\npersonal_pronouns = ['i', 'me', 'my', 'we', 'us', 'our', 'you', 'your']\n\n# Filter for pronouns only\npronoun_df = tokens_df[tokens_df['word'].isin(personal_pronouns)].copy()\n\n# Count by president\npronoun_by_president = (pronoun_df.groupby(['president', 'word'])\n                        .size()\n                        .reset_index(name='count'))\n\n# Calculate total words per president for normalization\nwords_per_president = tokens_df.groupby('president').size()\n\n# Normalize (calculate rate per 1000 words)\npronoun_by_president = pronoun_by_president.merge(\n    words_per_president.reset_index(name='total_words'),\n    on='president'\n)\npronoun_by_president['rate_per_1000'] = (\n    (pronoun_by_president['count'] / pronoun_by_president['total_words']) * 1000\n)\n\n# Show some examples\nprint(\"Sample of pronoun usage rates (per 1000 words):\")\nprint(pronoun_by_president[pronoun_by_president['president'].isin(\n    ['Donald J. Trump', 'Barack Obama', 'George W. Bush']\n)].head(15))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSample of pronoun usage rates (per 1000 words):\n          president  word  count  total_words  rate_per_1000\n24     Barack Obama     i    857       106566       8.041965\n25     Barack Obama    me    120       106566       1.126063\n26     Barack Obama    my    180       106566       1.689094\n27     Barack Obama   our   1811       106566      16.994163\n28     Barack Obama    us    287       106566       2.693167\n29     Barack Obama    we   1927       106566      18.082691\n30     Barack Obama   you    349       106566       3.274966\n31     Barack Obama  your    113       106566       1.060376\n56  Donald J. Trump     i    105        15058       6.973038\n57  Donald J. Trump    me      5        15058       0.332049\n58  Donald J. Trump    my     37        15058       2.457166\n59  Donald J. Trump   our    331        15058      21.981671\n60  Donald J. Trump    us     55        15058       3.652543\n61  Donald J. Trump    we    319        15058      21.184752\n62  Donald J. Trump   you     35        15058       2.324346\n```\n:::\n:::\n\n\n### Visualizing Style Differences\n\nLet's compare how different presidents use \"I\" vs \"we\":\n\n::: {#9762dc5a .cell execution_count=19}\n``` {.python .cell-code}\n# Focus on 'I' and 'we'\ni_we_df = pronoun_by_president[pronoun_by_president['word'].isin(['i', 'we'])].copy()\n\n# Pivot for easier plotting\ni_we_pivot = i_we_df.pivot(index='president', columns='word', values='rate_per_1000').fillna(0)\n\n# Get presidents with most speeches for clearer visualization\ntop_presidents = (tokens_df['president']\n                  .value_counts()\n                  .head(10)\n                  .index)\n\ni_we_plot = i_we_pivot.loc[i_we_pivot.index.isin(top_presidents)]\n\n# Create scatter plot\nplt.figure(figsize=(10, 8))\nplt.scatter(i_we_plot['i'], i_we_plot['we'], s=100, alpha=0.6)\n\n# Label points\nfor idx, row in i_we_plot.iterrows():\n    # Shorten long names\n    name = idx.split()[-1]  # Just last name\n    plt.annotate(name, (row['i'], row['we']), \n                xytext=(5, 5), textcoords='offset points', fontsize=9)\n\nplt.xlabel('Rate of \"I\" (per 1000 words)', fontsize=12)\nplt.ylabel('Rate of \"we\" (per 1000 words)', fontsize=12)\nplt.title('Presidential Pronoun Usage: \"I\" vs \"We\"', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](lab_01_files/figure-html/cell-20-output-1.svg){}\n:::\n:::\n\n\n::: {.callout-note}\n## ðŸ“Œ What This Tells Us\n\nPresidents who use \"I\" more often might be:\n\n- Speaking in a more personal style\n- Taking individual responsibility\n- Modern era (contemporary style)\n\nPresidents who use \"we\" more might be:\n\n- Emphasizing collective action\n- Speaking for the nation\n- Earlier era (formal style)\n\nThese patterns can distinguish authors even when topics overlap!\n:::\n\n### The Role of PCA (Principal Component Analysis)\n\nIn a full stylometric analysis, we would:\n\n1. Count many function words (not just pronouns)\n2. Have dozens or hundreds of features\n3. Need to reduce complexity to visualize patterns\n\n**PCA** (Principal Component Analysis) helps by:\n\n- Finding the main \"directions\" of variation\n- Reducing many features to 2-3 dimensions\n- Allowing us to plot and compare texts\n\n**What PCA gives us (practically):**\n\n- A 2D plot where similar authors cluster together\n- Ability to spot outliers or disputed authorship\n- Quantified measure of stylistic distance\n\nWe won't dive into the mathematics here, but know that PCA is a standard tool for reducing complex data to interpretable patterns.\n\n---\n\n## Summary and Key Takeaways\n\n### What We Learned\n\nToday we covered the fundamental workflow of computational text analysis:\n\n1. **Load text data** â†’ Working with structured formats (CSV, DataFrame)\n2. **Tokenize** â†’ Split text into words (or other units)\n3. **Count frequencies** â†’ Transform text into numbers\n4. **Explore patterns** â†’ Zipf's Law, frequency distributions\n5. **Preprocess** â†’ Remove stop words (when appropriate)\n6. **Analyze style** â†’ Use function words for stylometry\n\n### Key Concepts\n\n**Tokenization**\n: Splitting text into units (words, characters, sentences)\n\n**Bag of Words**\n: Treating text as unordered collection of words\n\n**Zipf's Law**\n: Word frequency follows power law distribution (few common, many rare)\n\n**Stop Words**\n: High-frequency grammatical words with little content\n\n**Stylometry**\n: Statistical analysis of writing style using function words\n\n**Preprocessing**\n: Transforming raw text for analysis (lowercase, remove punctuation, etc.)\n\n### Tools in Your Toolkit\n\n| Task | Python Tool |\n|------|-------------|\n| Load data | `pandas.read_csv()` |\n| Tokenize | `str.split()` or spaCy |\n| Count frequencies | `Counter()` or `value_counts()` |\n| Remove stop words | spaCy stop word list |\n| Visualize | matplotlib, seaborn, wordcloud |\n\n### Next Steps\n\nIn future labs, we'll explore:\n\n- More sophisticated tokenization (handling punctuation, contractions)\n- N-grams (sequences of words)\n- TF-IDF weighting (smarter than raw counts)\n\n---\n\n## Exercises\n\n### Exercise 1: Basic Frequency Analysis\n\nPick any president from the dataset and:\n\n1. Extract all their speeches\n2. Tokenize and count word frequencies\n3. Create a bar plot of their top 20 words\n4. Create a word cloud\n\n### Exercise 2: Stop Word Impact\n\nCompare word frequencies with and without stop words:\n\n1. Calculate top 50 words with stop words\n2. Calculate top 50 words without stop words\n3. How many overlap? What changes?\n\n### Exercise 3: Pronoun Style\n\nChoose three presidents and compare their use of:\n\n- First person singular (\"I\", \"me\", \"my\")\n- First person plural (\"we\", \"us\", \"our\")\n\nCreate a visualization showing the differences.\n\n### Exercise 4: Historical Change (Advanced)\n\nCompare speeches before and after 1950:\n\n1. Split the dataset into two time periods\n2. Calculate word frequencies for each period\n3. Identify words that became more/less common\n4. Create a Zipf's Law plot for each period\n\n---\n\n## References and Further Reading\n\n### Academic Papers\n- Piantadosi, S. T. (2014). Zipfâ€™s word frequency law in natural language: A critical review and future directions. Psychonomic Bulletin & Review, 21(5), 1112â€“1130. [https://doi.org/10.3758/s13423-014-0585-6](https://doi.org/10.3758/s13423-014-0585-6)\n- Jurafsky, D., & Martin, J. H. (2023). *Speech and Language Processing* (3rd ed., draft) [https://web.stanford.edu/~jurafsky/slp3/](https://web.stanford.edu/~jurafsky/slp3/)\n\n### Tutorials and Books\n- Silge, J., & Robinson, D. (2017). *Text Mining with R* (applicable\n  concepts) [https://www.tidytextmining.com/](https://www.tidytextmining.com/)\n- Bird, S., Klein, E., & Loper, E. (2009). *Natural Language\n  Processing with Python*. [https://www.nltk.org/book/](https://www.nltk.org/book/) (a bit dated,\n  but good)\n\n### Documentation\n- [pandas documentation](https://pandas.pydata.org/)\n- [spaCy documentation](https://spacy.io/)\n- [seaborn documentation](https://seaborn.pydata.org/)\n\n---\n\n**End of Lab 01**\n\n",
    "supporting": [
      "lab_01_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}