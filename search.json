[
  {
    "objectID": "lab_01.html",
    "href": "lab_01.html",
    "title": "Lab 01: Words as data points I",
    "section": "",
    "text": "By the end of this lab, you will understand:\n\nHow to transform unstructured text into structured data\nWhat tokenization is and why it matters\nHow word frequencies reveal patterns in text\nZipf‚Äôs Law and its implications for text analysis\nWhat stop words are and when to remove them\nThe basics of text preprocessing\nWhat stylometry is and how function words reveal authorship\nHow to use Python and pandas for basic text analysis"
  },
  {
    "objectID": "lab_01.html#learning-objectives",
    "href": "lab_01.html#learning-objectives",
    "title": "Lab 01: Words as data points I",
    "section": "",
    "text": "By the end of this lab, you will understand:\n\nHow to transform unstructured text into structured data\nWhat tokenization is and why it matters\nHow word frequencies reveal patterns in text\nZipf‚Äôs Law and its implications for text analysis\nWhat stop words are and when to remove them\nThe basics of text preprocessing\nWhat stylometry is and how function words reveal authorship\nHow to use Python and pandas for basic text analysis"
  },
  {
    "objectID": "lab_01.html#introduction-why-text-as-data",
    "href": "lab_01.html#introduction-why-text-as-data",
    "title": "Lab 01: Words as data points I",
    "section": "2 Introduction: Why Text as Data?",
    "text": "2 Introduction: Why Text as Data?\nText is everywhere: social media posts, news articles, scientific papers, government documents, customer reviews. But text in its raw form is unstructured - it‚Äôs just sequences of characters. To analyze text computationally, we need to transform it into structured data that computers can process mathematically.\nThink of it this way: if you wanted to compare two novels, you could read both and form an impression. But what if you had 1,000 novels? Or 100,000 tweets? This is where computational text analysis becomes essential.\n\n2.1 Our Dataset: State of the Union Addresses\nToday we‚Äôll work with a collection of State of the Union addresses - speeches given by US presidents from the 18th century to 2018. These speeches are:\n\nHistorical: Spanning over 200 years\nPolitical: Reflecting different eras and priorities\nComparable: Same genre, same occasion, different authors\n\nThis makes them perfect for learning text analysis techniques.\nData source: Brian Weinstein‚Äôs State of the Union corpus"
  },
  {
    "objectID": "lab_01.html#setup-loading-packages",
    "href": "lab_01.html#setup-loading-packages",
    "title": "Lab 01: Words as data points I",
    "section": "3 Setup: Loading Packages",
    "text": "3 Setup: Loading Packages\nFirst, let‚Äôs load the Python packages we‚Äôll need:\n# you will need to run it once after installing spaCy\n!python -m spacy download en_core_web_sm\n\n# Data manipulation\nimport pandas as pd\n\n# Text processing\nimport spacy\nfrom collections import Counter\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# For word clouds\nfrom wordcloud import WordCloud\n\n# Set visualization style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\nprint(\"‚úì Packages loaded successfully\")\n\n‚úì Packages loaded successfully\n\n\n\n\n\n\n\n\nNoteüìå About these packages\n\n\n\n\npandas: Works with tabular data (like spreadsheets)\nspaCy: Processes natural language text\nmatplotlib/seaborn: Create visualizations\nwordcloud: Generate word cloud visualizations"
  },
  {
    "objectID": "lab_01.html#loading-and-exploring-the-data",
    "href": "lab_01.html#loading-and-exploring-the-data",
    "title": "Lab 01: Words as data points I",
    "section": "4 Loading and Exploring the Data",
    "text": "4 Loading and Exploring the Data\nLet‚Äôs load our dataset:\n\n# Load the data\nspeeches = pd.read_csv(\"data/transcripts.csv\")\n\n# Display first few rows\nprint(\"Dataset shape:\", speeches.shape)\nprint(\"\\nFirst few rows:\")\nspeeches.head()\n\nDataset shape: (244, 5)\n\nFirst few rows:\n\n\n\n\n\n\n\n\n\ndate\npresident\ntitle\nurl\ntranscript\n\n\n\n\n0\n2018-01-30\nDonald J. Trump\nAddress Before a Joint Session of the Congress...\nhttps://www.cnn.com/2018/01/30/politics/2018-s...\n\\nMr. Speaker, Mr. Vice President, Members of ...\n\n\n1\n2017-02-28\nDonald J. Trump\nAddress Before a Joint Session of the Congress\nhttp://www.presidency.ucsb.edu/ws/index.php?pi...\nThank you very much. Mr. Speaker, Mr. Vice Pre...\n\n\n2\n2016-01-12\nBarack Obama\nAddress Before a Joint Session of the Congress...\nhttp://www.presidency.ucsb.edu/ws/index.php?pi...\nThank you. Mr. Speaker, Mr. Vice President, Me...\n\n\n3\n2015-01-20\nBarack Obama\nAddress Before a Joint Session of the Congress...\nhttp://www.presidency.ucsb.edu/ws/index.php?pi...\nThe President. Mr. Speaker, Mr. Vice President...\n\n\n4\n2014-01-28\nBarack Obama\nAddress Before a Joint Session of the Congress...\nhttp://www.presidency.ucsb.edu/ws/index.php?pi...\nThe President. Mr. Speaker, Mr. Vice President...\n\n\n\n\n\n\n\n\n4.1 Understanding the Data Structure\nOur data is in tabular format - like a spreadsheet with rows and columns:\n\n# Check column names and types\nprint(\"Columns:\")\nprint(speeches.dtypes)\n\n# Basic statistics\nprint(\"\\nNumber of speeches:\", len(speeches))\nprint(\"Number of presidents:\", speeches['president'].nunique())\nprint(\"\\nDate range:\")\nprint(\"  Earliest:\", speeches['date'].min())\nprint(\"  Latest:\", speeches['date'].max())\n\nColumns:\ndate          object\npresident     object\ntitle         object\nurl           object\ntranscript    object\ndtype: object\n\nNumber of speeches: 244\nNumber of presidents: 42\n\nDate range:\n  Earliest: 1790-01-08\n  Latest: 2018-01-30\n\n\nEach row represents one speech. The columns contain:\n\ndate: When the speech was given\npresident: Who gave it\ntitle: The speech‚Äôs official title\nurl: Where the transcript came from\ntranscript: The actual text of the speech\n\nLet‚Äôs look at a short excerpt:\n\n# Display a snippet of one speech\nsample_text = speeches.loc[0, 'transcript'][:500]  # First 500 characters\nprint(\"Sample from first speech:\")\nprint(sample_text)\nprint(\"...\")\n\nSample from first speech:\n\nMr. Speaker, Mr. Vice President, Members of Congress, the First Lady of the United States, and my fellow Americans:\nLess than 1 year has passed since I first stood at this podium, in this majestic chamber, to speak on behalf of the American People -- and to address their concerns, their hopes, and their dreams.  That night, our new Administration had already taken swift action.  A new tide of optimism was already sweeping across our land.\nEach day since, we have gone forward with a clear vision\n...\n\n\n\n\n\n\n\n\nTipüí° Why This Structure?\n\n\n\nText data commonly comes with metadata (data about data):\n\nAuthor, date, source, title, etc.\nThis metadata helps us compare and analyze texts\nWe keep text and metadata together in a table"
  },
  {
    "objectID": "lab_01.html#from-text-to-tokens---the-bag-of-words-model",
    "href": "lab_01.html#from-text-to-tokens---the-bag-of-words-model",
    "title": "Lab 01: Words as data points I",
    "section": "5 From Text to Tokens - The Bag of Words Model",
    "text": "5 From Text to Tokens - The Bag of Words Model\n\n5.1 The Challenge: Text is Unstructured\nRight now, our transcript column contains long strings of text. How do we measure or compare them? We can‚Äôt easily do math on text!\n\n\n5.2 The Solution: Tokenization\nTokenization is the process of splitting text into smaller units called tokens. Usually, tokens are words, but they could also be:\n\nCharacters (letters)\nSentences\nN-grams (sequences of N words)\n\nLet‚Äôs see this in action:\n\n# Simple example: split text into words\nexample = \"Python is great for text analysis\"\nwords = example.split()\nprint(\"Original text:\", example)\nprint(\"Tokens (words):\", words)\nprint(\"Number of tokens:\", len(words))\n\nOriginal text: Python is great for text analysis\nTokens (words): ['Python', 'is', 'great', 'for', 'text', 'analysis']\nNumber of tokens: 6\n\n\n\n\n5.3 Tokenizing Our Speeches\nNow let‚Äôs tokenize one full speech:\n\n# Get one speech\nspeech_text = speeches.loc[0, 'transcript']\n\n# Simple tokenization (just splitting on spaces)\nsimple_tokens = speech_text.split()\n\nprint(f\"Speech length: {len(speech_text)} characters\")\nprint(f\"Number of tokens: {len(simple_tokens)}\")\nprint(f\"\\nFirst 20 tokens:\")\nprint(simple_tokens[:20])\n\nSpeech length: 30354 characters\nNumber of tokens: 5190\n\nFirst 20 tokens:\n['Mr.', 'Speaker,', 'Mr.', 'Vice', 'President,', 'Members', 'of', 'Congress,', 'the', 'First', 'Lady', 'of', 'the', 'United', 'States,', 'and', 'my', 'fellow', 'Americans:', 'Less']\n\n\n\n\n5.4 The Bag of Words Model\nOnce we have tokens, we can treat text as a ‚Äúbag of words‚Äù - we:\n\nIgnore word order (‚Äúcat dog‚Äù = ‚Äúdog cat‚Äù)\nCount word frequencies\nRepresent text as numbers\n\nThis might seem like we‚Äôre throwing away information (word order matters!), but this simple model is surprisingly powerful for many tasks.\nLet‚Äôs create a bag of words for one speech:\n\n# Count word frequencies\nword_counts = Counter(simple_tokens)\n\n# Show most common words\nprint(\"Top 15 most frequent words:\")\nfor word, count in word_counts.most_common(15):\n    print(f\"  {word}: {count}\")\n\nTop 15 most frequent words:\n  the: 215\n  and: 184\n  to: 175\n  of: 121\n  our: 95\n  we: 88\n  a: 79\n  in: 72\n  is: 61\n  are: 48\n  that: 45\n  --: 44\n  have: 40\n  for: 34\n  will: 34\n\n\n\n\n5.5 Two Data Formats: Long vs Wide\nWe can represent tokenized text in two ways:\nLong format: One row per word occurrence\npresident    word\nTrump        the\nTrump        economy\nTrump        is\nTrump        strong\nWide format: One row per document, one column per unique word\npresident    the    economy    is    strong\nTrump        150    12         89    5\nFor now, we‚Äôll work with long format because it‚Äôs easier to understand and manipulate."
  },
  {
    "objectID": "lab_01.html#tokenizing-all-speeches",
    "href": "lab_01.html#tokenizing-all-speeches",
    "title": "Lab 01: Words as data points I",
    "section": "6 Tokenizing All Speeches",
    "text": "6 Tokenizing All Speeches\nLet‚Äôs tokenize all speeches and create a long-format dataset:\n\n# Initialize empty list to store results\nall_tokens = []\n\n# Process each speech\nfor idx, row in speeches.iterrows():\n    president = row['president']\n    text = row['transcript']\n    \n    # Simple tokenization\n    tokens = text.lower().split()  # Convert to lowercase\n    \n    # Add each token to our list\n    for token in tokens:\n        all_tokens.append({\n            'president': president,\n            'word': token\n        })\n\n# Convert to DataFrame\ntokens_df = pd.DataFrame(all_tokens)\n\nprint(f\"Total number of tokens: {len(tokens_df):,}\")\nprint(f\"\\nFirst few rows:\")\ntokens_df.head(10)\n\nTotal number of tokens: 3,947,946\n\nFirst few rows:\n\n\n\n\n\n\n\n\n\npresident\nword\n\n\n\n\n0\nDonald J. Trump\nmr.\n\n\n1\nDonald J. Trump\nspeaker,\n\n\n2\nDonald J. Trump\nmr.\n\n\n3\nDonald J. Trump\nvice\n\n\n4\nDonald J. Trump\npresident,\n\n\n5\nDonald J. Trump\nmembers\n\n\n6\nDonald J. Trump\nof\n\n\n7\nDonald J. Trump\ncongress,\n\n\n8\nDonald J. Trump\nthe\n\n\n9\nDonald J. Trump\nfirst\n\n\n\n\n\n\n\n\n6.1 How many unique words?\n\nunique_words = tokens_df['word'].nunique()\nprint(f\"Number of unique words: {unique_words:,}\")\n\nNumber of unique words: 68,356\n\n\nThat‚Äôs a lot of unique words! But are they all useful?"
  },
  {
    "objectID": "lab_01.html#word-frequencies",
    "href": "lab_01.html#word-frequencies",
    "title": "Lab 01: Words as data points I",
    "section": "7 Word Frequencies",
    "text": "7 Word Frequencies\nNow let‚Äôs count how often each word appears:\n\n# Count word frequencies\nword_freq = tokens_df['word'].value_counts().reset_index()\nword_freq.columns = ['word', 'count']\n\nprint(\"Top 20 most frequent words:\")\nprint(word_freq.head(20))\n\nTop 20 most frequent words:\n     word   count\n0     the  326862\n1      of  212531\n2      to  135329\n3     and  133944\n4      in   85772\n5       a   61992\n6    that   47130\n7     for   43079\n8      be   40521\n9     our   38759\n10     is   36752\n11     by   32429\n12     it   29271\n13     we   27276\n14   have   26784\n15   this   26594\n16     as   26510\n17   with   26400\n18  which   26237\n19   will   21913\n\n\n\n7.1 Visualizing Frequency\n\n# Plot top 15 words\ntop_15 = word_freq.head(15)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(data=top_15, y='word', x='count', palette='viridis')\nplt.title('Top 15 Most Frequent Words in State of the Union Addresses', fontsize=14, fontweight='bold')\nplt.xlabel('Frequency (count)', fontsize=12)\nplt.ylabel('Word', fontsize=12)\nplt.tight_layout()\nplt.show()\n\n/tmp/ipykernel_1863094/2171022789.py:5: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=top_15, y='word', x='count', palette='viridis')\n\n\n\n\n\n\n\n\n\n\n\n7.2 What do we notice?\nThe most frequent words are:\n\nArticles: ‚Äúthe‚Äù, ‚Äúa‚Äù, ‚Äúan‚Äù\nPrepositions: ‚Äúof‚Äù, ‚Äúto‚Äù, ‚Äúin‚Äù, ‚Äúfor‚Äù\nConjunctions: ‚Äúand‚Äù, ‚Äúbut‚Äù, ‚Äúor‚Äù\n\nThese are grammatical words that appear in almost any English text. They don‚Äôt tell us much about the content of the speeches!\nThis observation leads us to an important concept‚Ä¶"
  },
  {
    "objectID": "lab_01.html#zipfs-law---a-fundamental-pattern",
    "href": "lab_01.html#zipfs-law---a-fundamental-pattern",
    "title": "Lab 01: Words as data points I",
    "section": "8 Zipf‚Äôs Law - A Fundamental Pattern",
    "text": "8 Zipf‚Äôs Law - A Fundamental Pattern\n\n8.1 What is Zipf‚Äôs Law?\nIf you count word frequencies in any large collection of text and rank words by frequency, you‚Äôll observe something remarkable:\nThe most common word appears roughly twice as often as the second most common word, three times as often as the third most common, and so on.\nThis is called Zipf‚Äôs Law (named after linguist George Kingsley Zipf).\n\n\n8.2 Why Does This Matter?\nZipf‚Äôs Law tells us that:\n\nA few words are very common (‚Äúthe‚Äù, ‚Äúof‚Äù, ‚Äúand‚Äù)\nMost words are very rare (appear only once or twice)\nThis pattern is universal - it appears in all languages and genres\n\nThis has practical implications:\n\nMost words provide little statistical power (they‚Äôre too rare)\nA small vocabulary covers most of any text\nWe need strategies to deal with this imbalance\n\n\n\n8.3 Visualizing Zipf‚Äôs Law\nLet‚Äôs see if our data follows Zipf‚Äôs Law:\n\n# Add rank to our frequency table\nword_freq['rank'] = range(1, len(word_freq) + 1)\n\n# Create log-log plot\nplt.figure(figsize=(10, 6))\nplt.loglog(word_freq['rank'], word_freq['count'], 'b.')\nplt.xlabel('Rank (log scale)', fontsize=12)\nplt.ylabel('Frequency (log scale)', fontsize=12)\nplt.title(\"Zipf's Law in State of the Union Addresses\", fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe nearly straight line on a log-log plot confirms Zipf‚Äôs Law!\n\n\n8.4 The Long Tail\nLet‚Äôs look at how many words appear only once or twice:\n\n# Count words by frequency\nfreq_distribution = word_freq['count'].value_counts().sort_index()\n\nprint(\"Distribution of word frequencies:\")\nprint(f\"  Words appearing once: {freq_distribution.get(1, 0):,}\")\nprint(f\"  Words appearing twice: {freq_distribution.get(2, 0):,}\")\nprint(f\"  Words appearing 3-5 times: {freq_distribution.loc[3:5].sum():,}\")\nprint(f\"  Words appearing 6-10 times: {freq_distribution.loc[6:10].sum():,}\")\nprint(f\"  Words appearing &gt; 100 times: {(word_freq['count'] &gt; 100).sum():,}\")\n\nDistribution of word frequencies:\n  Words appearing once: 9,835\n  Words appearing twice: 27,310\n  Words appearing 3-5 times: 8,157\n  Words appearing 6-10 times: 7,957\n  Words appearing &gt; 100 times: 3,484\n\n\nThis is the ‚Äúlong tail‚Äù - many rare words, few common words.\n\n\n\n\n\n\nImportantüéØ Key Insight\n\n\n\nZipf‚Äôs Law means that word frequencies are highly skewed. This affects how we:\n\nBuild statistical models\nSelect features for machine learning\nPreprocess text\nInterpret results"
  },
  {
    "objectID": "lab_01.html#stop-words-and-preprocessing",
    "href": "lab_01.html#stop-words-and-preprocessing",
    "title": "Lab 01: Words as data points I",
    "section": "9 Stop Words and Preprocessing",
    "text": "9 Stop Words and Preprocessing\n\n9.1 What are Stop Words?\nStop words are extremely common words that appear frequently in almost any text:\n\nArticles: the, a, an\nPrepositions: in, on, at, to, from\nPronouns: I, you, he, she, it\nConjunctions: and, but, or\nAuxiliary verbs: is, are, was, were\n\n\n\n9.2 Why Remove Stop Words?\nThere are two main perspectives:\nReasons to remove stop words:\n\nContent analysis: They don‚Äôt tell us about the topic\nComputational efficiency: Fewer words = faster processing\nSignal-to-noise: They can overwhelm more informative words\n\nReasons to keep stop words:\n\nStylometry: They reveal personal writing style\nSyntax: Needed for parsing sentence structure\nMeaning: Sometimes they matter (‚Äúnot good‚Äù ‚â† ‚Äúgood‚Äù)\n\n\n\n9.3 Stop Words in Our Data\nLet‚Äôs see what Python‚Äôs spaCy library considers stop words:\n\n# Load English language model (small version)\ntry:\n    nlp = spacy.load(\"en_core_web_sm\")\nexcept:\n    # If not installed, show installation instructions\n    print(\"Please install spaCy model:\")\n    print(\"!python -m spacy download en_core_web_sm\")\n    # For now, use a simple stopword list\n    from spacy.lang.en.stop_words import STOP_WORDS\n    stopwords_set = STOP_WORDS\nelse:\n    stopwords_set = nlp.Defaults.stop_words\n\nprint(f\"Number of stop words: {len(stopwords_set)}\")\nprint(f\"\\nFirst 30 stop words (alphabetically):\")\nprint(sorted(list(stopwords_set))[:30])\n\nNumber of stop words: 326\n\nFirst 30 stop words (alphabetically):\n[\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amount', 'an', 'and', 'another', 'any']\n\n\n\n\n9.4 Comparing with Our Most Frequent Words\n\n# Check which top words are stop words\ntop_30 = word_freq.head(30).copy()\ntop_30['is_stopword'] = top_30['word'].isin(stopwords_set)\n\nprint(\"Top 30 words with stop word labels:\")\nprint(top_30[['word', 'count', 'is_stopword']])\n\nTop 30 words with stop word labels:\n     word   count  is_stopword\n0     the  326862         True\n1      of  212531         True\n2      to  135329         True\n3     and  133944         True\n4      in   85772         True\n5       a   61992         True\n6    that   47130         True\n7     for   43079         True\n8      be   40521         True\n9     our   38759         True\n10     is   36752         True\n11     by   32429         True\n12     it   29271         True\n13     we   27276         True\n14   have   26784         True\n15   this   26594         True\n16     as   26510         True\n17   with   26400         True\n18  which   26237         True\n19   will   21913         True\n20     on   20689         True\n21      i   20687         True\n22    has   19896         True\n23    are   19619         True\n24   been   19135         True\n25    not   18597         True\n26  their   16784         True\n27   from   16055         True\n28     at   14991         True\n29    all   13608         True\n\n\nNotice how most of the top frequent words are stop words!\n\n\n9.5 Removing Stop Words\n\n# Filter out stop words\ncontent_words = word_freq[~word_freq['word'].isin(stopwords_set)].copy()\n\nprint(f\"Words before removing stop words: {len(word_freq):,}\")\nprint(f\"Words after removing stop words: {len(content_words):,}\")\nprint(f\"\\nTop 30 content words:\")\nprint(content_words.head(30))\n\nWords before removing stop words: 68,356\nWords after removing stop words: 68,058\n\nTop 30 content words:\n            word  count  rank\n35    government  11209    36\n38        united  10158    39\n42        states   9524    43\n44      congress   8597    45\n52           new   7120    53\n56         great   6714    57\n59        public   6520    60\n62        people   6229    63\n66          year   5918    67\n69      american   5575    70\n72          time   5162    73\n76      national   4888    77\n81       country   4367    82\n82       present   4331    83\n88       federal   4093    89\n89         state   4073    90\n90         shall   4006    91\n91           war   3987    92\n99          work   3501   100\n100          act   3485   101\n102      foreign   3369   103\n103        years   3319   104\n105        power   3206   106\n106      general   3191   107\n107          law   3171   108\n108        world   3168   109\n112       system   2931   113\n116    necessary   2878   117\n117     increase   2818   118\n118  legislation   2790   119\n\n\nNow we see more content-bearing words: government, states, congress, country, people, etc.\n\n\n9.6 Visualizing Content Words\n\n# Create word cloud from content words\nwordcloud_dict = dict(zip(content_words['word'].head(100), \n                          content_words['count'].head(100)))\n\nwordcloud = WordCloud(width=800, height=400, \n                      background_color='white',\n                      colormap='viridis').generate_from_frequencies(wordcloud_dict)\n\nplt.figure(figsize=(14, 7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Most Frequent Content Words in State of the Union Addresses', \n          fontsize=16, fontweight='bold', pad=20)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipüí° Preprocessing is a Choice\n\n\n\nWhether to remove stop words depends on your research question:\n\nTopic modeling: Usually remove\nSentiment analysis: Keep (negations matter!)\nStylometry: Keep (they reveal style!)\nDocument classification: Test both"
  },
  {
    "objectID": "lab_01.html#introduction-to-stylometry",
    "href": "lab_01.html#introduction-to-stylometry",
    "title": "Lab 01: Words as data points I",
    "section": "10 Introduction to Stylometry",
    "text": "10 Introduction to Stylometry\n\n10.1 What is Stylometry?\nStylometry is the statistical analysis of writing style. It‚Äôs used to:\n\nAttribute authorship: Who wrote this anonymous text?\nDetect plagiarism: Did someone copy another‚Äôs style?\nStudy literary history: How did an author‚Äôs style evolve?\nForensics: Analyze threatening letters or disputed documents\n\n\n\n10.2 The Surprising Power of Function Words\nYou might think content words (nouns, verbs) reveal writing style. But actually, function words (the stop words we just removed!) are more revealing because:\n\nUnconscious use: Authors don‚Äôt think about them\nHigh frequency: Provide strong statistical signal\n\nStable patterns: Less affected by topic\nIndividual variation: People use them differently\n\n\n\n10.3 A Simple Example\nDifferent presidents might discuss the same topics but use different grammatical structures:\n\n‚ÄúWe must ensure‚Ä¶‚Äù vs ‚ÄúI believe we must‚Ä¶‚Äù\n‚ÄúThis is important‚Äù vs ‚ÄúThis has been important‚Äù\n‚ÄúThe people‚Äù vs ‚ÄúOur people‚Äù\n\nThese tiny differences add up to distinctive ‚Äústylistic fingerprints.‚Äù\n\n\n10.4 Analyzing Presidential Style\nLet‚Äôs look at how often different presidents use personal pronouns:\n\n# Define personal pronouns\npersonal_pronouns = ['i', 'me', 'my', 'we', 'us', 'our', 'you', 'your']\n\n# Filter for pronouns only\npronoun_df = tokens_df[tokens_df['word'].isin(personal_pronouns)].copy()\n\n# Count by president\npronoun_by_president = (pronoun_df.groupby(['president', 'word'])\n                        .size()\n                        .reset_index(name='count'))\n\n# Calculate total words per president for normalization\nwords_per_president = tokens_df.groupby('president').size()\n\n# Normalize (calculate rate per 1000 words)\npronoun_by_president = pronoun_by_president.merge(\n    words_per_president.reset_index(name='total_words'),\n    on='president'\n)\npronoun_by_president['rate_per_1000'] = (\n    (pronoun_by_president['count'] / pronoun_by_president['total_words']) * 1000\n)\n\n# Show some examples\nprint(\"Sample of pronoun usage rates (per 1000 words):\")\nprint(pronoun_by_president[pronoun_by_president['president'].isin(\n    ['Donald J. Trump', 'Barack Obama', 'George W. Bush']\n)].head(15))\n\nSample of pronoun usage rates (per 1000 words):\n          president  word  count  total_words  rate_per_1000\n24     Barack Obama     i    857       106566       8.041965\n25     Barack Obama    me    120       106566       1.126063\n26     Barack Obama    my    180       106566       1.689094\n27     Barack Obama   our   1811       106566      16.994163\n28     Barack Obama    us    287       106566       2.693167\n29     Barack Obama    we   1927       106566      18.082691\n30     Barack Obama   you    349       106566       3.274966\n31     Barack Obama  your    113       106566       1.060376\n56  Donald J. Trump     i    105        15058       6.973038\n57  Donald J. Trump    me      5        15058       0.332049\n58  Donald J. Trump    my     37        15058       2.457166\n59  Donald J. Trump   our    331        15058      21.981671\n60  Donald J. Trump    us     55        15058       3.652543\n61  Donald J. Trump    we    319        15058      21.184752\n62  Donald J. Trump   you     35        15058       2.324346\n\n\n\n\n10.5 Visualizing Style Differences\nLet‚Äôs compare how different presidents use ‚ÄúI‚Äù vs ‚Äúwe‚Äù:\n\n# Focus on 'I' and 'we'\ni_we_df = pronoun_by_president[pronoun_by_president['word'].isin(['i', 'we'])].copy()\n\n# Pivot for easier plotting\ni_we_pivot = i_we_df.pivot(index='president', columns='word', values='rate_per_1000').fillna(0)\n\n# Get presidents with most speeches for clearer visualization\ntop_presidents = (tokens_df['president']\n                  .value_counts()\n                  .head(10)\n                  .index)\n\ni_we_plot = i_we_pivot.loc[i_we_pivot.index.isin(top_presidents)]\n\n# Create scatter plot\nplt.figure(figsize=(10, 8))\nplt.scatter(i_we_plot['i'], i_we_plot['we'], s=100, alpha=0.6)\n\n# Label points\nfor idx, row in i_we_plot.iterrows():\n    # Shorten long names\n    name = idx.split()[-1]  # Just last name\n    plt.annotate(name, (row['i'], row['we']), \n                xytext=(5, 5), textcoords='offset points', fontsize=9)\n\nplt.xlabel('Rate of \"I\" (per 1000 words)', fontsize=12)\nplt.ylabel('Rate of \"we\" (per 1000 words)', fontsize=12)\nplt.title('Presidential Pronoun Usage: \"I\" vs \"We\"', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteüìå What This Tells Us\n\n\n\nPresidents who use ‚ÄúI‚Äù more often might be:\n\nSpeaking in a more personal style\nTaking individual responsibility\nModern era (contemporary style)\n\nPresidents who use ‚Äúwe‚Äù more might be:\n\nEmphasizing collective action\nSpeaking for the nation\nEarlier era (formal style)\n\nThese patterns can distinguish authors even when topics overlap!\n\n\n\n\n10.6 The Role of PCA (Principal Component Analysis)\nIn a full stylometric analysis, we would:\n\nCount many function words (not just pronouns)\nHave dozens or hundreds of features\nNeed to reduce complexity to visualize patterns\n\nPCA (Principal Component Analysis) helps by:\n\nFinding the main ‚Äúdirections‚Äù of variation\nReducing many features to 2-3 dimensions\nAllowing us to plot and compare texts\n\nWhat PCA gives us (practically):\n\nA 2D plot where similar authors cluster together\nAbility to spot outliers or disputed authorship\nQuantified measure of stylistic distance\n\nWe won‚Äôt dive into the mathematics here, but know that PCA is a standard tool for reducing complex data to interpretable patterns."
  },
  {
    "objectID": "lab_01.html#summary-and-key-takeaways",
    "href": "lab_01.html#summary-and-key-takeaways",
    "title": "Lab 01: Words as data points I",
    "section": "11 Summary and Key Takeaways",
    "text": "11 Summary and Key Takeaways\n\n11.1 What We Learned\nToday we covered the fundamental workflow of computational text analysis:\n\nLoad text data ‚Üí Working with structured formats (CSV, DataFrame)\nTokenize ‚Üí Split text into words (or other units)\nCount frequencies ‚Üí Transform text into numbers\nExplore patterns ‚Üí Zipf‚Äôs Law, frequency distributions\nPreprocess ‚Üí Remove stop words (when appropriate)\nAnalyze style ‚Üí Use function words for stylometry\n\n\n\n11.2 Key Concepts\n\nTokenization\n\nSplitting text into units (words, characters, sentences)\n\nBag of Words\n\nTreating text as unordered collection of words\n\nZipf‚Äôs Law\n\nWord frequency follows power law distribution (few common, many rare)\n\nStop Words\n\nHigh-frequency grammatical words with little content\n\nStylometry\n\nStatistical analysis of writing style using function words\n\nPreprocessing\n\nTransforming raw text for analysis (lowercase, remove punctuation, etc.)\n\n\n\n\n11.3 Tools in Your Toolkit\n\n\n\nTask\nPython Tool\n\n\n\n\nLoad data\npandas.read_csv()\n\n\nTokenize\nstr.split() or spaCy\n\n\nCount frequencies\nCounter() or value_counts()\n\n\nRemove stop words\nspaCy stop word list\n\n\nVisualize\nmatplotlib, seaborn, wordcloud\n\n\n\n\n\n11.4 Next Steps\nIn future labs, we‚Äôll explore:\n\nMore sophisticated tokenization (handling punctuation, contractions)\nN-grams (sequences of words)\nTF-IDF weighting (smarter than raw counts)"
  },
  {
    "objectID": "lab_01.html#exercises",
    "href": "lab_01.html#exercises",
    "title": "Lab 01: Words as data points I",
    "section": "12 Exercises",
    "text": "12 Exercises\n\n12.1 Exercise 1: Basic Frequency Analysis\nPick any president from the dataset and:\n\nExtract all their speeches\nTokenize and count word frequencies\nCreate a bar plot of their top 20 words\nCreate a word cloud\n\n\n\n12.2 Exercise 2: Stop Word Impact\nCompare word frequencies with and without stop words:\n\nCalculate top 50 words with stop words\nCalculate top 50 words without stop words\nHow many overlap? What changes?\n\n\n\n12.3 Exercise 3: Pronoun Style\nChoose three presidents and compare their use of:\n\nFirst person singular (‚ÄúI‚Äù, ‚Äúme‚Äù, ‚Äúmy‚Äù)\nFirst person plural (‚Äúwe‚Äù, ‚Äúus‚Äù, ‚Äúour‚Äù)\n\nCreate a visualization showing the differences.\n\n\n12.4 Exercise 4: Historical Change (Advanced)\nCompare speeches before and after 1950:\n\nSplit the dataset into two time periods\nCalculate word frequencies for each period\nIdentify words that became more/less common\nCreate a Zipf‚Äôs Law plot for each period"
  },
  {
    "objectID": "lab_01.html#references-and-further-reading",
    "href": "lab_01.html#references-and-further-reading",
    "title": "Lab 01: Words as data points I",
    "section": "13 References and Further Reading",
    "text": "13 References and Further Reading\n\n13.1 Academic Papers\n\nPiantadosi, S. T. (2014). Zipf‚Äôs word frequency law in natural language: A critical review and future directions. Psychonomic Bulletin & Review, 21(5), 1112‚Äì1130. https://doi.org/10.3758/s13423-014-0585-6\nJurafsky, D., & Martin, J. H. (2023). Speech and Language Processing (3rd ed., draft) https://web.stanford.edu/~jurafsky/slp3/\n\n\n\n13.2 Tutorials and Books\n\nSilge, J., & Robinson, D. (2017). Text Mining with R (applicable concepts) https://www.tidytextmining.com/\nBird, S., Klein, E., & Loper, E. (2009). Natural Language Processing with Python. https://www.nltk.org/book/ (a bit dated, but good)\n\n\n\n13.3 Documentation\n\npandas documentation\nspaCy documentation\nseaborn documentation\n\n\nEnd of Lab 01"
  },
  {
    "objectID": "basic-python.html",
    "href": "basic-python.html",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "",
    "text": "After completing this tutorial, you will be able to:\n\n‚úì Run Python code in Google Colab\n‚úì Read and understand Python code\n‚úì Work with strings for text analysis\n‚úì Use basic data structures (lists, dictionaries)\n‚úì Write simple functions and control flow\n‚úì Find help and documentation online\n‚úì Understand cloud vs.¬†local Python environments\n‚úì (Optional) Debug common errors"
  },
  {
    "objectID": "basic-python.html#learning-outcomes",
    "href": "basic-python.html#learning-outcomes",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "",
    "text": "After completing this tutorial, you will be able to:\n\n‚úì Run Python code in Google Colab\n‚úì Read and understand Python code\n‚úì Work with strings for text analysis\n‚úì Use basic data structures (lists, dictionaries)\n‚úì Write simple functions and control flow\n‚úì Find help and documentation online\n‚úì Understand cloud vs.¬†local Python environments\n‚úì (Optional) Debug common errors"
  },
  {
    "objectID": "basic-python.html#getting-started-with-google-colab",
    "href": "basic-python.html#getting-started-with-google-colab",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "1. Getting Started with Google Colab",
    "text": "1. Getting Started with Google Colab\n\nWhat is Google Colab?\nGoogle Colab (short for Colaboratory) is a free cloud service that lets you write and run Python code in your web browser. You don‚Äôt need to install anything on your computer - everything runs in the cloud. This makes it perfect for getting started with Python.\n\n\nAccessing Google Colab\n\nGo to https://colab.research.google.com\nSign in with your Google account\nClick on ‚ÄúNew Notebook‚Äù to create a new Python notebook\n\n\n\n\nCreating a new notebook\n\n\n\n\nUnderstanding the Colab Interface\nA Colab notebook consists of cells. There are two main types:\n\nCode cells: Where you write Python code\nText cells: Where you write notes and explanations (using Markdown)\n\n\n\n\nColab interface\n\n\n\n\nRunning Code\nTo run code in a cell:\n\nClick the Play button (‚ñ∂) on the left side of the cell, OR\nPress Shift + Enter on your keyboard\n\nThe output will appear below the cell.\n\n\n\nRunning a code cell\n\n\n\n\nSaving Your Work\nYour notebooks are automatically saved to your Google Drive in a folder called ‚ÄúColab Notebooks‚Äù. You can also:\n\nRename your notebook by clicking on the title at the top\nDownload your notebook (File ‚Üí Download ‚Üí Download .ipynb)\nShare it with others (Share button in top right)\n\n\n\n\n\n\n\nNoteüìå Key Point\n\n\n\nYour Colab notebooks are saved to Google Drive, so they count toward your Drive storage quota. We‚Äôll discuss local alternatives later in this tutorial."
  },
  {
    "objectID": "basic-python.html#your-first-python-code",
    "href": "basic-python.html#your-first-python-code",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "2. Your First Python Code",
    "text": "2. Your First Python Code\nLet‚Äôs start by looking at some Python code. Don‚Äôt worry if you don‚Äôt understand it yet - that‚Äôs what we‚Äôre here to learn!\n\nmessage = \"Hello, World!\"\nprint(message)\n\nHello, World!\n\n\n\nWhat Just Happened?\nLet‚Äôs break down this code:\n\nmessage = \"Hello, World!\" - This creates a variable named message and stores the text ‚ÄúHello, World!‚Äù in it\nprint(message) - This tells Python to display the contents of the message variable\n\nThink of a variable like a labeled box where you can store information. The = sign means ‚Äústore the value on the right into the variable on the left‚Äù.\n\n\nExercise 2.1\nCopy the following code into Google Colab and run it:\n\nmessage = \"Hello, World!\"\nprint(message)\n\nHello, World!\n\n\nNow modify it:\n\nChange \"Hello, World!\" to \"Python is fun!\" and run the code again\nChange the variable name from message to greeting (remember to change it in both places!)\nAdd another line: print(\"My first Python program\") and run the code\n\n\n\n\n\n\n\nTipüí° Tip\n\n\n\nVariable names can contain letters, numbers, and underscores, but they must start with a letter or underscore. Use descriptive names that help you remember what the variable contains."
  },
  {
    "objectID": "basic-python.html#getting-help-and-finding-information",
    "href": "basic-python.html#getting-help-and-finding-information",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "3. Getting Help and Finding Information",
    "text": "3. Getting Help and Finding Information\nBefore we dive deeper into Python, let‚Äôs learn how to find help when you‚Äôre stuck. This is one of the most important skills for working with Python!\n\nUsing Python‚Äôs Built-in Help\nPython has a built-in help() function that shows you information about functions and objects.\n\nhelp(print)\n\nThis will display documentation about the print function, including how to use it.\n\n\n\nUsing the help() function\n\n\n\n\nUsing dir() to Explore\nThe dir() function shows you what methods and attributes are available for an object:\n\ntext = \"hello\"\ndir(text)\n\n['__add__',\n '__class__',\n '__contains__',\n '__delattr__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getnewargs__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__rmod__',\n '__rmul__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'capitalize',\n 'casefold',\n 'center',\n 'count',\n 'encode',\n 'endswith',\n 'expandtabs',\n 'find',\n 'format',\n 'format_map',\n 'index',\n 'isalnum',\n 'isalpha',\n 'isascii',\n 'isdecimal',\n 'isdigit',\n 'isidentifier',\n 'islower',\n 'isnumeric',\n 'isprintable',\n 'isspace',\n 'istitle',\n 'isupper',\n 'join',\n 'ljust',\n 'lower',\n 'lstrip',\n 'maketrans',\n 'partition',\n 'removeprefix',\n 'removesuffix',\n 'replace',\n 'rfind',\n 'rindex',\n 'rjust',\n 'rpartition',\n 'rsplit',\n 'rstrip',\n 'split',\n 'splitlines',\n 'startswith',\n 'strip',\n 'swapcase',\n 'title',\n 'translate',\n 'upper',\n 'zfill']\n\n\nThis shows all the things you can do with a string. Methods that start with _ are internal - focus on the others like upper, lower, split, etc.\n\n\nReading Official Python Documentation\nThe official Python documentation is at https://docs.python.org. It‚Äôs comprehensive and well-organized.\nFor beginners, the Python Tutorial section is especially helpful: https://docs.python.org/3/tutorial/\n\n\n\nPython documentation\n\n\n\n\nSearching Online Effectively\nWhen you have a question or encounter an error:\n\nGoogle your question - Include ‚Äúpython‚Äù in your search\n\nGood: ‚Äúpython how to convert string to lowercase‚Äù\nBad: ‚Äúmake text smaller‚Äù\n\nStack Overflow - A Q&A site where programmers help each other\n\nLook for questions with many upvotes and accepted answers (green checkmark)\nRead the question to make sure it matches your problem\n\nRead the error message - Python error messages often tell you exactly what‚Äôs wrong\n\nWe‚Äôll cover this in detail in the optional debugging section\n\n\n\n\n\n\n\n\nTipüí° Tip\n\n\n\nWhen searching for help, include the Python version you‚Äôre using. Google Colab typically uses Python 3, so add ‚Äúpython 3‚Äù to your searches.\n\n\n\n\nExercise 3.1\nIn Google Colab, try the following:\n\nRun help(len) - what does the len() function do?\nCreate a variable: word = \"Python\", then run dir(word)\nFind a method in the output that sounds interesting (like upper or lower)\nTry using it: word.upper() or word.lower()\nUse help(word.upper) to learn more about that method"
  },
  {
    "objectID": "basic-python.html#basic-data-types",
    "href": "basic-python.html#basic-data-types",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "4. Basic Data Types",
    "text": "4. Basic Data Types\nPython works with different types of data. Let‚Äôs explore the most important ones for text and data analysis.\n\nStrings\nStrings are text data - anything you can type. They‚Äôre enclosed in quotes (either \" or ').\n\nauthor = \"Virginia Woolf\"\ntitle = 'Mrs Dalloway'\nsentence = \"She said, 'I love Python!'\"\n\n\nString Operations\nConcatenation (joining strings):\n\nfirst_name = \"Ada\"\nlast_name = \"Lovelace\"\nfull_name = first_name + \" \" + last_name\nprint(full_name)\n\nAda Lovelace\n\n\nGetting string length:\n\ntext = \"Hello\"\nlength = len(text)\nprint(length)\n\n5\n\n\n\ntext = \"Python\"\nprint(text[0])      # First character\nprint(text[1])      # Second character\nprint(text[-1])     # Last character\nprint(text[0:3])    # Characters 0, 1, 2 (not 3)\nprint(text[2:])     # From character 2 to the end\n\nP\ny\nn\nPyt\nthon\n\n\n\n\nString Methods\nStrings have many built-in methods (functions that belong to strings):\nChanging case:\n\ntext = \"Hello World\"\nprint(text.lower())\nprint(text.upper())\n\nhello world\nHELLO WORLD\n\n\n\nsentence = \"Python is great for text analysis\"\nwords = sentence.split()\nprint(words)\n\n['Python', 'is', 'great', 'for', 'text', 'analysis']\n\n\n\ntext = \"I like cats\"\nnew_text = text.replace(\"cats\", \"dogs\")\nprint(new_text)\n\nI like dogs\n\n\n\ntext = \"   hello   \"\nprint(text.strip())\n\nhello\n\n\n\nsentence = \"Python is amazing\"\nposition = sentence.find(\"is\")\nprint(position)\n\n7\n\n\n\ntext = \"how much wood would a woodchuck chuck\"\ncount = text.count(\"wood\")\nprint(count)\n\n2\n\n\n\n\nString Formatting with f-strings\nF-strings let you insert variable values into strings easily:\n\nname = \"Alice\"\nage = 25\nmessage = f\"My name is {name} and I am {age} years old\"\nprint(message)\n\nMy name is Alice and I am 25 years old\n\n\n\n\n\nExercise 4.1\nCopy this code into Colab:\n\nbook_title = \"pride and prejudice\"\nauthor = \"Jane Austen\"\n\nModify the code to:\n\nConvert book_title to title case using .title() and print it\nMake author all uppercase and print it\nCreate a sentence using an f-string: \"The book {book_title} was written by {author}\"\nUse .split() on book_title to separate it into words and print the result\nCount how many times the letter ‚Äúe‚Äù appears in book_title\n\n\n\nNumbers\nPython works with two main types of numbers:\nIntegers (whole numbers):\n\npages = 324\nchapters = 12\n\nFloats (decimal numbers):\n\nprice = 19.99\nrating = 4.5\n\n\nArithmetic Operations\n\n# Basic arithmetic\nprint(10 + 5)      # Addition: 15\nprint(10 - 5)      # Subtraction: 5\nprint(10 * 5)      # Multiplication: 50\nprint(10 / 5)      # Division: 2.0 (always returns float)\nprint(10 // 3)     # Integer division: 3 (rounds down)\nprint(10 % 3)      # Modulo (remainder): 1\nprint(10 ** 2)     # Exponentiation: 100\n\n15\n5\n50\n2.0\n3\n1\n100\n\n\n\n\nType Conversion\nSometimes you need to convert between strings and numbers:\n\n# String to number\ntext_number = \"42\"\nnumber = int(text_number)\nprint(number + 8)  # Output: 50\n\n# Number to string\nage = 25\nmessage = \"I am \" + str(age) + \" years old\"\nprint(message)\n\n50\nI am 25 years old\n\n\n\n\n\n\n\n\nWarning‚ö†Ô∏è Warning\n\n\n\nYou cannot directly concatenate strings and numbers. You‚Äôll get an error if you try \"Age: \" + 25. Convert the number to a string first: \"Age: \" + str(25), or use an f-string: f\"Age: {25}\".\n\n\n\n\n\nExercise 4.2\nCopy this code into Colab:\n\ntotal_words = 1000\npages = 5\n\nModify the code to:\n\nCalculate words per page by dividing total_words by pages and print it\nCreate a variable additional_pages = 3 and calculate the new total pages\nConvert the result to a string and create a message: \"The document has X pages\" (use f-string)\nCalculate how many pages you‚Äôd have if you doubled the current number\n\n\n\nBooleans\nBooleans represent True or False values. They‚Äôre essential for making decisions in code.\n\nis_published = True\nis_draft = False\n\n\nComparison Operators\nThese operators compare values and return True or False:\n\nx = 10\ny = 5\n\nprint(x &gt; y)       # Greater than: True\nprint(x &lt; y)       # Less than: False\nprint(x == y)      # Equal to: False\nprint(x != y)      # Not equal to: True\nprint(x &gt;= 10)     # Greater than or equal: True\nprint(x &lt;= 5)      # Less than or equal: False\n\nTrue\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n\n\n\n\n\n\nNoteüìå Key Point\n\n\n\nUse == to compare values (equality test) and = to assign values to variables. This is a common source of confusion!\n\n\nYou can also compare strings:\n\nword1 = \"apple\"\nword2 = \"banana\"\nprint(word1 == word2)    # False\nprint(word1 &lt; word2)     # True (alphabetical order)\n\nFalse\nTrue\n\n\n\n\n\nExercise 4.3\nCopy this code into Colab:\n\nword_count = 150\nminimum_required = 100\n\nModify the code to:\n\nCheck if word_count is greater than minimum_required and print the result\nCheck if word_count equals 150 and print the result\nCheck if word_count is not equal to 200 and print the result\nChange word_count to 75 and run the comparisons again"
  },
  {
    "objectID": "basic-python.html#data-structures",
    "href": "basic-python.html#data-structures",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "5. Data Structures",
    "text": "5. Data Structures\nData structures let you organize and store multiple pieces of information together.\n\nLists\nLists are ordered collections of items. They‚Äôre perfect for storing sequences of data.\n\nauthors = [\"Virginia Woolf\", \"James Joyce\", \"Marcel Proust\"]\nword_counts = [150, 200, 175, 300]\nmixed_data = [\"Python\", 3, True, 19.99]\n\n\nAccessing List Items\nLists use zero-based indexing, just like strings:\n\nfruits = [\"apple\", \"banana\", \"cherry\", \"date\"]\nprint(fruits[0])      # First item: apple\nprint(fruits[1])      # Second item: banana\nprint(fruits[-1])     # Last item: date\nprint(fruits[-2])     # Second to last: cherry\n\napple\nbanana\ndate\ncherry\n\n\n\n\nSlicing Lists\n\nnumbers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nprint(numbers[2:5])    # Items 2, 3, 4: [2, 3, 4]\nprint(numbers[:3])     # First 3 items: [0, 1, 2]\nprint(numbers[7:])     # From index 7 to end: [7, 8, 9]\nprint(numbers[::2])    # Every other item: [0, 2, 4, 6, 8]\n\n[2, 3, 4]\n[0, 1, 2]\n[7, 8, 9]\n[0, 2, 4, 6, 8]\n\n\n\n\nModifying Lists\nAdding items:\n\nbooks = [\"1984\", \"Brave New World\"]\nbooks.append(\"Fahrenheit 451\")\nprint(books)\n\n['1984', 'Brave New World', 'Fahrenheit 451']\n\n\n\nbooks = [\"1984\", \"Brave New World\", \"Fahrenheit 451\"]\nbooks.remove(\"Brave New World\")\nprint(books)\n\n['1984', 'Fahrenheit 451']\n\n\n\nbooks = [\"1984\", \"Brave New World\", \"Fahrenheit 451\"]\nlast_book = books.pop()\nprint(last_book)\nprint(books)\n\nFahrenheit 451\n['1984', 'Brave New World']\n\n\n\n\nList Methods\nGetting list length:\n\nwords = [\"the\", \"quick\", \"brown\", \"fox\"]\nprint(len(words))\n\n4\n\n\n\nnumbers = [3, 1, 4, 1, 5, 9, 2, 6]\nnumbers.sort()\nprint(numbers)\n\n[1, 1, 2, 3, 4, 5, 6, 9]\n\n\n\nletters = [\"a\", \"b\", \"c\", \"d\"]\nletters.reverse()\nprint(letters)\n\n['d', 'c', 'b', 'a']\n\n\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\nprint(\"banana\" in fruits)\nprint(\"grape\" in fruits)\n\nTrue\nFalse\n\n\n\n\nLists of Strings (Text Analysis)\nLists are especially useful for working with text data:\n\nsentence = \"Python is great for text analysis\"\nwords = sentence.split()\nprint(words)\nprint(f\"Number of words: {len(words)}\")\nprint(f\"First word: {words[0]}\")\nprint(f\"Last word: {words[-1]}\")\n\n['Python', 'is', 'great', 'for', 'text', 'analysis']\nNumber of words: 6\nFirst word: Python\nLast word: analysis\n\n\n\n\n\nExercise 5.1\nCopy this code into Colab:\n\ntext = \"to be or not to be that is the question\"\nwords = text.split()\n\nModify the code to:\n\nPrint the length of the words list\nPrint the first word and the last word\nUse .append() to add the word ‚Äúindeed‚Äù to the end of the list\nUse .remove() to remove the first occurrence of ‚Äúto‚Äù\nSort the words alphabetically and print the result\n\n\n\nDictionaries\nDictionaries store data as key-value pairs. They‚Äôre like a real dictionary where you look up a word (key) to find its definition (value).\n\nbook = {\n    \"title\": \"1984\",\n    \"author\": \"George Orwell\",\n    \"year\": 1949,\n    \"pages\": 328\n}\n\n\nAccessing Dictionary Values\nUse keys to access values:\n\nbook = {\n    \"title\": \"1984\",\n    \"author\": \"George Orwell\",\n    \"year\": 1949\n}\n\nprint(book[\"title\"])\nprint(book[\"author\"])\n\n1984\nGeorge Orwell\n\n\n\n\nAdding or Modifying Entries\n\nbook = {\"title\": \"1984\", \"author\": \"George Orwell\"}\n\n# Add a new entry\nbook[\"year\"] = 1949\nprint(book)\n\n# Modify an existing entry\nbook[\"year\"] = 1950\nprint(book)\n\n{'title': '1984', 'author': 'George Orwell', 'year': 1949}\n{'title': '1984', 'author': 'George Orwell', 'year': 1950}\n\n\n\n\nDictionary Methods\nGetting all keys:\n\nbook = {\"title\": \"1984\", \"author\": \"George Orwell\", \"year\": 1949}\nprint(book.keys())\n\ndict_keys(['title', 'author', 'year'])\n\n\n\nprint(book.values())\n\ndict_values(['1984', 'George Orwell', 1949])\n\n\n\nprint(book.items())\n\ndict_items([('title', '1984'), ('author', 'George Orwell'), ('year', 1949)])\n\n\n\nprint(\"author\" in book)\nprint(\"publisher\" in book)\n\nTrue\nFalse\n\n\n\n\nDictionaries for Text Analysis\nDictionaries are useful for counting and organizing text data:\n\nword_frequencies = {\n    \"the\": 150,\n    \"and\": 89,\n    \"to\": 76,\n    \"of\": 72\n}\n\nprint(f\"The word 'the' appears {word_frequencies['the']} times\")\n\nThe word 'the' appears 150 times\n\n\n\n\n\nExercise 5.2\nCopy this code into Colab:\n\npoem = {\n    \"title\": \"The Road Not Taken\",\n    \"author\": \"Robert Frost\",\n    \"year\": 1916\n}\n\nModify the code to:\n\nPrint the poem‚Äôs title\nAdd a new key ‚Äúlines‚Äù with the value 20\nChange the year to 1915\nPrint all the keys in the dictionary\nPrint all the values in the dictionary\nCheck if ‚Äúpublisher‚Äù is a key in the dictionary and print the result"
  },
  {
    "objectID": "basic-python.html#control-flow",
    "href": "basic-python.html#control-flow",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "6. Control Flow",
    "text": "6. Control Flow\nControl flow lets you make decisions and repeat actions in your code.\n\nIf Statements\nIf statements let your code make decisions based on conditions.\n\nword_count = 150\nminimum = 100\n\nif word_count &gt;= minimum:\n    print(\"You have enough words!\")\n    print(\"Good job!\")\n\nYou have enough words!\nGood job!\n\n\nOutput (if word_count is 150):\nYou have enough words!\nGood job!\n\n\n\n\n\n\nNoteüìå Key Point\n\n\n\nNotice the indentation (spaces at the start of lines). Python uses indentation to group code together. Everything indented under the if statement runs only if the condition is True.\n\n\n\nIf-Else\n\nword_count = 75\nminimum = 100\n\nif word_count &gt;= minimum:\n    print(\"You have enough words!\")\nelse:\n    print(\"You need more words\")\n    words_needed = minimum - word_count\n    print(f\"You need {words_needed} more words\")\n\nYou need more words\nYou need 25 more words\n\n\n\n\nIf-Elif-Else\nUse elif (else-if) for multiple conditions:\n\nscore = 85\n\nif score &gt;= 90:\n    grade = \"A\"\nelif score &gt;= 80:\n    grade = \"B\"\nelif score &gt;= 70:\n    grade = \"C\"\nelse:\n    grade = \"F\"\n\nprint(f\"Your grade is: {grade}\")\n\nYour grade is: B\n\n\n\n\n\nExercise 6.1\nCopy this code into Colab:\n\ntext = \"Python\"\n\nModify the code to:\n\nCheck if the length of text is greater than 5. If it is, print ‚ÄúLong word‚Äù, otherwise print ‚ÄúShort word‚Äù\nChange text to different words and test your code\nModify your code to handle three cases: length &gt; 8 (print ‚ÄúVery long‚Äù), length &gt; 5 (print ‚ÄúMedium‚Äù), otherwise (print ‚ÄúShort‚Äù)\n\n\n\nLoops\nLoops let you repeat actions multiple times.\n\nFor Loops\nFor loops iterate over sequences (lists, strings, etc.):\nLooping over a list:\n\nwords = [\"Python\", \"is\", \"great\"]\nfor word in words:\n    print(word)\n\nPython\nis\ngreat\n\n\n\ntext = \"Python\"\nfor letter in text:\n    print(letter)\n\nP\ny\nt\nh\no\nn\n\n\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\n\n# From 1 to 5\nfor i in range(1, 6):\n    print(i)\n\n# From 0 to 10, counting by 2\nfor i in range(0, 11, 2):\n    print(i)\n\n1\n2\n3\n4\n5\n0\n2\n4\n6\n8\n10\n\n\nCombining loops and if statements:\n\nwords = [\"apple\", \"banana\", \"cherry\", \"date\"]\nfor word in words:\n    if len(word) &gt; 5:\n        print(f\"{word} is a long word\")\n\nbanana is a long word\ncherry is a long word\n\n\n\nword_counts = {\"the\": 150, \"and\": 89, \"to\": 76}\n\n# Loop over keys\nfor word in word_counts:\n    print(word)\n\n# Loop over keys and values\nfor word, count in word_counts.items():\n    print(f\"{word}: {count}\")\n\nthe\nand\nto\nthe: 150\nand: 89\nto: 76\n\n\n\n\nWhile Loops\nWhile loops repeat as long as a condition is True:\n\ncount = 0\nwhile count &lt; 5:\n    print(count)\n    count = count + 1\n\n0\n1\n2\n3\n4\n\n\n\n\n\nExercise 6.2\nCopy this code into Colab:\n\nsentence = \"the quick brown fox jumps over the lazy dog\"\nwords = sentence.split()\n\nModify the code to:\n\nUse a for loop to print each word in words\nUse a for loop with an if statement to print only words with more than 3 letters\nUse a for loop to count how many words start with the letter ‚Äút‚Äù (use .startswith(\"t\"))\nUse range() to print the first 5 numbers (0-4)\nChange the range to print numbers from 1 to 10"
  },
  {
    "objectID": "basic-python.html#functions",
    "href": "basic-python.html#functions",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "7. Functions",
    "text": "7. Functions\nFunctions are reusable blocks of code that perform specific tasks.\n\nLooking at Function Code First\n\ndef greet(name):\n    message = f\"Hello, {name}!\"\n    return message\n\nresult = greet(\"Alice\")\nprint(result)\n\nHello, Alice!\n\n\n\n\nWhat Are Functions?\nLet‚Äôs break down that code:\n\ndef greet(name): - This defines a function named greet that takes one parameter called name\nThe indented code is the function body - what the function does\nreturn message - This sends the result back to whoever called the function\ngreet(\"Alice\") - This calls (runs) the function with the argument \"Alice\"\n\nThink of functions like recipes: you define the recipe once, then you can follow it many times with different ingredients.\n\n\nBuilt-in Functions Review\nWe‚Äôve already been using Python‚Äôs built-in functions:\n\n# len() - get length\ntext = \"Python\"\nprint(len(text))  # 6\n\n# type() - check data type\nprint(type(42))        # &lt;class 'int'&gt;\nprint(type(\"hello\"))   # &lt;class 'str'&gt;\nprint(type([1, 2, 3])) # &lt;class 'list'&gt;\n\n# print() - display output\nprint(\"Hello, World!\")\n\n# input() - get user input (works in Colab!)\nname = input(\"What is your name? \")\nprint(f\"Hello, {name}\")\n\n\n\nCreating Custom Functions\nFunction without parameters:\n\ndef say_hello():\n    print(\"Hello, World!\")\n    \nsay_hello()\n\nHello, World!\n\n\n\ndef create_greeting(first_name, last_name):\n    full_name = f\"{first_name} {last_name}\"\n    greeting = f\"Welcome, {full_name}!\"\n    return greeting\n\nmessage = create_greeting(\"Ada\", \"Lovelace\")\nprint(message)\n\nWelcome, Ada Lovelace!\n\n\n\ndef greet(name, greeting=\"Hello\"):\n    return f\"{greeting}, {name}!\"\n\nprint(greet(\"Alice\"))\nprint(greet(\"Bob\", \"Hi\"))\n\nHello, Alice!\nHi, Bob!\n\n\n\ndef count_words(text):\n    words = text.split()\n    return len(words)\n\nsentence = \"Python is great for text analysis\"\nnum_words = count_words(sentence)\nprint(f\"The sentence has {num_words} words\")\n\nThe sentence has 6 words\n\n\n\ndef analyze_text(text):\n    num_chars = len(text)\n    num_words = len(text.split())\n    return num_chars, num_words\n\ntext = \"Hello, World!\"\nchars, words = analyze_text(text)\nprint(f\"Characters: {chars}, Words: {words}\")\n\nCharacters: 13, Words: 2\n\n\n\n\nExercise 7.1\nCopy this code into Colab:\n\ndef process_word(word):\n    return word.upper()\n\nresult = process_word(\"python\")\nprint(result)\n\nPYTHON\n\n\nModify the code to:\n\nChange the function to return word.lower() instead\nAdd a second parameter case_type and use an if statement to return either uppercase or lowercase based on the parameter\nCreate a new function count_letter(text, letter) that counts how many times a specific letter appears in text\nTest your function with different texts and letters"
  },
  {
    "objectID": "basic-python.html#working-with-files",
    "href": "basic-python.html#working-with-files",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "8. Working with Files",
    "text": "8. Working with Files\nWorking with files lets you read and write data stored on your computer (or in Colab‚Äôs temporary storage).\n\nReading Text Files\n\n# Open and read a file\nfile = open(\"sample.txt\", \"r\")\ncontent = file.read()\nprint(content)\nfile.close()\n\nThis is line 1\nThis is line 2\nThis is line 3\n\n\n\n\n\n\n\n\n\nNoteüìå Key Point\n\n\n\nThe \"r\" means ‚Äúread mode‚Äù. Always close files when you‚Äôre done with file.close() to free up resources.\n\n\nBetter way: Using with statement (automatically closes the file):\n\nwith open(\"sample.txt\", \"r\") as file:\n    content = file.read()\n    print(content)\n# File is automatically closed here\n\nThis is line 1\nThis is line 2\nThis is line 3\n\n\n\nReading line by line:\n\nwith open(\"sample.txt\", \"r\") as file:\n    for line in file:\n        print(line.strip())  # strip() removes extra whitespace/newlines\n\nThis is line 1\nThis is line 2\nThis is line 3\n\n\nReading all lines into a list:\n\nwith open(\"sample.txt\", \"r\") as file:\n    lines = file.readlines()\n    print(f\"Number of lines: {len(lines)}\")\n    print(f\"First line: {lines[0].strip()}\")\n\nNumber of lines: 3\nFirst line: This is line 1\n\n\n\n\nWriting to Text Files\n\n# Writing to a file (creates new file or overwrites existing)\nwith open(\"output.txt\", \"w\") as file:\n    file.write(\"Hello, World!\\n\")\n    file.write(\"This is a new line.\\n\")\n\n\n\n\n\n\n\nWarning‚ö†Ô∏è Warning\n\n\n\nUsing \"w\" mode will overwrite the entire file if it exists. Use \"a\" (append mode) to add to the end of an existing file instead.\n\n\nAppending to a file:\n\nwith open(\"output.txt\", \"a\") as file:\n    file.write(\"This line is added to the end.\\n\")\n\nWriting multiple lines:\n\nlines = [\"First line\\n\", \"Second line\\n\", \"Third line\\n\"]\nwith open(\"output.txt\", \"w\") as file:\n    file.writelines(lines)\n\n\n\nFile Paths in Google Colab\nIn Google Colab, you can:\n\nUpload files using the file browser on the left (folder icon)\nCreate files in code cells\nMount Google Drive to access your Drive files\n\n\n\n\nUploading files in Colab. (1) Press ‚Äúfolder‚Äù icon. (2) The upload button.\n\n\nCreating a sample file in Colab:\n\n# Create a sample file to practice with\nwith open(\"sample.txt\", \"w\") as file:\n    file.write(\"This is line 1\\n\")\n    file.write(\"This is line 2\\n\")\n    file.write(\"This is line 3\\n\")\n\n# Now read it back\nwith open(\"sample.txt\", \"r\") as file:\n    content = file.read()\n    print(content)\n\nThis is line 1\nThis is line 2\nThis is line 3\n\n\n\n\n\nText Analysis Example\n\n# Count word frequencies in a file\nword_counts = {}\n\nwith open(\"sample.txt\", \"r\") as file:\n    for line in file:\n        words = line.lower().split()\n        for word in words:\n            if word in word_counts:\n                word_counts[word] += 1\n            else:\n                word_counts[word] = 1\n\nprint(word_counts)\n\n{'this': 3, 'is': 3, 'line': 3, '1': 1, '2': 1, '3': 1}\n\n\n\n\nExercise 8.1\nCopy this code into Colab to create a sample file:\n\nwith open(\"poem.txt\", \"w\") as file:\n    file.write(\"Roses are red\\n\")\n    file.write(\"Violets are blue\\n\")\n    file.write(\"Python is fun\\n\")\n    file.write(\"And so are you\\n\")\n\nNow modify the code to:\n\nRead the file and print its contents\nRead the file and print only the first two lines\nRead the file and count the total number of words across all lines\nRead the file and create a list of all words (split each line and combine)\nWrite a new file called ‚Äúoutput.txt‚Äù with all the words in uppercase"
  },
  {
    "objectID": "basic-python.html#introduction-to-packages",
    "href": "basic-python.html#introduction-to-packages",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "9. Introduction to Packages",
    "text": "9. Introduction to Packages\nSo far, we‚Äôve used Python‚Äôs built-in features. But Python‚Äôs real power comes from packages (also called libraries) - collections of pre-written code that add new capabilities.\n\nWhat Are Packages?\nPackages are like toolboxes. Each package contains functions and tools for specific tasks:\n\npandas - working with tabular data (like spreadsheets)\nnumpy - numerical computing and arrays\nmatplotlib - creating visualizations and charts\nnltk - natural language processing and text analysis\nscikit-learn - machine learning\n\n\n\nImporting Packages\nTo use a package, you import it:\n\nimport math\n\n# Now you can use functions from the math package\nresult = math.sqrt(16)\nprint(result)\n\n4.0\n\n\n\nimport math as m\n\nresult = m.sqrt(25)\nprint(result)\n\n5.0\n\n\n\nfrom math import sqrt, pi\n\nprint(sqrt(9))\nprint(pi)\n\n3.0\n3.141592653589793\n\n\n\n\nInstalling Packages in Colab\nMost common packages are already installed in Google Colab. If you need to install a package, use:\n!pip install package-name\nThe ! tells Colab to run this as a shell command, not Python code.\n\n\n\n\n\n\nNoteüìå Key Point\n\n\n\nIn Colab, you usually don‚Äôt need to install packages. Just import them! We‚Äôll discuss installing packages locally in the next section.\n\n\n\n\nExample: Reading CSV with Pandas\nLet‚Äôs see a quick example using pandas to work with tabular data:\n\nimport pandas as pd\n\n# Create a sample dataset\ndata = {\n    \"book\": [\"1984\", \"Brave New World\", \"Fahrenheit 451\"],\n    \"author\": [\"George Orwell\", \"Aldous Huxley\", \"Ray Bradbury\"],\n    \"year\": [1949, 1932, 1953],\n    \"pages\": [328, 311, 249]\n}\n\n# Create a DataFrame (pandas' table structure)\ndf = pd.DataFrame(data)\n\n# Display the data\nprint(df)\n\n              book         author  year  pages\n0             1984  George Orwell  1949    328\n1  Brave New World  Aldous Huxley  1932    311\n2   Fahrenheit 451   Ray Bradbury  1953    249\n\n\n\nprint(df[\"book\"])\nprint(df[\"year\"])\n\n0               1984\n1    Brave New World\n2     Fahrenheit 451\nName: book, dtype: object\n0    1949\n1    1932\n2    1953\nName: year, dtype: int64\n\n\nFiltering data:\n\n# Books published after 1940\nrecent_books = df[df[\"year\"] &gt; 1940]\nprint(recent_books)\n\n             book         author  year  pages\n0            1984  George Orwell  1949    328\n2  Fahrenheit 451   Ray Bradbury  1953    249\n\n\nBasic statistics:\n\nprint(f\"Average pages: {df['pages'].mean()}\")\nprint(f\"Earliest year: {df['year'].min()}\")\n\nAverage pages: 296.0\nEarliest year: 1932\n\n\nReading from a CSV file:\n\n# Create a sample CSV file first\nwith open(\"books.csv\", \"w\") as file:\n    file.write(\"book,author,year,pages\\n\")\n    file.write(\"1984,George Orwell,1949,328\\n\")\n    file.write(\"Brave New World,Aldous Huxley,1932,311\\n\")\n\n# Read it with pandas\ndf = pd.read_csv(\"books.csv\")\nprint(df)\n\n              book         author  year  pages\n0             1984  George Orwell  1949    328\n1  Brave New World  Aldous Huxley  1932    311\n\n\n\n\n\n\n\n\nTipüí° Tip\n\n\n\nPandas is incredibly powerful for data analysis. We‚Äôll explore it more deeply when working with real datasets. For now, just know that it exists and can read CSV files easily!\n\n\n\n\nExercise 9.1\nCopy this code into Colab:\n\nimport pandas as pd\n\ndata = {\n    \"word\": [\"the\", \"and\", \"to\", \"of\"],\n    \"count\": [150, 89, 76, 72]\n}\n\ndf = pd.DataFrame(data)\n\nModify the code to:\n\nPrint the entire DataFrame\nPrint only the ‚Äúword‚Äù column\nPrint only the ‚Äúcount‚Äù column\nFind the maximum count using df[\"count\"].max()\nFind the minimum count using df[\"count\"].min()"
  },
  {
    "objectID": "basic-python.html#local-python-environments",
    "href": "basic-python.html#local-python-environments",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "10. Local Python Environments",
    "text": "10. Local Python Environments\nSo far, we‚Äôve been using Google Colab, which runs Python in the cloud. But you might want to run Python on your own computer. Let‚Äôs explore why and how.\n\nCloud vs.¬†Local Development\nGoogle Colab (Cloud):\n\n‚úì No installation needed\n‚úì Works on any computer with a browser\n‚úì Free access to computing resources\n‚úó Requires internet connection\n‚úó Sessions timeout after inactivity\n‚úó Uses your Google Drive storage\n‚úó Some packages may not work\n\nLocal Python (Your Computer):\n\n‚úì Works offline\n‚úì Full control over environment\n‚úì No session timeouts\n‚úì All packages available\n‚úó Requires installation and setup\n‚úó Uses your computer‚Äôs resources\n\n\n\n\n\n\n\nNoteüìå Key Point\n\n\n\nFor learning and quick experiments, Colab is perfect. For larger projects or when you need specific packages, local Python is better.\n\n\n\n\nIntroduction to uv (Recommended)\nuv is a modern, fast Python package and project manager. We recommend it because it‚Äôs:\n\nVery fast (much faster than traditional tools)\nEasy to use\nHandles both Python installation and package management\nBecoming the industry standard\n\nLear more about uv here: https://docs.astral.sh/uv/\n\nInstalling uv\nOn macOS and Linux:\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nOn Windows:\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\nAfter installation, close and reopen your terminal.\n\n\nUsing uv\nCreate a new Python project:\nuv init my-project\ncd my-project\nThis creates a new directory with a basic Python project structure.\nInstall a package:\nuv add pandas\nThis installs pandas and adds it to your project‚Äôs dependencies.\nRun Python:\nuv run python\nThis starts a Python interpreter with your project‚Äôs packages available.\nRun a Python script:\nuv run python my_script.py\nInstall a specific Python version:\nuv python install 3.11\n\n\n\n\n\n\nTipüí° Tip\n\n\n\nuv automatically creates isolated environments for each project, so different projects can use different package versions without conflicts.\n\n\n\n\n\nIntroduction to Conda (Fallback Option)\nConda is a more established tool that also manages Python environments and packages. It‚Äôs widely used in data science.\n\nInstalling Conda\nDownload and install Miniconda (a minimal conda installation) from: https://docs.conda.io/en/latest/miniconda.html\nChoose the installer for your operating system and follow the installation instructions.\n\n\n\nMiniconda download page. At the bottom of https://www.anaconda.com/download\n\n\n\n\n\nMiniconda download page. Then choose ‚ÄúMiniconda Installers‚Äù\n\n\n\n\nUsing Conda\nCreate a new environment:\nconda create -n my-env python=3.11\nThis creates a new environment named ‚Äúmy-env‚Äù with Python 3.11.\nActivate the environment:\nconda activate my-env\nInstall packages:\nconda install pandas\nDeactivate the environment:\nconda deactivate\nList all environments:\nconda env list\nRemove an environment:\nconda remove -n my-env --all\n\n\n\nUsing Jupyter Locally\nOnce you have Python installed locally (with either uv or conda), you can run Jupyter notebooks on your computer.\nWith uv:\nuv add jupyter\nuv run jupyter notebook\nWith conda:\nconda install jupyter\njupyter notebook\nThis opens Jupyter in your web browser, running locally on your computer.\n\n\n\nJupyter running locally\n\n\n\n\n\nJupyter running locally. A new notebook\n\n\n\n\n\nJupyter running locally. Always have a Python kernel selected.\n\n\n\n\n\nJupyter running locally. Some useful UI elements.\n\n\n\n\nWhen to Use Which Tool\nUse uv when:\n\nStarting a new project\nYou want the fastest tool\nYou‚Äôre working on modern Python projects\nYou want the latest features\n\nUse conda when:\n\nWorking with data science packages (it handles dependencies well)\nYou need packages that aren‚Äôt on PyPI (Python Package Index)\nYou‚Äôre following tutorials that use conda\nYou need packages that require complex non-Python dependencies\n\nUse Colab when:\n\nLearning and experimenting\nYou don‚Äôt have Python installed locally\nYou need quick access from any computer\nYou‚Äôre sharing code with others who may not have Python\n\n\n\n\n\n\n\nTipüí° Tip\n\n\n\nMany people use multiple tools: Colab for quick experiments and learning, uv for new projects, and conda for data science work. You don‚Äôt have to choose just one!\n\n\n\n\nExercise 10.1\nThis exercise requires setting up on your local computer (optional):\n\nInstall uv following the instructions for your operating system\nCreate a new project called ‚Äúpython-practice‚Äù\nAdd the pandas package to your project\nCreate a file called test.py with this code:\n\nimport pandas as pd\nprint(\"Pandas version:\", pd.__version__)\n\nPandas version: 2.3.3\n\n\nRun the file using uv run python test.py\n\nAlternative exercise with conda:\n\nInstall Miniconda following the instructions for your operating system\nCreate a new environment called ‚Äútest-env‚Äù with Python 3.11\nActivate the environment\nInstall pandas\nRun Python and import pandas to verify it works"
  },
  {
    "objectID": "basic-python.html#conclusion",
    "href": "basic-python.html#conclusion",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "11. Conclusion",
    "text": "11. Conclusion\nCongratulations! You‚Äôve completed this introduction to Python for text and data analysis.\n\nWhat You‚Äôve Learned\nYou can now:\n\n‚úì Run Python code in Google Colab\n‚úì Read and understand Python code by recognizing variables, data types, functions, and control flow\n‚úì Work with strings using methods like .lower(), .split(), .replace(), and string formatting\n‚úì Use data structures (lists and dictionaries) to organize data\n‚úì Control program flow with if statements and loops\n‚úì Create functions to organize reusable code\n‚úì Read and write files for data persistence\n‚úì Use packages like pandas for data analysis\n‚úì Find help using help(), documentation, and online resources\n‚úì Understand the difference between cloud (Colab) and local Python environments\n‚úì Set up local Python using uv or conda (optional)\n\n\n\nNext Steps\n\nPractice regularly - The best way to learn programming is by doing. Try writing small programs to solve problems you encounter.\nWork with real data - Apply these skills to actual text or datasets that interest you.\nLearn more packages - Explore packages like:\n\nnltk or spaCy for natural language processing\nmatplotlib or seaborn for data visualization\nnumpy for numerical computing\nscikit-learn for machine learning\n\nRead other people‚Äôs code - Since you‚Äôre learning to read code, study examples from tutorials and open-source projects.\nDebug and experiment - Don‚Äôt be afraid of errors! They‚Äôre a natural part of programming. See the optional debugging section for help.\nJoin communities - Consider joining Python forums, Reddit‚Äôs r/learnpython, or Stack Overflow to ask questions and learn from others.\n\n\n\nResources for Continued Learning\n\nOfficial Python Tutorial: https://docs.python.org/3/tutorial/\nPython for Data Analysis by Wes McKinney (pandas creator)\nReal Python: https://realpython.com - excellent tutorials\nAutomate the Boring Stuff with Python: Free online book at https://automatetheboringstuff.com\n\n\n\nKeep Learning!\nRemember: everyone who programs started as a beginner. The key is persistence and practice. You‚Äôve taken the first important steps, and you have all the foundational knowledge you need to continue learning.\nGood luck with your Python journey!"
  },
  {
    "objectID": "basic-python.html#appendix-a-debugging-and-error-messages-optional",
    "href": "basic-python.html#appendix-a-debugging-and-error-messages-optional",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "Appendix A: Debugging and Error Messages (Optional)",
    "text": "Appendix A: Debugging and Error Messages (Optional)\nThis section is optional but important. Understanding errors will help you fix problems faster and become more independent in your learning.\n\nWhy Errors Are Helpful\nErrors are not failures - they‚Äôre feedback! Python is telling you exactly what went wrong. Learning to read error messages is a crucial skill.\n\n\nCommon Error Types\n\nSyntaxError\nWhat it means: You wrote code that doesn‚Äôt follow Python‚Äôs grammar rules.\n\nprint(\"Hello\"\n\n\n  Cell In[85], line 1\n    print(\"Hello\"\n                 ^\n_IncompleteInputError: incomplete input\n\n\n\n\nError:\nSyntaxError: unexpected EOF while parsing\nWhat went wrong: Missing closing parenthesis.\nHow to fix: Add the closing ):\n\nprint(\"Hello\")\n\nHello\n\n\nAnother example:\n\nif x &gt; 5\n    print(\"Big\")\n\n\n  Cell In[87], line 1\n    if x &gt; 5\n            ^\nSyntaxError: expected ':'\n\n\n\n\nError:\nSyntaxError: invalid syntax\nWhat went wrong: Missing colon after if statement.\nHow to fix:\n\nif x &gt; 5:\n    print(\"Big\")\n\nBig\n\n\n\n\nNameError\nWhat it means: You‚Äôre trying to use a variable or function that doesn‚Äôt exist.\n\nprint(message)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[90], line 1\n----&gt; 1 print(message)\n\nNameError: name 'message' is not defined\n\n\n\nError:\nNameError: name 'message' is not defined\nWhat went wrong: The variable message was never created.\nHow to fix: Define the variable first:\n\nmessage = \"Hello\"\nprint(message)\n\nHello\n\n\nCommon cause: Typos in variable names\n\nmy_name = \"Alice\"\nprint(my_nane)  # Typo: 'nane' instead of 'name'\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[92], line 2\n      1 my_name = \"Alice\"\n----&gt; 2 print(my_nane)  # Typo: 'nane' instead of 'name'\n\nNameError: name 'my_nane' is not defined\n\n\n\n\n\nTypeError\nWhat it means: You‚Äôre using a value in a way that doesn‚Äôt work with its type.\n\nage = 25\nmessage = \"I am \" + age\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[93], line 2\n      1 age = 25\n----&gt; 2 message = \"I am \" + age\n\nTypeError: can only concatenate str (not \"int\") to str\n\n\n\nError:\nTypeError: can only concatenate str (not \"int\") to str\nWhat went wrong: Can‚Äôt directly add a string and a number.\nHow to fix: Convert the number to a string:\n\nage = 25\nmessage = \"I am \" + str(age)\n# OR use an f-string:\nmessage = f\"I am {age}\"\n\nAnother example:\n\ntext = \"Python\"\nprint(text[0:3])\ntext[0] = \"p\"  # Trying to change a character\n\nPyt\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[96], line 3\n      1 text = \"Python\"\n      2 print(text[0:3])\n----&gt; 3 text[0] = \"p\"  # Trying to change a character\n\nTypeError: 'str' object does not support item assignment\n\n\n\nError:\nTypeError: 'str' object does not support item assignment\nWhat went wrong: Strings are immutable (can‚Äôt be changed).\nHow to fix: Create a new string:\n\ntext = \"Python\"\ntext = \"p\" + text[1:]\n\n\n\nIndexError\nWhat it means: You‚Äôre trying to access a list/string position that doesn‚Äôt exist.\n\nwords = [\"apple\", \"banana\", \"cherry\"]\nprint(words[5])\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[98], line 2\n      1 words = [\"apple\", \"banana\", \"cherry\"]\n----&gt; 2 print(words[5])\n\nIndexError: list index out of range\n\n\n\nError:\nIndexError: list index out of range\nWhat went wrong: The list only has indices 0, 1, 2 (three items), but we tried to access index 5.\nHow to fix: Use a valid index:\n\nwords = [\"apple\", \"banana\", \"cherry\"]\nprint(words[2])  # The last item\n\ncherry\n\n\n\n\nKeyError\nWhat it means: You‚Äôre trying to access a dictionary key that doesn‚Äôt exist.\n\nbook = {\"title\": \"1984\", \"author\": \"George Orwell\"}\nprint(book[\"year\"])\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[100], line 2\n      1 book = {\"title\": \"1984\", \"author\": \"George Orwell\"}\n----&gt; 2 print(book[\"year\"])\n\nKeyError: 'year'\n\n\n\nError:\nKeyError: 'year'\nWhat went wrong: The dictionary doesn‚Äôt have a ‚Äúyear‚Äù key.\nHow to fix: Use an existing key or check if the key exists first:\n\n# Option 1: Use get() method (returns None if key doesn't exist)\nprint(book.get(\"year\"))\n\n# Option 2: Check if key exists\nif \"year\" in book:\n    print(book[\"year\"])\nelse:\n    print(\"Year not found\")\n\nNone\nYear not found\n\n\n\n\nIndentationError\nWhat it means: Your code indentation is inconsistent or incorrect.\n\ndef greet():\nprint(\"Hello\")\n\n\n  Cell In[102], line 2\n    print(\"Hello\")\n    ^\nIndentationError: expected an indented block after function definition on line 1\n\n\n\n\nError:\nIndentationError: expected an indented block\nWhat went wrong: The function body needs to be indented.\nHow to fix:\n\ndef greet():\n    print(\"Hello\")\n\nAnother example (mixing tabs and spaces):\n\nif True:\n    print(\"First line\")\n        print(\"Second line\")  # Too much indentation\n\n\n  Cell In[104], line 3\n    print(\"Second line\")  # Too much indentation\n    ^\nIndentationError: unexpected indent\n\n\n\n\nError:\nIndentationError: unexpected indent\n\n\n\n\n\n\nWarning‚ö†Ô∏è Warning\n\n\n\nAlways use spaces for indentation in Python (4 spaces is the standard). Don‚Äôt mix tabs and spaces!\n\n\n\n\nValueError\nWhat it means: You passed a value of the right type but inappropriate value.\n\nnumber = int(\"hello\")\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[105], line 1\n----&gt; 1 number = int(\"hello\")\n\nValueError: invalid literal for int() with base 10: 'hello'\n\n\n\nError:\nValueError: invalid literal for int() with base 10: 'hello'\nWhat went wrong: ‚Äúhello‚Äù can‚Äôt be converted to an integer.\nHow to fix: Use a string that represents a number:\n\nnumber = int(\"42\")\n\n\n\nAttributeError\nWhat it means: You‚Äôre trying to use a method or attribute that doesn‚Äôt exist for that type.\n\nnumber = 42\nresult = number.upper()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[107], line 2\n      1 number = 42\n----&gt; 2 result = number.upper()\n\nAttributeError: 'int' object has no attribute 'upper'\n\n\n\nError:\nAttributeError: 'int' object has no attribute 'upper'\nWhat went wrong: Numbers don‚Äôt have an .upper() method (only strings do).\nHow to fix: Use the correct type:\n\ntext = \"hello\"\nresult = text.upper()\n\n\n\n\nHow to Read Error Messages\nPython error messages have a standard format:\nTraceback (most recent call last):\n  File \"script.py\", line 3, in &lt;module&gt;\n    print(message)\nNameError: name 'message' is not defined\nReading from bottom to top:\n\nError type and description (NameError: name 'message' is not defined) - What went wrong\nLine number (line 3) - Where it happened\nCode snippet (print(message)) - The actual problematic line\nTraceback - The sequence of function calls leading to the error\n\n\n\n\n\n\n\nTipüí° Tip\n\n\n\nAlways read error messages from the bottom up. The last line tells you what went wrong, and the lines above show you where.\n\n\n\n\nCommon Beginner Mistakes\n\n1. Forgetting Colons\n\n# Wrong\nif x &gt; 5\n    print(\"Big\")\n\n# Right\nif x &gt; 5:\n    print(\"Big\")\n\n\n  Cell In[109], line 2\n    if x &gt; 5\n            ^\nSyntaxError: expected ':'\n\n\n\n\n\n\n2. Inconsistent Indentation\n\n# Wrong\ndef greet():\nprint(\"Hello\")\n    print(\"Welcome\")\n\n# Right\ndef greet():\n    print(\"Hello\")\n    print(\"Welcome\")\n\n\n  Cell In[110], line 3\n    print(\"Hello\")\n    ^\nIndentationError: expected an indented block after function definition on line 2\n\n\n\n\n\n\n3. Using = Instead of ==\n\n# Wrong (assignment, not comparison)\nif x = 5:\n    print(\"Five\")\n\n# Right\nif x == 5:\n    print(\"Five\")\n\n\n  Cell In[111], line 2\n    if x = 5:\n       ^\nSyntaxError: invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n\n\n\n\n\n\n4. Forgetting Quotes Around Strings\n\n# Wrong\nmessage = Hello\n\n# Right\nmessage = \"Hello\"\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[112], line 2\n      1 # Wrong\n----&gt; 2 message = Hello\n      4 # Right\n      5 message = \"Hello\"\n\nNameError: name 'Hello' is not defined\n\n\n\n\n\n5. Modifying a List While Iterating\n\n# Problematic\nnumbers = [1, 2, 3, 4, 5]\nfor num in numbers:\n    if num % 2 == 0:\n        numbers.remove(num)  # Modifying while iterating!\n\n# Better\nnumbers = [1, 2, 3, 4, 5]\nnumbers = [num for num in numbers if num % 2 != 0]\n\n\n\n6. Not Converting Types\n\n# Wrong\nage = input(\"Enter age: \")\nif age &gt; 18:  # Comparing string to number!\n    print(\"Adult\")\n\n# Right\nage = int(input(\"Enter age: \"))\nif age &gt; 18:\n    print(\"Adult\")\n\n\n\n\nDebugging Strategies\n\n1. Print Debugging\nAdd print statements to see what‚Äôs happening:\n\ndef calculate(x, y):\n    print(f\"x = {x}, y = {y}\")  # Debug print\n    result = x * y\n    print(f\"result = {result}\")  # Debug print\n    return result\n\n\n\n2. Check Variable Types\nUse type() to verify types:\n\nx = \"42\"\nprint(type(x))  # &lt;class 'str'&gt;\nx = int(x)\nprint(type(x))  # &lt;class 'int'&gt;\n\n&lt;class 'str'&gt;\n&lt;class 'int'&gt;\n\n\n\n\n3. Test Small Parts\nBreak your code into smaller pieces and test each part:\n# Instead of this all at once:\nresult = data.split()[0].upper().replace(\"X\", \"Y\")\n\n# Test step by step:\nstep1 = data.split()\nprint(step1)\nstep2 = step1[0]\nprint(step2)\nstep3 = step2.upper()\nprint(step3)\nresult = step3.replace(\"X\", \"Y\")\nprint(result)\n\n\n4. Use Try-Except (Advanced)\nHandle errors gracefully:\n\ntry:\n    number = int(input(\"Enter a number: \"))\n    result = 100 / number\n    print(result)\nexcept ValueError:\n    print(\"That's not a valid number!\")\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero!\")\n\n\n\n5. Read Documentation\nUse help() to understand how functions work:\n\nhelp(str.split)\n\n\n\n6. Search the Error\nCopy the error message (without your specific variable names) and search online:\n\nGood search: ‚Äúpython NameError name not defined‚Äù\nLess helpful: ‚Äúmy code doesn‚Äôt work‚Äù\n\n\n\n\nExercise A.1\nEach code snippet below has an error. Copy them into Colab one at a time and:\n\nRun the code and read the error message\nIdentify what type of error it is\nFix the error\n\nCode snippets:\n\n# Error 1\nprint(\"Hello World\"\n\n\n  Cell In[116], line 2\n    print(\"Hello World\"\n                       ^\n_IncompleteInputError: incomplete input\n\n\n\n\n\n# Error 2\nage = 25\nmessage = \"I am \" + age + \" years old\"\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[117], line 3\n      1 # Error 2\n      2 age = 25\n----&gt; 3 message = \"I am \" + age + \" years old\"\n\nTypeError: can only concatenate str (not \"int\") to str\n\n\n\n\n# Error 3\nwords = [\"apple\", \"banana\", \"cherry\"]\nprint(words[3])\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[118], line 3\n      1 # Error 3\n      2 words = [\"apple\", \"banana\", \"cherry\"]\n----&gt; 3 print(words[3])\n\nIndexError: list index out of range\n\n\n\n\n# Error 4\ndef greet():\nprint(\"Hello\")\n\n\n  Cell In[119], line 3\n    print(\"Hello\")\n    ^\nIndentationError: expected an indented block after function definition on line 2\n\n\n\n\n\n# Error 5\nx = 10\nif x &gt; 5\n    print(\"Big number\")\n\n\n  Cell In[120], line 3\n    if x &gt; 5\n            ^\nSyntaxError: expected ':'\n\n\n\n\n\n# Error 6\nbook = {\"title\": \"1984\", \"author\": \"Orwell\"}\nprint(book[\"year\"])\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[121], line 3\n      1 # Error 6\n      2 book = {\"title\": \"1984\", \"author\": \"Orwell\"}\n----&gt; 3 print(book[\"year\"])\n\nKeyError: 'year'\n\n\n\n\n# Error 7\ntext = \"Python\"\nresult = text.find()\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[122], line 3\n      1 # Error 7\n      2 text = \"Python\"\n----&gt; 3 result = text.find()\n\nTypeError: find expected at least 1 argument, got 0\n\n\n\n\n\nFinal Debugging Tips\n\n\n\n\n\n\nTipüí° Tips for Effective Debugging\n\n\n\n\nRead the error message carefully - Python tells you exactly what‚Äôs wrong\nCheck line numbers - But remember, the actual error might be on a previous line\nLook for typos - Variable names, function names, syntax\nVerify your assumptions - Use print() to check what values variables actually have\nSearch for help - You‚Äôre probably not the first person with this error\nTake breaks - Sometimes stepping away helps you see the problem fresh\nStart simple - Comment out code to isolate the problem\nDon‚Äôt panic - Every programmer deals with errors constantly. It‚Äôs normal!\n\n\n\n\nEnd of Tutorial\nYou now have a comprehensive reference for Python basics, from running your first code to debugging errors. Return to this document whenever you need to refresh your knowledge!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Text Analysis with Python",
    "section": "",
    "text": "Start here: Python Basics\nLab 01: Words as data points I"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Code and other materials for 1MEWI3647V Computational text analysis: An Introduction to text as data (WiSe25-26)\nQuestions, comments, suggestions? -&gt; Open an issue https://github.com/paskn/cta-with-python/issues\nOr email: sergei.pashakhin@uni-siegen.de"
  }
]