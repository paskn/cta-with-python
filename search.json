[
  {
    "objectID": "lab_02.html",
    "href": "lab_02.html",
    "title": "Lab 02: Words as data points II",
    "section": "",
    "text": "By the end of this lab, you will understand:\n\nHow to compare word usage across different groups or corpora\nWhat lemmatization is and why it matters for text analysis\nThe difference between stemming and lemmatization\nWhy simple frequency comparisons can be misleading\nHow to measure statistical significance with log-likelihood (G²)\nHow to quantify effect size with log odds ratio\nWhat named entities are and how to extract them\nHow to use spaCy for advanced NLP tasks in Python"
  },
  {
    "objectID": "lab_02.html#learning-objectives",
    "href": "lab_02.html#learning-objectives",
    "title": "Lab 02: Words as data points II",
    "section": "",
    "text": "By the end of this lab, you will understand:\n\nHow to compare word usage across different groups or corpora\nWhat lemmatization is and why it matters for text analysis\nThe difference between stemming and lemmatization\nWhy simple frequency comparisons can be misleading\nHow to measure statistical significance with log-likelihood (G²)\nHow to quantify effect size with log odds ratio\nWhat named entities are and how to extract them\nHow to use spaCy for advanced NLP tasks in Python"
  },
  {
    "objectID": "lab_02.html#introduction-why-compare-corpora",
    "href": "lab_02.html#introduction-why-compare-corpora",
    "title": "Lab 02: Words as data points II",
    "section": "2 Introduction: Why compare corpora?",
    "text": "2 Introduction: Why compare corpora?\nIn social and political science, texts often serve as proxies for social phenomena, sentiments, ideas, or discourses. A common research design involves collecting texts from different institutions, groups, or actors to create contrasting corpora. By comparing word usage across these corpora, we can infer something about the underlying social or political features of the entities they represent.\n\n2.1 The research question\nConsider this scenario: Do Democratic and Republican presidents talk differently? Not just in terms of political positions, but in the actual words they use?\nIn Lab 01, we compared authors based on pre-selected words (stop words, personal pronouns). This worked well for stylometry because function words are a closed class - we know all of them in advance.\nBut what about content words? If we want to compare the substance of what different groups talk about, how do we:\n\nAvoid arbitrary word selection?\nDistinguish meaningful differences from random variation?\nQuantify both the significance and magnitude of differences?\n\nThis is where corpus comparison methods come in.\n\n\n2.2 Our approach today\nWe’ll create two contrasting corpora:\n\nCorpus A: State of the Union addresses by Democratic presidents (since 1917)\nCorpus B: State of the Union addresses by Republican presidents (since 1917)\n\nThen we’ll use statistical measures to identify which words are significantly over- or under-used in one corpus compared to the other.\nKey insight: We’re not just looking for different words - we’re looking for statistically significant differences that reveal meaningful patterns."
  },
  {
    "objectID": "lab_02.html#setup-loading-packages",
    "href": "lab_02.html#setup-loading-packages",
    "title": "Lab 02: Words as data points II",
    "section": "3 Setup: Loading packages",
    "text": "3 Setup: Loading packages\n\n# Data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Text processing\nimport spacy\nfrom collections import Counter\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Statistical functions\nfrom scipy.stats import chi2\n\n# Set visualization style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\nprint(\"✓ Packages loaded successfully\")\n\n✓ Packages loaded successfully\n\n\n\n\n\n\n\n\nNoteAbout these packages\n\n\n\nNew packages in this lab:\n\nnumpy: Numerical computing (for log calculations)\nscipy.stats: Statistical functions (for significance testing)\n\nWe’ll continue using pandas, spaCy, and visualization libraries from Lab 01.\n\n\n\n3.1 Loading spaCy model\n\n# Load English language model\nnlp = spacy.load(\"en_core_web_sm\")\n\nnlp.max_length = 1530000        # https://github.com/explosion/spaCy/issues/13207#issuecomment-1865973378\n\nprint(f\"✓ spaCy model loaded: {nlp.meta['name']}\")\nprint(f\"  Language: {nlp.meta['lang']}\")\nprint(f\"  Components: {nlp.pipe_names}\")\n\n✓ spaCy model loaded: core_web_sm\n  Language: en\n  Components: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
  },
  {
    "objectID": "lab_02.html#loading-and-preparing-the-data",
    "href": "lab_02.html#loading-and-preparing-the-data",
    "title": "Lab 02: Words as data points II",
    "section": "4 Loading and preparing the data",
    "text": "4 Loading and preparing the data\nLet’s load our State of the Union dataset:\n\n# Load the data\nspeeches = pd.read_csv(\"data/transcripts.csv\")\n\nprint(f\"Total speeches: {len(speeches)}\")\nprint(f\"Date range: {speeches['date'].min()} to {speeches['date'].max()}\")\nprint(f\"\\nFirst few rows:\")\nspeeches.head()\n\nTotal speeches: 244\nDate range: 1790-01-08 to 2018-01-30\n\nFirst few rows:\n\n\n\n\n\n\n\n\n\ndate\npresident\ntitle\nurl\ntranscript\n\n\n\n\n0\n2018-01-30\nDonald J. Trump\nAddress Before a Joint Session of the Congress...\nhttps://www.cnn.com/2018/01/30/politics/2018-s...\n\\nMr. Speaker, Mr. Vice President, Members of ...\n\n\n1\n2017-02-28\nDonald J. Trump\nAddress Before a Joint Session of the Congress\nhttp://www.presidency.ucsb.edu/ws/index.php?pi...\nThank you very much. Mr. Speaker, Mr. Vice Pre...\n\n\n2\n2016-01-12\nBarack Obama\nAddress Before a Joint Session of the Congress...\nhttp://www.presidency.ucsb.edu/ws/index.php?pi...\nThank you. Mr. Speaker, Mr. Vice President, Me...\n\n\n3\n2015-01-20\nBarack Obama\nAddress Before a Joint Session of the Congress...\nhttp://www.presidency.ucsb.edu/ws/index.php?pi...\nThe President. Mr. Speaker, Mr. Vice President...\n\n\n4\n2014-01-28\nBarack Obama\nAddress Before a Joint Session of the Congress...\nhttp://www.presidency.ucsb.edu/ws/index.php?pi...\nThe President. Mr. Speaker, Mr. Vice President...\n\n\n\n\n\n\n\n\n4.1 Creating contrasting corpora\nWe’ll split speeches by party affiliation. First, let’s define which presidents belong to which party (since 1917):\n\n# Democratic presidents since 1917\ndemocrats = [\n    \"Woodrow Wilson\", \n    \"Franklin D. Roosevelt\", \n    \"Harry S. Truman\", \n    \"John F. Kennedy\", \n    \"Lyndon B. Johnson\", \n    \"Jimmy Carter\",\n    \"William J. Clinton\", \n    \"Barack Obama\"\n]\n\n# Filter speeches\nspeeches_after_1917 = speeches[speeches['date'] &gt; '1917-10-25'].copy()\n\n# Create party labels\nspeeches_after_1917['party'] = speeches_after_1917['president'].apply(\n    lambda x: 'Democrat' if x in democrats else 'Republican'\n)\n\n# Split into two corpora\ndem_speeches = speeches_after_1917[speeches_after_1917['party'] == 'Democrat']\nrep_speeches = speeches_after_1917[speeches_after_1917['party'] == 'Republican']\n\nprint(\"Democratic presidents:\")\nprint(dem_speeches['president'].value_counts())\nprint(f\"\\nTotal Democratic speeches: {len(dem_speeches)}\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"\\nRepublican presidents:\")\nprint(rep_speeches['president'].value_counts())\nprint(f\"\\nTotal Republican speeches: {len(rep_speeches)}\")\n\nDemocratic presidents:\npresident\nFranklin D. Roosevelt    13\nBarack Obama              8\nHarry S. Truman           8\nWilliam J. Clinton        8\nJimmy Carter              7\nLyndon B. Johnson         6\nWoodrow Wilson            4\nJohn F. Kennedy           3\nName: count, dtype: int64\n\nTotal Democratic speeches: 57\n\n==================================================\n\nRepublican presidents:\npresident\nRichard Nixon           12\nDwight D. Eisenhower    10\nRonald Reagan            8\nGeorge W. Bush           8\nCalvin Coolidge          6\nHerbert Hoover           4\nGeorge Bush              4\nGerald R. Ford           3\nDonald J. Trump          2\nWarren G. Harding        2\nName: count, dtype: int64\n\nTotal Republican speeches: 59\n\n\n\n\n\n\n\n\nTipWhy start in 1917?\n\n\n\nWe chose 1917 as a cutoff to:\n\nEnsure both parties have substantial representation\nFocus on relatively modern political language\nAvoid complications from 19th century political realignments\n\nIn your own research, such choices should be explicit and justified.\n\n\n\n\n4.2 Combining texts by party\nFor corpus comparison, we’ll combine all speeches from each party into two large text collections:\n\n# Combine all speeches by party\ndem_corpus = \" \".join(dem_speeches['transcript'].tolist())\nrep_corpus = \" \".join(rep_speeches['transcript'].tolist())\n\nprint(f\"Democratic corpus: {len(dem_corpus):,} characters\")\nprint(f\"Republican corpus: {len(rep_corpus):,} characters\")\n\nDemocratic corpus: 4,874,617 characters\nRepublican corpus: 4,176,938 characters"
  },
  {
    "objectID": "lab_02.html#from-wordforms-to-lemmas-introduction-to-lemmatization",
    "href": "lab_02.html#from-wordforms-to-lemmas-introduction-to-lemmatization",
    "title": "Lab 02: Words as data points II",
    "section": "5 From wordforms to lemmas: Introduction to lemmatization",
    "text": "5 From wordforms to lemmas: Introduction to lemmatization\n\n5.1 The problem with raw words\nConsider these sentences:\n\n“The government regulates industry.”\n“These regulations affect small businesses.”\n“The regulatory framework is complex.”\n\nThese three words - regulates, regulations, regulatory - are clearly related. They share the same root concept of “regulation.” But if we count them separately, we miss this connection.\nThis problem is especially acute in languages with rich inflection (Russian, German, Finnish), but it exists in English too:\n\nVerbs: walk, walks, walked, walking\nNouns: cat, cats, mouse, mice\nAdjectives: big, bigger, biggest\n\n\n\n5.2 Two solutions: Stemming vs lemmatization\nStemming: Crudely chop off word endings\n\nrunning → run\nbetter → bet (⚠️ wrong!)\norganization → organ (⚠️ wrong!)\nFast but imprecise\n\nLemmatization: Reduce words to their dictionary form (lemma)\n\nrunning → run\nbetter → good\nmice → mouse\nSlower but accurate\n\n\n\n5.3 How lemmatization works\nLemmatization requires:\n\nPart-of-speech information: Is “running” a verb or a noun?\nMorphological dictionary: What are all the forms of “run”?\nLinguistic rules: How do irregular forms work?\n\nFortunately, spaCy does all this for us!\n\n\n5.4 Lemmatization with spaCy\nLet’s see lemmatization in action:\n\n# Example text\nexample = \"The regulations are regulating industries more effectively than previous regulatory frameworks.\"\n\n# Process with spaCy\ndoc = nlp(example)\n\n# Show original word, lemma, and part of speech\nprint(\"Word → Lemma (Part of Speech)\")\nprint(\"-\" * 40)\nfor token in doc:\n    if not token.is_punct:\n        print(f\"{token.text:15} → {token.lemma_:15} ({token.pos_})\")\n\nWord → Lemma (Part of Speech)\n----------------------------------------\nThe             → the             (DET)\nregulations     → regulation      (NOUN)\nare             → be              (AUX)\nregulating      → regulate        (VERB)\nindustries      → industry        (NOUN)\nmore            → more            (ADV)\neffectively     → effectively     (ADV)\nthan            → than            (ADP)\nprevious        → previous        (ADJ)\nregulatory      → regulatory      (ADJ)\nframeworks      → framework       (NOUN)\n\n\nNotice how:\n\n“regulations” → “regulation”\n“are regulating” → “be regulate” (splits auxiliary verb)\n“regulatory” → “regulatory” (already base form)\n\n\n\n\n\n\n\nImportantWhen to lemmatize?\n\n\n\nUse lemmatization when:\n\nComparing content across documents\nBuilding topic models\nWorking with inflected languages\nVocabulary is too large\n\nDon’t lemmatize when:\n\nDoing stylometry (exact forms matter)\nStudying syntax or grammar\nTense/number/person is important\nTraining neural language models"
  },
  {
    "objectID": "lab_02.html#processing-our-corpora-with-spacy",
    "href": "lab_02.html#processing-our-corpora-with-spacy",
    "title": "Lab 02: Words as data points II",
    "section": "6 Processing our corpora with spaCy",
    "text": "6 Processing our corpora with spaCy\nNow let’s lemmatize both of our political corpora. This will take a few minutes as spaCy processes all the text.\n\n\n\n\n\n\nNoteProcessing time\n\n\n\nProcessing large texts with spaCy is computationally intensive. For very large corpora (millions of words), you might want to:\n\nUse spaCy’s nlp.pipe() for batch processing\nDisable unnecessary components (nlp.disable_pipes())\nSave processed results to disk\n\nFor this lab, the processing should take 2-5 minutes.\n\n\n\n# Process Democratic speeches (this takes time!)\nprint(\"Processing Democratic speeches...\")\ndem_doc = nlp(dem_corpus)\nprint(\"✓ Democratic corpus processed\")\n\n# Process Republican speeches\nprint(\"Processing Republican speeches...\")\nrep_doc = nlp(rep_corpus)\nprint(\"✓ Republican corpus processed\")\n\nFor the purposes of this lab, let’s work with a sample to speed things up:\n\n# Take a sample of each corpus for faster processing\ndem_sample = \" \".join(dem_speeches.sample(n=min(20, len(dem_speeches)), random_state=42)['transcript'].tolist())\nrep_sample = \" \".join(rep_speeches.sample(n=min(20, len(rep_speeches)), random_state=42)['transcript'].tolist())\n\n# Process samples\nprint(\"Processing samples...\")\ndem_doc = nlp(dem_sample)\nrep_doc = nlp(rep_sample)\nprint(\"✓ Processing complete\")\n\nprint(f\"\\nDemocratic sample: {len(dem_doc)} tokens\")\nprint(f\"Republican sample: {len(rep_doc)} tokens\")\n\nProcessing samples...\n✓ Processing complete\n\nDemocratic sample: 295013 tokens\nRepublican sample: 267382 tokens\n\n\n\n6.1 Extracting lemmas and filtering\nWe want to keep only content-bearing words. Let’s filter out:\n\nPunctuation (., ,, !, etc.)\nNumbers (1, 2020, million)\nSymbols ($, %, @)\nProper nouns (specific names of people and places)\nStop words (optional - let’s keep them for now to see what happens)\n\n\n# Extract lemmas from Democratic speeches\ndem_lemmas = []\nfor token in dem_doc:\n    if not token.is_punct and not token.is_space and token.pos_ not in ['NUM', 'SYM', 'PROPN']:\n        dem_lemmas.append({\n            'lemma': token.lemma_.lower(),\n            'pos': token.pos_,\n            'party': 'Democrat'\n        })\n\n# Extract lemmas from Republican speeches\nrep_lemmas = []\nfor token in rep_doc:\n    if not token.is_punct and not token.is_space and token.pos_ not in ['NUM', 'SYM', 'PROPN']:\n        rep_lemmas.append({\n            'lemma': token.lemma_.lower(),\n            'pos': token.pos_,\n            'party': 'Republican'\n        })\n\n# Combine into DataFrames\ndem_df = pd.DataFrame(dem_lemmas)\nrep_df = pd.DataFrame(rep_lemmas)\n\nprint(f\"Democratic lemmas: {len(dem_df):,}\")\nprint(f\"Republican lemmas: {len(rep_df):,}\")\nprint(f\"\\nSample of Democratic lemmas:\")\nprint(dem_df.head(10))\n\nDemocratic lemmas: 247,919\nRepublican lemmas: 225,161\n\nSample of Democratic lemmas:\n     lemma   pos     party\n0    thank  VERB  Democrat\n1      you  PRON  Democrat\n2       of   ADP  Democrat\n3       my  PRON  Democrat\n4   fellow   ADJ  Democrat\n5  tonight  NOUN  Democrat\n6     mark  VERB  Democrat\n7      the   DET  Democrat\n8   eighth   ADJ  Democrat\n9     year  NOUN  Democrat\n\n\n\n\n6.2 Creating frequency tables\nNow let’s count how often each lemma appears in each corpus:\n\n# Count frequencies by party\ndem_counts = dem_df['lemma'].value_counts().reset_index()\ndem_counts.columns = ['lemma', 'democrat']\n\nrep_counts = rep_df['lemma'].value_counts().reset_index()\nrep_counts.columns = ['lemma', 'republican']\n\n# Merge into one table\nfreq_table = dem_counts.merge(rep_counts, on='lemma', how='outer').fillna(0)\n\n# Convert to integers\nfreq_table['democrat'] = freq_table['democrat'].astype(int)\nfreq_table['republican'] = freq_table['republican'].astype(int)\n\n# Filter out very rare words (appears less than 10 times in both corpora)\nfreq_table = freq_table[(freq_table['democrat'] &gt; 10) | (freq_table['republican'] &gt; 10)].copy()\n\nprint(f\"Unique lemmas (after filtering): {len(freq_table):,}\")\nprint(f\"\\nTop 20 by total frequency:\")\nfreq_table['total'] = freq_table['democrat'] + freq_table['republican']\nprint(freq_table.sort_values('total', ascending=False).head(20))\n\nUnique lemmas (after filtering): 2,072\n\nTop 20 by total frequency:\n     lemma  democrat  republican  total\n6817   the     14722       14995  29717\n4617    of      8965        9654  18619\n395    and      9610        8446  18056\n6915    to      9536        7944  17480\n708     be      8286        8421  16707\n3462    in      5361        5390  10751\n7507    we      5757        4214   9971\n94       a      4410        4106   8516\n4700   our      4423        3595   8018\n6816  that      3850        2843   6693\n3187  have      3386        3183   6569\n2805   for      2936        2765   5701\n3364     i      2742        1921   4663\n7583  will      2390        1964   4354\n3797    it      1995        1740   3735\n6849  this      1952        1738   3690\n4548   not      1780        1372   3152\n7604  with      1626        1383   3009\n4640    on      1419        1445   2864\n6842  they      1658        1065   2723"
  },
  {
    "objectID": "lab_02.html#the-problem-with-simple-frequency-comparisons",
    "href": "lab_02.html#the-problem-with-simple-frequency-comparisons",
    "title": "Lab 02: Words as data points II",
    "section": "7 The problem with simple frequency comparisons",
    "text": "7 The problem with simple frequency comparisons\nLooking at raw frequencies is tempting, but it can be misleading. Let’s see why.\n\n7.1 Corpus size matters\n\n# Total tokens per party\ntotal_dem = freq_table['democrat'].sum()\ntotal_rep = freq_table['republican'].sum()\n\nprint(f\"Total Democratic tokens: {total_dem:,}\")\nprint(f\"Total Republican tokens: {total_rep:,}\")\nprint(f\"Ratio (Dem/Rep): {total_dem/total_rep:.2f}\")\n\nTotal Democratic tokens: 234,922\nTotal Republican tokens: 212,286\nRatio (Dem/Rep): 1.11\n\n\nIf one corpus is larger, it will naturally have higher raw counts for most words. We need to account for this.\n\n\n7.2 Example: The word “people”\n\n# Look at a specific word\npeople_row = freq_table[freq_table['lemma'] == 'people']\n\nif len(people_row) &gt; 0:\n    dem_count = people_row['democrat'].values[0]\n    rep_count = people_row['republican'].values[0]\n    \n    # Raw counts\n    print(f\"Raw counts for 'people':\")\n    print(f\"  Democrats: {dem_count}\")\n    print(f\"  Republicans: {rep_count}\")\n    print(f\"  Difference: {dem_count - rep_count}\")\n    \n    # Normalized (per 1000 words)\n    dem_rate = (dem_count / total_dem) * 1000\n    rep_rate = (rep_count / total_rep) * 1000\n    \n    print(f\"\\nNormalized rates (per 1,000 words):\")\n    print(f\"  Democrats: {dem_rate:.2f}\")\n    print(f\"  Republicans: {rep_rate:.2f}\")\n    print(f\"  Difference: {dem_rate - rep_rate:.2f}\")\n\nRaw counts for 'people':\n  Democrats: 972\n  Republicans: 687\n  Difference: 285\n\nNormalized rates (per 1,000 words):\n  Democrats: 4.14\n  Republicans: 3.24\n  Difference: 0.90\n\n\nThe raw difference might be large just because one corpus is bigger!\n\n\n7.3 Two questions we need to answer\n\nIs the difference statistically significant?\n\nCould this difference occur by chance?\nHow confident can we be that it’s a real pattern?\n→ We’ll use log-likelihood (G²) for this\n\nHow large is the effect?\n\nIs it a huge difference or a tiny one?\nWhich words show the strongest contrast?\n→ We’ll use log odds ratio for this\n\n\n\n\n\n\n\n\nNoteStatistical significance ≠ practical importance\n\n\n\nA difference can be:\n\nStatistically significant but tiny (large sample)\nLarge but not significant (small sample)\n\nWe need both measures to draw meaningful conclusions."
  },
  {
    "objectID": "lab_02.html#measuring-significance-log-likelihood-g²",
    "href": "lab_02.html#measuring-significance-log-likelihood-g²",
    "title": "Lab 02: Words as data points II",
    "section": "8 Measuring significance: Log-likelihood (G²)",
    "text": "8 Measuring significance: Log-likelihood (G²)\n\n8.1 The problem: When is a difference real?\nLet’s say we’re comparing Republican and Democratic speeches, and we find that the word “freedom” appears:\n\n100 times in Republican speeches\n50 times in Democratic speeches\n\nShould we conclude that Republicans talk twice as much about freedom?\nNot necessarily. Here’s why: What if the Republican corpus contains 1,000,000 words total, while the Democratic corpus contains 500,000 words? Then both parties use “freedom” at exactly the same rate (100 per million words). The difference in raw counts is simply because we have more Republican text.\nThis is why we need a statistical test that accounts for corpus size.\n\n\n8.2 What is log-likelihood (G²)?\nLog-likelihood, abbreviated as G², is a statistical test that answers one simple question:\n\n“Given the sizes of my two corpora, how surprising is this word’s distribution?”\n\nThe logic:\n\nIf a word is distributed just as we’d expect (proportional to corpus size), G² is close to 0\nIf the distribution is very different from what we’d expect, G² is large\nThe larger G², the more confident we can be that the difference is real, not just random variation\n\nThink of G² as a “surprise meter” - it measures how surprised we should be by what we observe.\n\n\n8.3 How to read G² values\nG² follows a well-known statistical distribution, which means we have standard thresholds for interpretation:\n\n\n\n\n\n\n\n\nG² value\nConfidence level\nWhat it means\n\n\n\n\n&lt; 3.84\nNot significant\nDifference might be random chance\n\n\n&gt; 3.84\n95% confident\nProbably a real pattern (p &lt; 0.05)\n\n\n&gt; 6.63\n99% confident\nVery likely a real pattern (p &lt; 0.01)\n\n\n&gt; 10.83\n99.9% confident\nAlmost certainly a real pattern (p &lt; 0.001)\n\n\n\nRule of thumb: We typically use G² &gt; 6.63 as our cutoff for trusting a difference.\n\n\n\n\n\n\nNoteWhat does “99% confident” mean?\n\n\n\nIt means: “If there were actually no real difference, we’d see a result this extreme less than 1% of the time.” In other words, we’re very confident the pattern is real, not just luck.\n\n\n\n\n\n\n\n\nNoteFor the mathematically curious: How G² is calculated\n\n\n\n\n\nG² compares observed frequencies (what we actually see) to expected frequencies (what we’d see if words were distributed proportionally to corpus size).\nThe formula is:\n\\[G^2 = 2 \\sum O \\times \\ln\\left(\\frac{O}{E}\\right)\\]\nWhere:\n\n\\(O\\) = observed frequency\n\\(E\\) = expected frequency\n\\(\\ln\\) = natural logarithm\n\nFor two corpora, this expands to:\n\\[G^2 = 2 \\times \\left[ a \\times \\ln\\left(\\frac{a}{E_1}\\right) + b \\times \\ln\\left(\\frac{b}{E_2}\\right) \\right]\\]\nWhere:\n\n\\(a\\) = word count in Corpus A\n\\(b\\) = word count in Corpus B\n\\(E_1\\) = expected count in Corpus A\n\\(E_2\\) = expected count in Corpus B\n\nThe expected frequencies account for corpus size:\n\\[E_1 = C \\times \\frac{a + b}{C + D}\\] \\[E_2 = D \\times \\frac{a + b}{C + D}\\]\nWhere:\n\n\\(C\\) = total size of Corpus A\n\\(D\\) = total size of Corpus B\n\nThis test is based on Dunning (1993), a foundational paper in corpus linguistics. It’s preferred over chi-squared for text data because it handles sparse data (rare words) more reliably.\n\n\n\n\n\n8.4 Calculating G² in Python\nWe’ll create a function that does all the mathematical work for us:\n\ndef log_likelihood(a, b):\n    \"\"\"\n    Calculate log-likelihood (G²) for word frequencies in two corpora.\n\n    This function compares observed word frequencies to expected frequencies\n    (based on corpus size) and returns a G² value indicating how surprising\n    the observed distribution is.\n\n    Parameters:\n    -----------\n    a : array-like\n        Word counts in corpus A (e.g., Democratic speeches)\n    b : array-like\n        Word counts in corpus B (e.g., Republican speeches)\n\n    Returns:\n    --------\n    array-like\n        G² values for each word (higher = more surprising/significant)\n    \"\"\"\n    # Total corpus sizes\n    C = np.sum(a)  # Total tokens in corpus A\n    D = np.sum(b)  # Total tokens in corpus B\n\n    # Calculate expected frequencies (what we'd expect if words were distributed proportionally)\n    E1 = C * ((a + b) / (C + D))\n    E2 = D * ((a + b) / (C + D))\n\n    # Calculate G² statistic\n    # Note: We add a tiny constant (1e-10) to avoid mathematical errors when counts are zero\n    g2 = 2 * ((a * np.log(a / E1 + 1e-10)) + (b * np.log(b / E2 + 1e-10)))\n\n    return g2\n\n\n\n8.5 Using G² to find significant differences\n\n# Calculate log-likelihood for all words\nfreq_table['g2'] = log_likelihood(\n    freq_table['democrat'].values, \n    freq_table['republican'].values\n)\n\n# Sort by G² (most significant differences)\nfreq_table_sorted = freq_table.sort_values('g2', ascending=False).copy()\n\nprint(\"Words with highest G² (most significant differences):\")\nprint(freq_table_sorted[['lemma', 'democrat', 'republican', 'g2']].head(20))\n\nWords with highest G² (most significant differences):\n            lemma  democrat  republican          g2\n2095           do      1380         612  231.203248\n2986          get       421         122  145.273635\n4617           of      8965        9654  143.074502\n7675          you       941         443  136.369324\n7625         work      1034         505  135.944566\n7507           we      5757        4214  108.982754\n5829        right       503         198  108.139684\n6817          the     14722       14995  106.400966\n7343           us       174          28  103.171606\n1226      college       174          29  100.740012\n3123          gun        78           0  100.428162\n3813          job       544         232   99.441736\n1061         cent         6          86   91.520368\n5163      present        86         238   90.417812\n7555        which       805        1117   87.414119\n7560          who       837         443   86.633621\n1278      company       120          14   85.637924\n4673           or       930         514   83.115776\n2546  expenditure        28         130   82.156755\n6803    terrorist        42         156   81.910170\n\n\nLook at the G² values. Many are well above 6.63, meaning we can be very confident these differences are real.\n\n\n8.6 How many significant differences did we find?\nLet’s count how many words show statistically significant differences at different confidence levels:\n\n# Count significant differences\nsig_05 = (freq_table['g2'] &gt; 3.84).sum()\nsig_01 = (freq_table['g2'] &gt; 6.63).sum()\nsig_001 = (freq_table['g2'] &gt; 10.83).sum()\n\nprint(f\"Significant differences:\")\nprint(f\"  95% confident (G² &gt; 3.84):   {sig_05} words\")\nprint(f\"  99% confident (G² &gt; 6.63):   {sig_01} words\")\nprint(f\"  99.9% confident (G² &gt; 10.83): {sig_001} words\")\nprint(f\"\\nTotal words tested: {len(freq_table)}\")\n\nSignificant differences:\n  95% confident (G² &gt; 3.84):   994 words\n  99% confident (G² &gt; 6.63):   737 words\n  99.9% confident (G² &gt; 10.83): 487 words\n\nTotal words tested: 2072\n\n\nSo we have hundreds of words with statistically significant differences. But are they all interesting?\n\n\n8.7 The problem: Stop words dominate\nNot all statistically significant differences are interesting. Let’s check what kinds of words have the highest G² values:\n\n# Load stop words from spaCy\nstop_words = nlp.Defaults.stop_words\n\n# Check if top G² words are stop words\nfreq_table_sorted['is_stopword'] = freq_table_sorted['lemma'].isin(stop_words)\n\nprint(\"Top 20 by G² - are they stop words?\")\nprint(freq_table_sorted[['lemma', 'g2', 'is_stopword']].head(20))\n\nTop 20 by G² - are they stop words?\n            lemma          g2  is_stopword\n2095           do  231.203248         True\n2986          get  145.273635         True\n4617           of  143.074502         True\n7675          you  136.369324         True\n7625         work  135.944566        False\n7507           we  108.982754         True\n5829        right  108.139684        False\n6817          the  106.400966         True\n7343           us  103.171606         True\n1226      college  100.740012        False\n3123          gun  100.428162        False\n3813          job   99.441736        False\n1061         cent   91.520368        False\n5163      present   90.417812        False\n7555        which   87.414119         True\n7560          who   86.633621         True\n1278      company   85.637924        False\n4673           or   83.115776         True\n2546  expenditure   82.156755        False\n6803    terrorist   81.910170        False\n\n\nNotice that many high-G² words are stop words (words like “the”, “and”, “of”).\nWhy does this happen?\n\nStop words appear thousands of times in our corpora\nG² is sensitive to absolute frequencies - when a word appears 5,000 times, even a small proportional difference produces high G²\nA word that’s 51% vs 49% between corpora can have higher G² than a word that’s 90% vs 10%, just because the first word is more common overall\n\nThe solution: Filter to focus on content words (nouns, verbs, adjectives) by removing stop words.\n\n# Focus on content words by removing stop words\ncontent_words = freq_table_sorted[~freq_table_sorted['is_stopword']].copy()\n\nprint(\"Top 20 content words by G²:\")\nprint(content_words[['lemma', 'democrat', 'republican', 'g2']].head(20))\n\nTop 20 content words by G²:\n            lemma  democrat  republican          g2\n7625         work      1034         505  135.944566\n5829        right       503         198  108.139684\n1226      college       174          29  100.740012\n3123          gun        78           0  100.428162\n3813          job       544         232   99.441736\n1061         cent         6          86   91.520368\n5163      present        86         238   90.417812\n1278      company       120          14   85.637924\n2546  expenditure        28         130   82.156755\n6803    terrorist        42         156   81.910170\n7477         want       326         119   80.271861\n3164         hard       196          51   76.786385\n1582        court        18         103   74.890387\n3866         know       423         186   72.262874\n5282     property         6          67   66.090068\n776           big       120          25   58.448358\n6891         tile         0          38   56.626794\n5806      revenue        22          94   55.715059\n4211      measure        66         166   55.275584\n1830    dependent         2          46   54.495605\n\n\nMuch better! Now we’re seeing substantive words about policy, governance, and political issues.\n\n\n8.8 What G² doesn’t tell us\nG² tells us that a difference exists and how confident we can be about it. But it doesn’t tell us:\n\nWhich corpus uses the word more\nHow much more it’s used\n\nFor that, we need another measure: log odds ratio."
  },
  {
    "objectID": "lab_02.html#measuring-effect-size-log-odds-ratio",
    "href": "lab_02.html#measuring-effect-size-log-odds-ratio",
    "title": "Lab 02: Words as data points II",
    "section": "9 Measuring effect size: Log odds ratio",
    "text": "9 Measuring effect size: Log odds ratio\n\n9.1 The problem: G² doesn’t tell us everything\nLook back at the content words with high G² values. Can you quickly tell which party uses each word more? Is “health” more Democratic or Republican? What about “security”?\nG² told us that differences exist and that they’re statistically significant. But it doesn’t tell us:\n\nDirection: Which corpus uses the word more?\nMagnitude: Is it slightly more common, or dramatically more common?\n\nFor this, we need a different measure: log odds ratio.\n\n\n9.2 What is log odds ratio?\nLog odds ratio is a measure of effect size that answers:\n\n“How much more is this word used in one corpus compared to the other?”\n\nIt gives us two pieces of information:\n\nThe sign (+ or -) tells us which corpus uses the word more\nThe number tells us how much more it’s used\n\n\n\n9.3 How to read log odds values\nIn our analysis, we calculate log odds where:\n\nPositive values = word is more common in Democratic speeches\nNegative values = word is more common in Republican speeches\nZero = word is equally common in both\n\nThe magnitude tells us how big the difference is:\n\n\n\nLog Odds\nMeaning\n\n\n\n\n+1.0\nWord is 2× more common in Democratic speeches\n\n\n+2.0\nWord is 4× more common in Democratic speeches\n\n\n+3.0\nWord is 8× more common in Democratic speeches\n\n\n-1.0\nWord is 2× more common in Republican speeches\n\n\n-2.0\nWord is 4× more common in Republican speeches\n\n\n0.0\nWord is equally common in both\n\n\n\n\n\n9.4 A concrete example\nLet’s say the word “healthcare” appears:\n\n200 times in Democratic speeches (out of 100,000 total Democratic words)\n50 times in Republican speeches (out of 100,000 total Republican words)\n\nThe proportions are:\n\nDemocratic: 200/100,000 = 0.002 (0.2%)\nRepublican: 50/100,000 = 0.0005 (0.05%)\n\nThe ratio is 0.002/0.0005 = 4.0 (Democrats use it 4× more often).\nThe log₂(4.0) = 2.0\nSo this word would have a log odds ratio of +2.0, meaning Democrats use it 4× more than Republicans.\n\n\n\n\n\n\nNoteWhy use logarithm?\n\n\n\n\n\nRaw ratios are asymmetric and hard to interpret:\n\n“2× more common” = ratio of 2.0\n“2× less common” = ratio of 0.5\n\nThese don’t look symmetric even though they represent the same magnitude of difference.\nTaking the logarithm makes them symmetric:\n\n2× more common: log₂(2.0) = +1.0\n2× less common: log₂(0.5) = -1.0\n\nWe use base-2 logarithm (log₂) because it’s easy to interpret:\n\nEach +1 means “doubled”\nEach -1 means “halved”\n\nThis makes effect sizes comparable across different words.\n\n\n\n\n\n\n\n\n\nNoteThe mathematical formula\n\n\n\n\n\nLog odds ratio is calculated as:\n\\[\\text{Log Odds Ratio} = \\log_2\\left(\\frac{a/C}{b/D}\\right)\\]\nWhere:\n\n\\(a\\) = word count in Corpus A (Democrats)\n\\(b\\) = word count in Corpus B (Republicans)\n\\(C\\) = total size of Corpus A\n\\(D\\) = total size of Corpus B\n\nThis simplifies to comparing the proportions (a/C vs b/D) of how often each corpus uses the word.\n\n\n\n\n\n9.5 Calculating log odds ratio in Python\nLet’s create a function to calculate log odds ratio for all our words:\n\ndef log_odds_ratio(a, b):\n    \"\"\"\n    Calculate log odds ratio for word frequencies in two corpora.\n\n    This function compares how often words appear in each corpus\n    (accounting for corpus size) and returns a number telling us\n    which corpus uses each word more and by how much.\n\n    Positive values = more common in corpus A (Democrats)\n    Negative values = more common in corpus B (Republicans)\n    Magnitude = how much more (1 = 2×, 2 = 4×, 3 = 8×, etc.)\n\n    Parameters:\n    -----------\n    a : array-like\n        Word counts in corpus A (e.g., Democratic speeches)\n    b : array-like\n        Word counts in corpus B (e.g., Republican speeches)\n\n    Returns:\n    --------\n    array-like\n        Log odds ratios (base 2) for each word\n    \"\"\"\n    # Total corpus sizes\n    C = np.sum(a)  # Total words in corpus A\n    D = np.sum(b)  # Total words in corpus B\n\n    # Calculate proportions (what percentage of each corpus is this word?)\n    prop_a = a / C\n    prop_b = b / D\n\n    # Calculate log odds ratio\n    # Note: We add a tiny constant (1e-10) to avoid mathematical errors when counts are zero\n    lor = np.log2((prop_a + 1e-10) / (prop_b + 1e-10))\n\n    return lor\n\n\n\n9.6 Using log odds ratio to see which party uses each word\n\n# Calculate log odds ratio\nfreq_table['log_odds'] = log_odds_ratio(\n    freq_table['democrat'].values,\n    freq_table['republican'].values\n)\n\n# Add to our content words table too\ncontent_words['log_odds'] = log_odds_ratio(\n    content_words['democrat'].values,\n    content_words['republican'].values\n)\n\nprint(\"Words most strongly associated with Democrats (positive log odds):\")\nprint(content_words.nlargest(15, 'log_odds')[['lemma', 'democrat', 'republican', 'log_odds', 'g2']])\n\nprint(\"\\nWords most strongly associated with Republicans (negative log odds):\")\nprint(content_words.nsmallest(15, 'log_odds')[['lemma', 'democrat', 'republican', 'log_odds', 'g2']])\n\nWords most strongly associated with Democrats (positive log odds):\n          lemma  democrat  republican   log_odds          g2\n3123        gun        78           0  23.022742  100.428162\n4042   lobbyist        32           0  21.737340   41.201297\n3706   internet        30           0  21.644231   38.626216\n2085  diversity        28           0  21.544695   36.051135\n6756       tech        24           0  21.322303   30.900973\n6152     shrink        24           0  21.322303   30.900973\n1397   conquest        22           0  21.196772   28.325892\n4817  paperwork        22           0  21.196772   28.325892\n4238     mental        20           0  21.059268   25.750811\n7081         tv        20           0  21.059268   25.750811\n6763       teen        18           0  20.907265   23.175730\n91         96th        18           0  20.907265   23.175730\n6734       tank        18           0  20.907265   23.175730\n5617   reinvent        18           0  20.907265   23.175730\n2887     french        18           0  20.907265   23.175730\n\nWords most strongly associated with Republicans (negative log odds):\n            lemma  democrat  republican   log_odds         g2\n6891         tile         0          38 -22.100187  56.626794\n53           11th         0          28 -21.659614  41.725006\n3090        gross         0          26 -21.552699  38.744648\n7036     tribunal         0          22 -21.311691  32.783933\n3517        index         0          20 -21.174187  29.803576\n244      advisory         0          20 -21.174187  29.803576\n5256  prohibition         0          19 -21.100187  28.313397\n3028     governor         0          16 -20.852259  23.842860\n6279       solely         0          16 -20.852259  23.842860\n6808      testing         0          16 -20.852259  23.842860\n5564        refer         0          16 -20.852259  23.842860\n5442     reaction         0          16 -20.852259  23.842860\n6082      seventy         0          16 -20.852259  23.842860\n7342     urgently         0          16 -20.852259  23.842860\n5455      realism         0          15 -20.759150  22.352682\n\n\nNow we can see the full picture! Look at the output:\n\nPositive log odds (e.g., +2.5) means Democrats use this word more (roughly 2^2.5 ≈ 5-6× more often)\nNegative log odds (e.g., -1.8) means Republicans use this word more (roughly 2^1.8 ≈ 3-4× more often)\n\n\n\n9.7 Reading the results: Putting it all together\nFor each word, we now have three key numbers:\n\nDemocrat count / Republican count: Raw frequencies (affected by corpus size)\nLog odds ratio: Effect size - which party uses it more and by how much\nG² value: Statistical significance - how confident we can be\n\nExample interpretation:\nIf you see a word with: - Log odds = +2.0 - G² = 45.3\nThis means: “Democrats use this word about 4× more often than Republicans, and we’re extremely confident (p &lt; 0.001) this is a real pattern, not chance.”\n\n\n\n\n\n\nTipBest practice: Filter for both significance AND effect size\n\n\n\nNot every statistically significant difference is interesting. And not every large difference is reliable.\nThe most meaningful words are those that pass three tests:\n\nStatistically significant: G² &gt; 6.63 (we’re 99% confident it’s real)\nLarge effect: |log odds| &gt; 0.5 (at least 40% more frequent in one corpus)\nNot too rare: Appears at least 5 times in both corpora (reliable measurement)\n\nOnly words that pass all three tests are truly distinctive and reliable.\n\n\n\n\n9.8 Finding the most meaningful differences\nLet’s filter our results to find words that are both statistically significant AND show large effects:\n\n# Find meaningful differences - must pass all three tests\nmeaningful = content_words[\n    (content_words['g2'] &gt; 6.63) &                    # Test 1: Statistically significant\n    (np.abs(content_words['log_odds']) &gt; 0.5) &       # Test 2: Large effect size\n    (content_words['democrat'] &gt; 5) &                 # Test 3: Not too rare\n    (content_words['republican'] &gt; 5)\n].copy()\n\nprint(f\"Words with significant AND large differences: {len(meaningful)}\")\nprint(\"\\nTop 10 most distinctively Democratic words:\")\nprint(meaningful.nlargest(10, 'log_odds')[['lemma', 'democrat', 'republican', 'log_odds', 'g2']])\n\nprint(\"\\nTop 10 most distinctively Republican words:\")\nprint(meaningful.nsmallest(10, 'log_odds')[['lemma', 'democrat', 'republican', 'log_odds', 'g2']])\n\nWords with significant AND large differences: 376\n\nTop 10 most distinctively Democratic words:\n           lemma  democrat  republican  log_odds          g2\n1278     company       120          14  2.984616   85.637924\n4296     minimum        56           8  2.692435   35.797119\n2469   everybody        37           6  2.509570   21.825902\n1226     college       174          29  2.470043  100.740012\n3605  innovation        54          10  2.318039   28.953921\n4071         lot        78          15  2.263592   40.605421\n282   aggression        80          16  2.207008   40.338351\n2955         gas        60          12  2.207008   30.253763\n776          big       120          25  2.148115   58.448358\n5008      planet        28           6  2.107472   13.304258\n\nTop 10 most distinctively Republican words:\n          lemma  democrat  republican  log_odds         g2\n1061       cent         6          86 -3.956219  91.520368\n5282   property         6          67 -3.596044  66.090068\n3765      iraqi         6          48 -3.114917  41.579958\n1263  commodity         6          42 -2.922272  34.142816\n1743   decrease         6          40 -2.851883  31.708856\n826       board         6          40 -2.851883  31.708856\n5588     regime        10          61 -2.723727  46.054082\n2921   function         8          48 -2.699880  35.895878\n4907      pende         6          35 -2.659238  25.744069\n1582      court        18         103 -2.631494  74.890387\n\n\nThese are the words that truly distinguish Democratic from Republican political rhetoric - they’re both statistically reliable and substantively important."
  },
  {
    "objectID": "lab_02.html#visualizing-differences",
    "href": "lab_02.html#visualizing-differences",
    "title": "Lab 02: Words as data points II",
    "section": "10 Visualizing differences",
    "text": "10 Visualizing differences\n\n10.1 Bar chart of log odds ratios\n\n# Get top 15 for each party\ntop_dem = meaningful.nlargest(15, 'log_odds')\ntop_rep = meaningful.nsmallest(15, 'log_odds')\ntop_both = pd.concat([top_dem, top_rep])\n\n# Sort by log odds for plotting\ntop_both = top_both.sort_values('log_odds')\n\n# Create plot\nfig, ax = plt.subplots(figsize=(10, 8))\n\ncolors = ['#0015BC' if x &gt; 0 else '#E81B23' for x in top_both['log_odds']]\nax.barh(range(len(top_both)), top_both['log_odds'], color=colors, alpha=0.7)\nax.set_yticks(range(len(top_both)))\nax.set_yticklabels(top_both['lemma'])\nax.axvline(0, color='black', linewidth=0.8, linestyle='--')\nax.set_xlabel('Log Odds Ratio (negative = Republican, positive = Democrat)', fontsize=12)\nax.set_title('Most Distinctive Words by Party', fontsize=14, fontweight='bold')\n\n# Add legend\nfrom matplotlib.patches import Patch\nlegend_elements = [\n    Patch(facecolor='#0015BC', alpha=0.7, label='More Democratic'),\n    Patch(facecolor='#E81B23', alpha=0.7, label='More Republican')\n]\nax.legend(handles=legend_elements, loc='lower right')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n10.2 Scatter plot: Significance vs effect size\nA scatter plot helps us visualize the relationship between effect size (log odds ratio) and statistical significance (G²).\nHowever, we need to be careful about very rare words. Words that appear only once or twice in one corpus but zero times in the other create extreme log odds ratios (dividing by near-zero) with low statistical significance. These are statistical artifacts, not meaningful patterns.\nTo avoid misleading visualizations, we’ll filter out words that don’t appear at least 5 times in both corpora:\n\n# Create scatter plot\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Filter for plotting - remove very rare words that create artifacts\nplot_data = content_words[\n    (content_words['democrat'] &gt;= 5) &\n    (content_words['republican'] &gt;= 5)\n].copy()\n\n# Color by which party uses word more\ncolors = ['#0015BC' if x &gt; 0 else '#E81B23' for x in plot_data['log_odds']]\n\nax.scatter(plot_data['log_odds'], plot_data['g2'],\n           c=colors, alpha=0.5, s=30)\n\n# Add significance threshold line\nax.axhline(6.63, color='gray', linestyle='--', linewidth=1, alpha=0.7, label='p &lt; 0.01')\n\n# Add effect size threshold lines\nax.axvline(-0.5, color='gray', linestyle=':', linewidth=1, alpha=0.5)\nax.axvline(0.5, color='gray', linestyle=':', linewidth=1, alpha=0.5)\n\nax.set_xlabel('Log Odds Ratio (negative = Republican, positive = Democrat)', fontsize=12)\nax.set_ylabel('Log-Likelihood (G²)', fontsize=12)\nax.set_title('Effect Size vs Statistical Significance', fontsize=14, fontweight='bold')\n\n# Annotate some interesting words\nfor _, row in meaningful.head(10).iterrows():\n    ax.annotate(row['lemma'], \n                (row['log_odds'], row['g2']),\n                fontsize=8, alpha=0.7,\n                xytext=(5, 5), textcoords='offset points')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis scatter plot shows the relationship between:\n\nX-axis: Effect size (how different?)\nY-axis: Statistical significance (how confident?)\n\nThe most interesting words are in the upper left and upper right corners - both statistically significant (high G²) and distinctive (large absolute log odds ratio). These are the words that show strong, reliable differences between the two parties.\nWords near the bottom (low G²) may have large log odds ratios but aren’t statistically reliable - often because they’re too rare. The horizontal line at G² = 6.63 marks the p &lt; 0.01 significance threshold."
  },
  {
    "objectID": "lab_02.html#named-entity-recognition",
    "href": "lab_02.html#named-entity-recognition",
    "title": "Lab 02: Words as data points II",
    "section": "11 Named entity recognition",
    "text": "11 Named entity recognition\nSo far we’ve analyzed individual words (lemmas). But sometimes we’re interested in references to real-world entities:\n\nPERSON: Barack Obama, Hillary Clinton\nORG: United Nations, Department of Defense\nGPE: America, Iraq, New York\nDATE: tomorrow, 2020, next year\nMONEY: $1 billion, five dollars\n\nThis is called Named Entity Recognition (NER), and spaCy does it automatically!\n\n11.1 How NER works\nNER is a classification task:\n\nIdentify spans of text that might be entities\nClassify each span into entity types\nUse machine learning models trained on annotated data\n\nModern NER systems use neural networks trained on large corpora of hand-labeled examples.\n\n\n11.2 Extracting entities with spaCy\nLet’s look at entities in a sample speech:\n\n# Get one speech\nsample_speech = dem_speeches.iloc[0]['transcript'][:1000]  # First 1000 chars\n\n# Process it\nsample_doc = nlp(sample_speech)\n\n# Display entities\nprint(\"Named entities found:\\n\")\nprint(f\"{'Entity':&lt;25} {'Type':&lt;15} {'Explanation'}\")\nprint(\"-\" * 65)\n\nfor ent in sample_doc.ents:\n    print(f\"{ent.text:&lt;25} {ent.label_:&lt;15} {spacy.explain(ent.label_)}\")\n\nNamed entities found:\n\nEntity                    Type            Explanation\n-----------------------------------------------------------------\nSpeaker                   PERSON          People, including fictional\nCongress                  ORG             Companies, agencies, institutions, etc.\nAmericans                 NORP            Nationalities or religious or political groups\nTonight                   TIME            Times smaller than a day\nthe eighth year           DATE            Absolute or relative dates or periods\nthe State of the Union    ORG             Companies, agencies, institutions, etc.\nIowa                      GPE             Countries, cities, states\nan election season        DATE            Absolute or relative dates or periods\nthis year                 DATE            Absolute or relative dates or periods\nSpeaker                   PERSON          People, including fictional\nthe end of last year      DATE            Absolute or relative dates or periods\nthis year                 DATE            Absolute or relative dates or periods\ntonight                   TIME            Times smaller than a day\nthe year ahead            DATE            Absolute or relative dates or periods\nDon                       PERSON          People, including fictional\n\n\n\n\n11.3 Comparing entity usage across parties\nLet’s extract all location entities (GPE = Geo-Political Entity) from both corpora:\n\n# Extract GPE entities from both corpora\ndem_locations = [ent.text.lower() for ent in dem_doc.ents if ent.label_ == 'GPE']\nrep_locations = [ent.text.lower() for ent in rep_doc.ents if ent.label_ == 'GPE']\n\nprint(f\"Democratic location mentions: {len(dem_locations)}\")\nprint(f\"Republican location mentions: {len(rep_locations)}\")\n\n# Count frequencies\ndem_loc_counts = pd.Series(dem_locations).value_counts().reset_index()\ndem_loc_counts.columns = ['location', 'democrat']\n\nrep_loc_counts = pd.Series(rep_locations).value_counts().reset_index()\nrep_loc_counts.columns = ['location', 'republican']\n\n# Merge\nlocation_freq = dem_loc_counts.merge(rep_loc_counts, on='location', how='outer').fillna(0)\nlocation_freq['democrat'] = location_freq['democrat'].astype(int)\nlocation_freq['republican'] = location_freq['republican'].astype(int)\n\n# Filter for locations mentioned at least 5 times\nlocation_freq = location_freq[\n    (location_freq['democrat'] &gt;= 5) | (location_freq['republican'] &gt;= 5)\n].copy()\n\nprint(f\"\\nLocations mentioned frequently:\")\nprint(location_freq.head(15))\n\nDemocratic location mentions: 2133\nRepublican location mentions: 2007\n\nLocations mentioned frequently:\n       location  democrat  republican\n3   afghanistan        40          57\n5        alaska        10           2\n7       america       554         701\n19    australia         0           6\n21      baghdad         0          18\n28      belgium         6           2\n29       berlin        20           2\n38       brazil         4           6\n42        burma         6           4\n44   california        18          13\n45       canada         2          16\n52      chicago         4           6\n54        china        68          31\n57     colombia        14           0\n61        congo         6           2\n\n\n\n\n11.4 Statistical comparison of locations\n\n# Calculate G² and log odds for locations\nlocation_freq['g2'] = log_likelihood(\n    location_freq['democrat'].values,\n    location_freq['republican'].values\n)\n\nlocation_freq['log_odds'] = log_odds_ratio(\n    location_freq['democrat'].values,\n    location_freq['republican'].values\n)\n\n# Find significant differences\nsig_locations = location_freq[\n    (location_freq['g2'] &gt; 6.63) &\n    (location_freq['democrat'] &gt;= 3) &\n    (location_freq['republican'] &gt;= 3)\n].copy()\n\nprint(\"Locations with significant usage differences:\\n\")\nprint(\"Most Democratic:\")\nprint(sig_locations.nlargest(10, 'log_odds')[['location', 'democrat', 'republican', 'log_odds', 'g2']])\n\nprint(\"\\nMost Republican:\")\nprint(sig_locations.nsmallest(10, 'log_odds')[['location', 'democrat', 'republican', 'log_odds', 'g2']])\n\nLocations with significant usage differences:\n\nMost Democratic:\n                         location  democrat  republican  log_odds         g2\n96                        germany        24           4  2.539343  15.224309\n62                           cuba        16           3  2.369418   9.359102\n158                        mexico        16           3  2.369418   9.359102\n113                         india        16           4  1.954381   7.335339\n238                        russia        36          11  1.664874  13.230245\n290  the united states of america        48          20  1.217415  11.011168\n284              the soviet union        64          28  1.147026  13.355114\n54                          china        68          31  1.087647  13.024438\n314                       vietnam        48          22  1.079912   9.087790\n7                         america       554         701 -0.385148  22.219891\n\nMost Republican:\n                         location  democrat  republican  log_odds         g2\n119                          iraq        19         119 -2.692510  83.903263\n261                        states        59          88 -0.622408   6.712539\n288             the united states       116         163 -0.536366   9.511364\n7                         america       554         701 -0.385148  22.219891\n314                       vietnam        48          22  1.079912   9.087790\n54                          china        68          31  1.087647  13.024438\n284              the soviet union        64          28  1.147026  13.355114\n290  the united states of america        48          20  1.217415  11.011168\n238                        russia        36          11  1.664874  13.230245\n113                         india        16           4  1.954381   7.335339\n\n\n\n\n11.5 Visualizing location mentions\n\nif len(sig_locations) &gt; 0:\n    # Get top locations for each party\n    top_dem_loc = sig_locations.nlargest(10, 'log_odds')\n    top_rep_loc = sig_locations.nsmallest(10, 'log_odds')\n    top_loc = pd.concat([top_dem_loc, top_rep_loc]).drop_duplicates().sort_values('log_odds')\n    \n    # Plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    colors = ['#0015BC' if x &gt; 0 else '#E81B23' for x in top_loc['log_odds']]\n    \n    ax.barh(range(len(top_loc)), top_loc['log_odds'], color=colors, alpha=0.7)\n    ax.set_yticks(range(len(top_loc)))\n    ax.set_yticklabels(top_loc['location'])\n    ax.axvline(0, color='black', linewidth=0.8, linestyle='--')\n    ax.set_xlabel('Log Odds Ratio (negative = Republican, positive = Democrat)', fontsize=12)\n    ax.set_title('Geographic Focus: Location Mentions by Party', fontsize=14, fontweight='bold')\n    \n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Not enough significant location differences in our sample.\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteOther entity types\n\n\n\nYou can analyze other entity types the same way:\n\nPERSON: Which individuals are mentioned?\nORG: What organizations are discussed?\nDATE: How are temporal references used?\nMONEY: How are financial amounts discussed?\n\nTry exploring these in the exercises!"
  },
  {
    "objectID": "lab_02.html#summary-and-key-takeaways",
    "href": "lab_02.html#summary-and-key-takeaways",
    "title": "Lab 02: Words as data points II",
    "section": "12 Summary and key takeaways",
    "text": "12 Summary and key takeaways\n\n12.1 What we learned\nToday we covered methods for statistically comparing corpora:\n\nLemmatization → Reducing words to dictionary forms\nCorpus preparation → Creating contrasting text collections\nLog-likelihood (G²) → Testing statistical significance\nLog odds ratio → Measuring effect size\nNamed entity recognition → Extracting references to real-world entities\n\n\n\n12.2 Key concepts\n\nLemmatization\n\nReducing wordforms to their base dictionary form (lemma)\n\nContrasting corpora\n\nCollections of texts from different sources for comparison\n\nLog-likelihood (G²)\n\nStatistical test for significance of frequency differences\n\nLog odds ratio\n\nMeasure of effect size (how much more frequent)\n\nNamed entity\n\nReference to a real-world entity (person, place, organization)\n\nEffect size vs significance\n\nSignificance = confidence; effect size = magnitude\n\n\n\n\n12.3 Critical insights\n\n\n\n\n\n\nImportantDon’t trust p-values alone!\n\n\n\nA word can be:\n\nHighly significant but barely different (large sample)\nHighly different but not significant (rare word)\n\nAlways report both significance and effect size.\n\n\n\n\n\n\n\n\nImportantPreprocessing choices matter\n\n\n\n\nLemmatize or not?\nRemove stop words or not?\nFilter by part of speech or not?\n\nEach choice affects your results. Make them explicit and justified.\n\n\n\n\n12.4 Statistical comparison workflow\n\nPrepare corpora → Split data into contrasting groups\nLemmatize → Reduce morphological variation (if appropriate)\nCount frequencies → Create frequency table\nFilter → Remove very rare words, stop words (if appropriate)\nCalculate G² → Test significance\nCalculate log odds → Measure effect size\nFilter meaningful differences → Both significant AND large\nInterpret → What do the differences tell us?"
  },
  {
    "objectID": "lab_02.html#exercises",
    "href": "lab_02.html#exercises",
    "title": "Lab 02: Words as data points II",
    "section": "13 Exercises",
    "text": "13 Exercises\n\n13.1 Exercise 1: Full corpus analysis\nWe used samples for speed in this lab. Now process the full corpora:\n\nProcess all Democratic and all Republican speeches (not just samples)\nCalculate G² and log odds ratio for all lemmas\nIdentify the 20 most distinctive content words for each party\nCreate visualizations\n\nNote: This will take 10-15 minutes to process!\n\n\n13.2 Exercise 2: Stop word investigation\nInvestigate whether stop words show political patterns:\n\nFilter for only stop words in your frequency table\nCalculate G² and log odds ratio\nWhich stop words differ most between parties?\nCan you interpret why? (Think about formality, rhetorical style)\n\n\n\n13.3 Exercise 3: Temporal comparison\nInstead of comparing parties, compare time periods:\n\nSplit speeches into before/after 1970 (or another meaningful date)\nCalculate distinctive words for each period\nWhat changes in American political discourse can you observe?\n\n\n\n13.4 Exercise 4: Named entity deep dive\nChoose one entity type (PERSON, ORG, or DATE) and:\n\nExtract all entities of that type from both corpora\nCalculate frequency differences\nIdentify significant patterns\nInterpret: What do these patterns reveal about political priorities?\n\n\n\n13.5 Exercise 5: Part-of-speech patterns (Advanced)\nCompare parts of speech:\n\nCount how often each POS tag appears in each corpus\nDo Democrats use more adjectives? Republicans more verbs?\nCalculate significance and effect size\nWhat might linguistic differences reveal about rhetorical style?\n\n\n\n13.6 Exercise 6: Creating your own contrasting corpora\nThink of another comparison that interests you in the State of the Union data:\n\nWar vs peace time presidents\nFirst term vs second term speeches\n19th vs 20th vs 21st century\nHigh vs low approval ratings (you’d need to add this data)\n\nDesign and execute your own corpus comparison study."
  },
  {
    "objectID": "lab_02.html#references-and-further-reading",
    "href": "lab_02.html#references-and-further-reading",
    "title": "Lab 02: Words as data points II",
    "section": "14 References and further reading",
    "text": "14 References and further reading\n\n14.1 Academic papers\n\nDunning, T. (1993). Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1), 61-74. https://aclanthology.org/J93-1003.pdf\nRayson, P., & Garside, R. (2000). Comparing corpora using frequency profiling. Proceedings of the Workshop on Comparing Corpora, 1-6. https://doi.org/10.3115/1117729.1117730\nMonroe, B. L., Colaresi, M. P., & Quinn, K. M. (2008). Fightin’ words: Lexical feature selection and evaluation for identifying the content of political conflict. Political Analysis, 16(4), 372-403. https://doi.org/10.1093/pan/mpn018\n\n\n\n14.2 Textbooks\n\nJurafsky, D., & Martin, J. H. (2023). Speech and Language Processing (3rd ed., draft). Chapter 2 (Regular Expressions, Text Normalization, Edit Distance). https://web.stanford.edu/~jurafsky/slp3/\nSilge, J., & Robinson, D. (2017). Text Mining with R. Chapter 4 (Relationships between words). https://www.tidytextmining.com/ngrams.html\n\n\n\n14.3 Tutorials\n\nspaCy documentation on lemmatization: https://spacy.io/usage/linguistic-features#lemmatization\nspaCy documentation on NER: https://spacy.io/usage/linguistic-features#named-entities\nLog-likelihood calculator and explanation: http://ucrel.lancs.ac.uk/llwizard.html\n\n\n\n14.4 Tools\n\nspaCy: Industrial-strength NLP library - https://spacy.io\nNLTK: Classic Python NLP toolkit - https://www.nltk.org\nLancaster Stats Tools: Log-likelihood calculator - http://ucrel.lancs.ac.uk/llwizard.html\n\n\nEnd of Lab 02"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Code and other materials for 1MEWI3647V Computational text analysis: An Introduction to text as data (WiSe25-26)\nQuestions, comments, suggestions? -&gt; Open an issue https://github.com/paskn/cta-with-python/issues\nOr email: sergei.pashakhin@uni-siegen.de"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Text Analysis with Python",
    "section": "",
    "text": "Start here: Python Basics\nLab 01: Words as data points I\nLab 02: Words as data points II"
  },
  {
    "objectID": "basic-python.html",
    "href": "basic-python.html",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "",
    "text": "After completing this tutorial, you will be able to:\n\n✓ Run Python code in Google Colab\n✓ Read and understand Python code\n✓ Work with strings for text analysis\n✓ Use basic data structures (lists, dictionaries)\n✓ Write simple functions and control flow\n✓ Find help and documentation online\n✓ Understand cloud vs. local Python environments\n✓ (Optional) Debug common errors"
  },
  {
    "objectID": "basic-python.html#learning-outcomes",
    "href": "basic-python.html#learning-outcomes",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "",
    "text": "After completing this tutorial, you will be able to:\n\n✓ Run Python code in Google Colab\n✓ Read and understand Python code\n✓ Work with strings for text analysis\n✓ Use basic data structures (lists, dictionaries)\n✓ Write simple functions and control flow\n✓ Find help and documentation online\n✓ Understand cloud vs. local Python environments\n✓ (Optional) Debug common errors"
  },
  {
    "objectID": "basic-python.html#getting-started-with-google-colab",
    "href": "basic-python.html#getting-started-with-google-colab",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "1. Getting Started with Google Colab",
    "text": "1. Getting Started with Google Colab\n\nWhat is Google Colab?\nGoogle Colab (short for Colaboratory) is a free cloud service that lets you write and run Python code in your web browser. You don’t need to install anything on your computer - everything runs in the cloud. This makes it perfect for getting started with Python.\n\n\nAccessing Google Colab\n\nGo to https://colab.research.google.com\nSign in with your Google account\nClick on “New Notebook” to create a new Python notebook\n\n\n\n\nCreating a new notebook\n\n\n\n\nUnderstanding the Colab Interface\nA Colab notebook consists of cells. There are two main types:\n\nCode cells: Where you write Python code\nText cells: Where you write notes and explanations (using Markdown)\n\n\n\n\nColab interface\n\n\n\n\nRunning Code\nTo run code in a cell:\n\nClick the Play button (▶) on the left side of the cell, OR\nPress Shift + Enter on your keyboard\n\nThe output will appear below the cell.\n\n\n\nRunning a code cell\n\n\n\n\nSaving Your Work\nYour notebooks are automatically saved to your Google Drive in a folder called “Colab Notebooks”. You can also:\n\nRename your notebook by clicking on the title at the top\nDownload your notebook (File → Download → Download .ipynb)\nShare it with others (Share button in top right)\n\n\n\n\n\n\n\nNote📌 Key Point\n\n\n\nYour Colab notebooks are saved to Google Drive, so they count toward your Drive storage quota. We’ll discuss local alternatives later in this tutorial."
  },
  {
    "objectID": "basic-python.html#your-first-python-code",
    "href": "basic-python.html#your-first-python-code",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "2. Your First Python Code",
    "text": "2. Your First Python Code\nLet’s start by looking at some Python code. Don’t worry if you don’t understand it yet - that’s what we’re here to learn!\n\nmessage = \"Hello, World!\"\nprint(message)\n\nHello, World!\n\n\n\nWhat Just Happened?\nLet’s break down this code:\n\nmessage = \"Hello, World!\" - This creates a variable named message and stores the text “Hello, World!” in it\nprint(message) - This tells Python to display the contents of the message variable\n\nThink of a variable like a labeled box where you can store information. The = sign means “store the value on the right into the variable on the left”.\n\n\nExercise 2.1\nCopy the following code into Google Colab and run it:\n\nmessage = \"Hello, World!\"\nprint(message)\n\nHello, World!\n\n\nNow modify it:\n\nChange \"Hello, World!\" to \"Python is fun!\" and run the code again\nChange the variable name from message to greeting (remember to change it in both places!)\nAdd another line: print(\"My first Python program\") and run the code\n\n\n\n\n\n\n\nTip💡 Tip\n\n\n\nVariable names can contain letters, numbers, and underscores, but they must start with a letter or underscore. Use descriptive names that help you remember what the variable contains."
  },
  {
    "objectID": "basic-python.html#getting-help-and-finding-information",
    "href": "basic-python.html#getting-help-and-finding-information",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "3. Getting Help and Finding Information",
    "text": "3. Getting Help and Finding Information\nBefore we dive deeper into Python, let’s learn how to find help when you’re stuck. This is one of the most important skills for working with Python!\n\nUsing Python’s Built-in Help\nPython has a built-in help() function that shows you information about functions and objects.\n\nhelp(print)\n\nThis will display documentation about the print function, including how to use it.\n\n\n\nUsing the help() function\n\n\n\n\nUsing dir() to Explore\nThe dir() function shows you what methods and attributes are available for an object:\n\ntext = \"hello\"\ndir(text)\n\n['__add__',\n '__class__',\n '__contains__',\n '__delattr__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getnewargs__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__rmod__',\n '__rmul__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'capitalize',\n 'casefold',\n 'center',\n 'count',\n 'encode',\n 'endswith',\n 'expandtabs',\n 'find',\n 'format',\n 'format_map',\n 'index',\n 'isalnum',\n 'isalpha',\n 'isascii',\n 'isdecimal',\n 'isdigit',\n 'isidentifier',\n 'islower',\n 'isnumeric',\n 'isprintable',\n 'isspace',\n 'istitle',\n 'isupper',\n 'join',\n 'ljust',\n 'lower',\n 'lstrip',\n 'maketrans',\n 'partition',\n 'removeprefix',\n 'removesuffix',\n 'replace',\n 'rfind',\n 'rindex',\n 'rjust',\n 'rpartition',\n 'rsplit',\n 'rstrip',\n 'split',\n 'splitlines',\n 'startswith',\n 'strip',\n 'swapcase',\n 'title',\n 'translate',\n 'upper',\n 'zfill']\n\n\nThis shows all the things you can do with a string. Methods that start with _ are internal - focus on the others like upper, lower, split, etc.\n\n\nReading Official Python Documentation\nThe official Python documentation is at https://docs.python.org. It’s comprehensive and well-organized.\nFor beginners, the Python Tutorial section is especially helpful: https://docs.python.org/3/tutorial/\n\n\n\nPython documentation\n\n\n\n\nSearching Online Effectively\nWhen you have a question or encounter an error:\n\nGoogle your question - Include “python” in your search\n\nGood: “python how to convert string to lowercase”\nBad: “make text smaller”\n\nStack Overflow - A Q&A site where programmers help each other\n\nLook for questions with many upvotes and accepted answers (green checkmark)\nRead the question to make sure it matches your problem\n\nRead the error message - Python error messages often tell you exactly what’s wrong\n\nWe’ll cover this in detail in the optional debugging section\n\n\n\n\n\n\n\n\nTip💡 Tip\n\n\n\nWhen searching for help, include the Python version you’re using. Google Colab typically uses Python 3, so add “python 3” to your searches.\n\n\n\n\nExercise 3.1\nIn Google Colab, try the following:\n\nRun help(len) - what does the len() function do?\nCreate a variable: word = \"Python\", then run dir(word)\nFind a method in the output that sounds interesting (like upper or lower)\nTry using it: word.upper() or word.lower()\nUse help(word.upper) to learn more about that method"
  },
  {
    "objectID": "basic-python.html#basic-data-types",
    "href": "basic-python.html#basic-data-types",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "4. Basic Data Types",
    "text": "4. Basic Data Types\nPython works with different types of data. Let’s explore the most important ones for text and data analysis.\n\nStrings\nStrings are text data - anything you can type. They’re enclosed in quotes (either \" or ').\n\nauthor = \"Virginia Woolf\"\ntitle = 'Mrs Dalloway'\nsentence = \"She said, 'I love Python!'\"\n\n\nString Operations\nConcatenation (joining strings):\n\nfirst_name = \"Ada\"\nlast_name = \"Lovelace\"\nfull_name = first_name + \" \" + last_name\nprint(full_name)\n\nAda Lovelace\n\n\nGetting string length:\n\ntext = \"Hello\"\nlength = len(text)\nprint(length)\n\n5\n\n\n\ntext = \"Python\"\nprint(text[0])      # First character\nprint(text[1])      # Second character\nprint(text[-1])     # Last character\nprint(text[0:3])    # Characters 0, 1, 2 (not 3)\nprint(text[2:])     # From character 2 to the end\n\nP\ny\nn\nPyt\nthon\n\n\n\n\nString Methods\nStrings have many built-in methods (functions that belong to strings):\nChanging case:\n\ntext = \"Hello World\"\nprint(text.lower())\nprint(text.upper())\n\nhello world\nHELLO WORLD\n\n\n\nsentence = \"Python is great for text analysis\"\nwords = sentence.split()\nprint(words)\n\n['Python', 'is', 'great', 'for', 'text', 'analysis']\n\n\n\ntext = \"I like cats\"\nnew_text = text.replace(\"cats\", \"dogs\")\nprint(new_text)\n\nI like dogs\n\n\n\ntext = \"   hello   \"\nprint(text.strip())\n\nhello\n\n\n\nsentence = \"Python is amazing\"\nposition = sentence.find(\"is\")\nprint(position)\n\n7\n\n\n\ntext = \"how much wood would a woodchuck chuck\"\ncount = text.count(\"wood\")\nprint(count)\n\n2\n\n\n\n\nString Formatting with f-strings\nF-strings let you insert variable values into strings easily:\n\nname = \"Alice\"\nage = 25\nmessage = f\"My name is {name} and I am {age} years old\"\nprint(message)\n\nMy name is Alice and I am 25 years old\n\n\n\n\n\nExercise 4.1\nCopy this code into Colab:\n\nbook_title = \"pride and prejudice\"\nauthor = \"Jane Austen\"\n\nModify the code to:\n\nConvert book_title to title case using .title() and print it\nMake author all uppercase and print it\nCreate a sentence using an f-string: \"The book {book_title} was written by {author}\"\nUse .split() on book_title to separate it into words and print the result\nCount how many times the letter “e” appears in book_title\n\n\n\nNumbers\nPython works with two main types of numbers:\nIntegers (whole numbers):\n\npages = 324\nchapters = 12\n\nFloats (decimal numbers):\n\nprice = 19.99\nrating = 4.5\n\n\nArithmetic Operations\n\n# Basic arithmetic\nprint(10 + 5)      # Addition: 15\nprint(10 - 5)      # Subtraction: 5\nprint(10 * 5)      # Multiplication: 50\nprint(10 / 5)      # Division: 2.0 (always returns float)\nprint(10 // 3)     # Integer division: 3 (rounds down)\nprint(10 % 3)      # Modulo (remainder): 1\nprint(10 ** 2)     # Exponentiation: 100\n\n15\n5\n50\n2.0\n3\n1\n100\n\n\n\n\nType Conversion\nSometimes you need to convert between strings and numbers:\n\n# String to number\ntext_number = \"42\"\nnumber = int(text_number)\nprint(number + 8)  # Output: 50\n\n# Number to string\nage = 25\nmessage = \"I am \" + str(age) + \" years old\"\nprint(message)\n\n50\nI am 25 years old\n\n\n\n\n\n\n\n\nWarning⚠️ Warning\n\n\n\nYou cannot directly concatenate strings and numbers. You’ll get an error if you try \"Age: \" + 25. Convert the number to a string first: \"Age: \" + str(25), or use an f-string: f\"Age: {25}\".\n\n\n\n\n\nExercise 4.2\nCopy this code into Colab:\n\ntotal_words = 1000\npages = 5\n\nModify the code to:\n\nCalculate words per page by dividing total_words by pages and print it\nCreate a variable additional_pages = 3 and calculate the new total pages\nConvert the result to a string and create a message: \"The document has X pages\" (use f-string)\nCalculate how many pages you’d have if you doubled the current number\n\n\n\nBooleans\nBooleans represent True or False values. They’re essential for making decisions in code.\n\nis_published = True\nis_draft = False\n\n\nComparison Operators\nThese operators compare values and return True or False:\n\nx = 10\ny = 5\n\nprint(x &gt; y)       # Greater than: True\nprint(x &lt; y)       # Less than: False\nprint(x == y)      # Equal to: False\nprint(x != y)      # Not equal to: True\nprint(x &gt;= 10)     # Greater than or equal: True\nprint(x &lt;= 5)      # Less than or equal: False\n\nTrue\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n\n\n\n\n\n\nNote📌 Key Point\n\n\n\nUse == to compare values (equality test) and = to assign values to variables. This is a common source of confusion!\n\n\nYou can also compare strings:\n\nword1 = \"apple\"\nword2 = \"banana\"\nprint(word1 == word2)    # False\nprint(word1 &lt; word2)     # True (alphabetical order)\n\nFalse\nTrue\n\n\n\n\n\nExercise 4.3\nCopy this code into Colab:\n\nword_count = 150\nminimum_required = 100\n\nModify the code to:\n\nCheck if word_count is greater than minimum_required and print the result\nCheck if word_count equals 150 and print the result\nCheck if word_count is not equal to 200 and print the result\nChange word_count to 75 and run the comparisons again"
  },
  {
    "objectID": "basic-python.html#data-structures",
    "href": "basic-python.html#data-structures",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "5. Data Structures",
    "text": "5. Data Structures\nData structures let you organize and store multiple pieces of information together.\n\nLists\nLists are ordered collections of items. They’re perfect for storing sequences of data.\n\nauthors = [\"Virginia Woolf\", \"James Joyce\", \"Marcel Proust\"]\nword_counts = [150, 200, 175, 300]\nmixed_data = [\"Python\", 3, True, 19.99]\n\n\nAccessing List Items\nLists use zero-based indexing, just like strings:\n\nfruits = [\"apple\", \"banana\", \"cherry\", \"date\"]\nprint(fruits[0])      # First item: apple\nprint(fruits[1])      # Second item: banana\nprint(fruits[-1])     # Last item: date\nprint(fruits[-2])     # Second to last: cherry\n\napple\nbanana\ndate\ncherry\n\n\n\n\nSlicing Lists\n\nnumbers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nprint(numbers[2:5])    # Items 2, 3, 4: [2, 3, 4]\nprint(numbers[:3])     # First 3 items: [0, 1, 2]\nprint(numbers[7:])     # From index 7 to end: [7, 8, 9]\nprint(numbers[::2])    # Every other item: [0, 2, 4, 6, 8]\n\n[2, 3, 4]\n[0, 1, 2]\n[7, 8, 9]\n[0, 2, 4, 6, 8]\n\n\n\n\nModifying Lists\nAdding items:\n\nbooks = [\"1984\", \"Brave New World\"]\nbooks.append(\"Fahrenheit 451\")\nprint(books)\n\n['1984', 'Brave New World', 'Fahrenheit 451']\n\n\n\nbooks = [\"1984\", \"Brave New World\", \"Fahrenheit 451\"]\nbooks.remove(\"Brave New World\")\nprint(books)\n\n['1984', 'Fahrenheit 451']\n\n\n\nbooks = [\"1984\", \"Brave New World\", \"Fahrenheit 451\"]\nlast_book = books.pop()\nprint(last_book)\nprint(books)\n\nFahrenheit 451\n['1984', 'Brave New World']\n\n\n\n\nList Methods\nGetting list length:\n\nwords = [\"the\", \"quick\", \"brown\", \"fox\"]\nprint(len(words))\n\n4\n\n\n\nnumbers = [3, 1, 4, 1, 5, 9, 2, 6]\nnumbers.sort()\nprint(numbers)\n\n[1, 1, 2, 3, 4, 5, 6, 9]\n\n\n\nletters = [\"a\", \"b\", \"c\", \"d\"]\nletters.reverse()\nprint(letters)\n\n['d', 'c', 'b', 'a']\n\n\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\nprint(\"banana\" in fruits)\nprint(\"grape\" in fruits)\n\nTrue\nFalse\n\n\n\n\nLists of Strings (Text Analysis)\nLists are especially useful for working with text data:\n\nsentence = \"Python is great for text analysis\"\nwords = sentence.split()\nprint(words)\nprint(f\"Number of words: {len(words)}\")\nprint(f\"First word: {words[0]}\")\nprint(f\"Last word: {words[-1]}\")\n\n['Python', 'is', 'great', 'for', 'text', 'analysis']\nNumber of words: 6\nFirst word: Python\nLast word: analysis\n\n\n\n\n\nExercise 5.1\nCopy this code into Colab:\n\ntext = \"to be or not to be that is the question\"\nwords = text.split()\n\nModify the code to:\n\nPrint the length of the words list\nPrint the first word and the last word\nUse .append() to add the word “indeed” to the end of the list\nUse .remove() to remove the first occurrence of “to”\nSort the words alphabetically and print the result\n\n\n\nDictionaries\nDictionaries store data as key-value pairs. They’re like a real dictionary where you look up a word (key) to find its definition (value).\n\nbook = {\n    \"title\": \"1984\",\n    \"author\": \"George Orwell\",\n    \"year\": 1949,\n    \"pages\": 328\n}\n\n\nAccessing Dictionary Values\nUse keys to access values:\n\nbook = {\n    \"title\": \"1984\",\n    \"author\": \"George Orwell\",\n    \"year\": 1949\n}\n\nprint(book[\"title\"])\nprint(book[\"author\"])\n\n1984\nGeorge Orwell\n\n\n\n\nAdding or Modifying Entries\n\nbook = {\"title\": \"1984\", \"author\": \"George Orwell\"}\n\n# Add a new entry\nbook[\"year\"] = 1949\nprint(book)\n\n# Modify an existing entry\nbook[\"year\"] = 1950\nprint(book)\n\n{'title': '1984', 'author': 'George Orwell', 'year': 1949}\n{'title': '1984', 'author': 'George Orwell', 'year': 1950}\n\n\n\n\nDictionary Methods\nGetting all keys:\n\nbook = {\"title\": \"1984\", \"author\": \"George Orwell\", \"year\": 1949}\nprint(book.keys())\n\ndict_keys(['title', 'author', 'year'])\n\n\n\nprint(book.values())\n\ndict_values(['1984', 'George Orwell', 1949])\n\n\n\nprint(book.items())\n\ndict_items([('title', '1984'), ('author', 'George Orwell'), ('year', 1949)])\n\n\n\nprint(\"author\" in book)\nprint(\"publisher\" in book)\n\nTrue\nFalse\n\n\n\n\nDictionaries for Text Analysis\nDictionaries are useful for counting and organizing text data:\n\nword_frequencies = {\n    \"the\": 150,\n    \"and\": 89,\n    \"to\": 76,\n    \"of\": 72\n}\n\nprint(f\"The word 'the' appears {word_frequencies['the']} times\")\n\nThe word 'the' appears 150 times\n\n\n\n\n\nExercise 5.2\nCopy this code into Colab:\n\npoem = {\n    \"title\": \"The Road Not Taken\",\n    \"author\": \"Robert Frost\",\n    \"year\": 1916\n}\n\nModify the code to:\n\nPrint the poem’s title\nAdd a new key “lines” with the value 20\nChange the year to 1915\nPrint all the keys in the dictionary\nPrint all the values in the dictionary\nCheck if “publisher” is a key in the dictionary and print the result"
  },
  {
    "objectID": "basic-python.html#control-flow",
    "href": "basic-python.html#control-flow",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "6. Control Flow",
    "text": "6. Control Flow\nControl flow lets you make decisions and repeat actions in your code.\n\nIf Statements\nIf statements let your code make decisions based on conditions.\n\nword_count = 150\nminimum = 100\n\nif word_count &gt;= minimum:\n    print(\"You have enough words!\")\n    print(\"Good job!\")\n\nYou have enough words!\nGood job!\n\n\nOutput (if word_count is 150):\nYou have enough words!\nGood job!\n\n\n\n\n\n\nNote📌 Key Point\n\n\n\nNotice the indentation (spaces at the start of lines). Python uses indentation to group code together. Everything indented under the if statement runs only if the condition is True.\n\n\n\nIf-Else\n\nword_count = 75\nminimum = 100\n\nif word_count &gt;= minimum:\n    print(\"You have enough words!\")\nelse:\n    print(\"You need more words\")\n    words_needed = minimum - word_count\n    print(f\"You need {words_needed} more words\")\n\nYou need more words\nYou need 25 more words\n\n\n\n\nIf-Elif-Else\nUse elif (else-if) for multiple conditions:\n\nscore = 85\n\nif score &gt;= 90:\n    grade = \"A\"\nelif score &gt;= 80:\n    grade = \"B\"\nelif score &gt;= 70:\n    grade = \"C\"\nelse:\n    grade = \"F\"\n\nprint(f\"Your grade is: {grade}\")\n\nYour grade is: B\n\n\n\n\n\nExercise 6.1\nCopy this code into Colab:\n\ntext = \"Python\"\n\nModify the code to:\n\nCheck if the length of text is greater than 5. If it is, print “Long word”, otherwise print “Short word”\nChange text to different words and test your code\nModify your code to handle three cases: length &gt; 8 (print “Very long”), length &gt; 5 (print “Medium”), otherwise (print “Short”)\n\n\n\nLoops\nLoops let you repeat actions multiple times.\n\nFor Loops\nFor loops iterate over sequences (lists, strings, etc.):\nLooping over a list:\n\nwords = [\"Python\", \"is\", \"great\"]\nfor word in words:\n    print(word)\n\nPython\nis\ngreat\n\n\n\ntext = \"Python\"\nfor letter in text:\n    print(letter)\n\nP\ny\nt\nh\no\nn\n\n\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\n\n# From 1 to 5\nfor i in range(1, 6):\n    print(i)\n\n# From 0 to 10, counting by 2\nfor i in range(0, 11, 2):\n    print(i)\n\n1\n2\n3\n4\n5\n0\n2\n4\n6\n8\n10\n\n\nCombining loops and if statements:\n\nwords = [\"apple\", \"banana\", \"cherry\", \"date\"]\nfor word in words:\n    if len(word) &gt; 5:\n        print(f\"{word} is a long word\")\n\nbanana is a long word\ncherry is a long word\n\n\n\nword_counts = {\"the\": 150, \"and\": 89, \"to\": 76}\n\n# Loop over keys\nfor word in word_counts:\n    print(word)\n\n# Loop over keys and values\nfor word, count in word_counts.items():\n    print(f\"{word}: {count}\")\n\nthe\nand\nto\nthe: 150\nand: 89\nto: 76\n\n\n\n\nWhile Loops\nWhile loops repeat as long as a condition is True:\n\ncount = 0\nwhile count &lt; 5:\n    print(count)\n    count = count + 1\n\n0\n1\n2\n3\n4\n\n\n\n\n\nExercise 6.2\nCopy this code into Colab:\n\nsentence = \"the quick brown fox jumps over the lazy dog\"\nwords = sentence.split()\n\nModify the code to:\n\nUse a for loop to print each word in words\nUse a for loop with an if statement to print only words with more than 3 letters\nUse a for loop to count how many words start with the letter “t” (use .startswith(\"t\"))\nUse range() to print the first 5 numbers (0-4)\nChange the range to print numbers from 1 to 10"
  },
  {
    "objectID": "basic-python.html#functions",
    "href": "basic-python.html#functions",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "7. Functions",
    "text": "7. Functions\nFunctions are reusable blocks of code that perform specific tasks.\n\nLooking at Function Code First\n\ndef greet(name):\n    message = f\"Hello, {name}!\"\n    return message\n\nresult = greet(\"Alice\")\nprint(result)\n\nHello, Alice!\n\n\n\n\nWhat Are Functions?\nLet’s break down that code:\n\ndef greet(name): - This defines a function named greet that takes one parameter called name\nThe indented code is the function body - what the function does\nreturn message - This sends the result back to whoever called the function\ngreet(\"Alice\") - This calls (runs) the function with the argument \"Alice\"\n\nThink of functions like recipes: you define the recipe once, then you can follow it many times with different ingredients.\n\n\nBuilt-in Functions Review\nWe’ve already been using Python’s built-in functions:\n\n# len() - get length\ntext = \"Python\"\nprint(len(text))  # 6\n\n# type() - check data type\nprint(type(42))        # &lt;class 'int'&gt;\nprint(type(\"hello\"))   # &lt;class 'str'&gt;\nprint(type([1, 2, 3])) # &lt;class 'list'&gt;\n\n# print() - display output\nprint(\"Hello, World!\")\n\n# input() - get user input (works in Colab!)\nname = input(\"What is your name? \")\nprint(f\"Hello, {name}\")\n\n\n\nCreating Custom Functions\nFunction without parameters:\n\ndef say_hello():\n    print(\"Hello, World!\")\n    \nsay_hello()\n\nHello, World!\n\n\n\ndef create_greeting(first_name, last_name):\n    full_name = f\"{first_name} {last_name}\"\n    greeting = f\"Welcome, {full_name}!\"\n    return greeting\n\nmessage = create_greeting(\"Ada\", \"Lovelace\")\nprint(message)\n\nWelcome, Ada Lovelace!\n\n\n\ndef greet(name, greeting=\"Hello\"):\n    return f\"{greeting}, {name}!\"\n\nprint(greet(\"Alice\"))\nprint(greet(\"Bob\", \"Hi\"))\n\nHello, Alice!\nHi, Bob!\n\n\n\ndef count_words(text):\n    words = text.split()\n    return len(words)\n\nsentence = \"Python is great for text analysis\"\nnum_words = count_words(sentence)\nprint(f\"The sentence has {num_words} words\")\n\nThe sentence has 6 words\n\n\n\ndef analyze_text(text):\n    num_chars = len(text)\n    num_words = len(text.split())\n    return num_chars, num_words\n\ntext = \"Hello, World!\"\nchars, words = analyze_text(text)\nprint(f\"Characters: {chars}, Words: {words}\")\n\nCharacters: 13, Words: 2\n\n\n\n\nExercise 7.1\nCopy this code into Colab:\n\ndef process_word(word):\n    return word.upper()\n\nresult = process_word(\"python\")\nprint(result)\n\nPYTHON\n\n\nModify the code to:\n\nChange the function to return word.lower() instead\nAdd a second parameter case_type and use an if statement to return either uppercase or lowercase based on the parameter\nCreate a new function count_letter(text, letter) that counts how many times a specific letter appears in text\nTest your function with different texts and letters"
  },
  {
    "objectID": "basic-python.html#working-with-files",
    "href": "basic-python.html#working-with-files",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "8. Working with Files",
    "text": "8. Working with Files\nWorking with files lets you read and write data stored on your computer (or in Colab’s temporary storage).\n\nReading Text Files\n\n# Open and read a file\nfile = open(\"sample.txt\", \"r\")\ncontent = file.read()\nprint(content)\nfile.close()\n\nThis is line 1\nThis is line 2\nThis is line 3\n\n\n\n\n\n\n\n\n\nNote📌 Key Point\n\n\n\nThe \"r\" means “read mode”. Always close files when you’re done with file.close() to free up resources.\n\n\nBetter way: Using with statement (automatically closes the file):\n\nwith open(\"sample.txt\", \"r\") as file:\n    content = file.read()\n    print(content)\n# File is automatically closed here\n\nThis is line 1\nThis is line 2\nThis is line 3\n\n\n\nReading line by line:\n\nwith open(\"sample.txt\", \"r\") as file:\n    for line in file:\n        print(line.strip())  # strip() removes extra whitespace/newlines\n\nThis is line 1\nThis is line 2\nThis is line 3\n\n\nReading all lines into a list:\n\nwith open(\"sample.txt\", \"r\") as file:\n    lines = file.readlines()\n    print(f\"Number of lines: {len(lines)}\")\n    print(f\"First line: {lines[0].strip()}\")\n\nNumber of lines: 3\nFirst line: This is line 1\n\n\n\n\nWriting to Text Files\n\n# Writing to a file (creates new file or overwrites existing)\nwith open(\"output.txt\", \"w\") as file:\n    file.write(\"Hello, World!\\n\")\n    file.write(\"This is a new line.\\n\")\n\n\n\n\n\n\n\nWarning⚠️ Warning\n\n\n\nUsing \"w\" mode will overwrite the entire file if it exists. Use \"a\" (append mode) to add to the end of an existing file instead.\n\n\nAppending to a file:\n\nwith open(\"output.txt\", \"a\") as file:\n    file.write(\"This line is added to the end.\\n\")\n\nWriting multiple lines:\n\nlines = [\"First line\\n\", \"Second line\\n\", \"Third line\\n\"]\nwith open(\"output.txt\", \"w\") as file:\n    file.writelines(lines)\n\n\n\nFile Paths in Google Colab\nIn Google Colab, you can:\n\nUpload files using the file browser on the left (folder icon)\nCreate files in code cells\nMount Google Drive to access your Drive files\n\n\n\n\nUploading files in Colab. (1) Press “folder” icon. (2) The upload button.\n\n\nCreating a sample file in Colab:\n\n# Create a sample file to practice with\nwith open(\"sample.txt\", \"w\") as file:\n    file.write(\"This is line 1\\n\")\n    file.write(\"This is line 2\\n\")\n    file.write(\"This is line 3\\n\")\n\n# Now read it back\nwith open(\"sample.txt\", \"r\") as file:\n    content = file.read()\n    print(content)\n\nThis is line 1\nThis is line 2\nThis is line 3\n\n\n\n\n\nText Analysis Example\n\n# Count word frequencies in a file\nword_counts = {}\n\nwith open(\"sample.txt\", \"r\") as file:\n    for line in file:\n        words = line.lower().split()\n        for word in words:\n            if word in word_counts:\n                word_counts[word] += 1\n            else:\n                word_counts[word] = 1\n\nprint(word_counts)\n\n{'this': 3, 'is': 3, 'line': 3, '1': 1, '2': 1, '3': 1}\n\n\n\n\nExercise 8.1\nCopy this code into Colab to create a sample file:\n\nwith open(\"poem.txt\", \"w\") as file:\n    file.write(\"Roses are red\\n\")\n    file.write(\"Violets are blue\\n\")\n    file.write(\"Python is fun\\n\")\n    file.write(\"And so are you\\n\")\n\nNow modify the code to:\n\nRead the file and print its contents\nRead the file and print only the first two lines\nRead the file and count the total number of words across all lines\nRead the file and create a list of all words (split each line and combine)\nWrite a new file called “output.txt” with all the words in uppercase"
  },
  {
    "objectID": "basic-python.html#introduction-to-packages",
    "href": "basic-python.html#introduction-to-packages",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "9. Introduction to Packages",
    "text": "9. Introduction to Packages\nSo far, we’ve used Python’s built-in features. But Python’s real power comes from packages (also called libraries) - collections of pre-written code that add new capabilities.\n\nWhat Are Packages?\nPackages are like toolboxes. Each package contains functions and tools for specific tasks:\n\npandas - working with tabular data (like spreadsheets)\nnumpy - numerical computing and arrays\nmatplotlib - creating visualizations and charts\nnltk - natural language processing and text analysis\nscikit-learn - machine learning\n\n\n\nImporting Packages\nTo use a package, you import it:\n\nimport math\n\n# Now you can use functions from the math package\nresult = math.sqrt(16)\nprint(result)\n\n4.0\n\n\n\nimport math as m\n\nresult = m.sqrt(25)\nprint(result)\n\n5.0\n\n\n\nfrom math import sqrt, pi\n\nprint(sqrt(9))\nprint(pi)\n\n3.0\n3.141592653589793\n\n\n\n\nInstalling Packages in Colab\nMost common packages are already installed in Google Colab. If you need to install a package, use:\n!pip install package-name\nThe ! tells Colab to run this as a shell command, not Python code.\n\n\n\n\n\n\nNote📌 Key Point\n\n\n\nIn Colab, you usually don’t need to install packages. Just import them! We’ll discuss installing packages locally in the next section.\n\n\n\n\nExample: Reading CSV with Pandas\nLet’s see a quick example using pandas to work with tabular data:\n\nimport pandas as pd\n\n# Create a sample dataset\ndata = {\n    \"book\": [\"1984\", \"Brave New World\", \"Fahrenheit 451\"],\n    \"author\": [\"George Orwell\", \"Aldous Huxley\", \"Ray Bradbury\"],\n    \"year\": [1949, 1932, 1953],\n    \"pages\": [328, 311, 249]\n}\n\n# Create a DataFrame (pandas' table structure)\ndf = pd.DataFrame(data)\n\n# Display the data\nprint(df)\n\n              book         author  year  pages\n0             1984  George Orwell  1949    328\n1  Brave New World  Aldous Huxley  1932    311\n2   Fahrenheit 451   Ray Bradbury  1953    249\n\n\n\nprint(df[\"book\"])\nprint(df[\"year\"])\n\n0               1984\n1    Brave New World\n2     Fahrenheit 451\nName: book, dtype: object\n0    1949\n1    1932\n2    1953\nName: year, dtype: int64\n\n\nFiltering data:\n\n# Books published after 1940\nrecent_books = df[df[\"year\"] &gt; 1940]\nprint(recent_books)\n\n             book         author  year  pages\n0            1984  George Orwell  1949    328\n2  Fahrenheit 451   Ray Bradbury  1953    249\n\n\nBasic statistics:\n\nprint(f\"Average pages: {df['pages'].mean()}\")\nprint(f\"Earliest year: {df['year'].min()}\")\n\nAverage pages: 296.0\nEarliest year: 1932\n\n\nReading from a CSV file:\n\n# Create a sample CSV file first\nwith open(\"books.csv\", \"w\") as file:\n    file.write(\"book,author,year,pages\\n\")\n    file.write(\"1984,George Orwell,1949,328\\n\")\n    file.write(\"Brave New World,Aldous Huxley,1932,311\\n\")\n\n# Read it with pandas\ndf = pd.read_csv(\"books.csv\")\nprint(df)\n\n              book         author  year  pages\n0             1984  George Orwell  1949    328\n1  Brave New World  Aldous Huxley  1932    311\n\n\n\n\n\n\n\n\nTip💡 Tip\n\n\n\nPandas is incredibly powerful for data analysis. We’ll explore it more deeply when working with real datasets. For now, just know that it exists and can read CSV files easily!\n\n\n\n\nExercise 9.1\nCopy this code into Colab:\n\nimport pandas as pd\n\ndata = {\n    \"word\": [\"the\", \"and\", \"to\", \"of\"],\n    \"count\": [150, 89, 76, 72]\n}\n\ndf = pd.DataFrame(data)\n\nModify the code to:\n\nPrint the entire DataFrame\nPrint only the “word” column\nPrint only the “count” column\nFind the maximum count using df[\"count\"].max()\nFind the minimum count using df[\"count\"].min()"
  },
  {
    "objectID": "basic-python.html#local-python-environments",
    "href": "basic-python.html#local-python-environments",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "10. Local Python Environments",
    "text": "10. Local Python Environments\nSo far, we’ve been using Google Colab, which runs Python in the cloud. But you might want to run Python on your own computer. Let’s explore why and how.\n\nCloud vs. Local Development\nGoogle Colab (Cloud):\n\n✓ No installation needed\n✓ Works on any computer with a browser\n✓ Free access to computing resources\n✗ Requires internet connection\n✗ Sessions timeout after inactivity\n✗ Uses your Google Drive storage\n✗ Some packages may not work\n\nLocal Python (Your Computer):\n\n✓ Works offline\n✓ Full control over environment\n✓ No session timeouts\n✓ All packages available\n✗ Requires installation and setup\n✗ Uses your computer’s resources\n\n\n\n\n\n\n\nNote📌 Key Point\n\n\n\nFor learning and quick experiments, Colab is perfect. For larger projects or when you need specific packages, local Python is better.\n\n\n\n\nIntroduction to uv (Recommended)\nuv is a modern, fast Python package and project manager. We recommend it because it’s:\n\nVery fast (much faster than traditional tools)\nEasy to use\nHandles both Python installation and package management\nBecoming the industry standard\n\nLear more about uv here: https://docs.astral.sh/uv/\n\nInstalling uv\nOn macOS and Linux:\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nOn Windows:\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\nAfter installation, close and reopen your terminal.\n\n\nUsing uv\nCreate a new Python project:\nuv init my-project\ncd my-project\nThis creates a new directory with a basic Python project structure.\nInstall a package:\nuv add pandas\nThis installs pandas and adds it to your project’s dependencies.\nRun Python:\nuv run python\nThis starts a Python interpreter with your project’s packages available.\nRun a Python script:\nuv run python my_script.py\nInstall a specific Python version:\nuv python install 3.11\n\n\n\n\n\n\nTip💡 Tip\n\n\n\nuv automatically creates isolated environments for each project, so different projects can use different package versions without conflicts.\n\n\n\n\n\nIntroduction to Conda (Fallback Option)\nConda is a more established tool that also manages Python environments and packages. It’s widely used in data science.\n\nInstalling Conda\nDownload and install Miniconda (a minimal conda installation) from: https://docs.conda.io/en/latest/miniconda.html\nChoose the installer for your operating system and follow the installation instructions.\n\n\n\nMiniconda download page. At the bottom of https://www.anaconda.com/download\n\n\n\n\n\nMiniconda download page. Then choose “Miniconda Installers”\n\n\n\n\nUsing Conda\nCreate a new environment:\nconda create -n my-env python=3.11\nThis creates a new environment named “my-env” with Python 3.11.\nActivate the environment:\nconda activate my-env\nInstall packages:\nconda install pandas\nDeactivate the environment:\nconda deactivate\nList all environments:\nconda env list\nRemove an environment:\nconda remove -n my-env --all\n\n\n\nUsing Jupyter Locally\nOnce you have Python installed locally (with either uv or conda), you can run Jupyter notebooks on your computer.\nWith uv:\nuv add jupyter\nuv run jupyter notebook\nWith conda:\nconda install jupyter\njupyter notebook\nThis opens Jupyter in your web browser, running locally on your computer.\n\n\n\nJupyter running locally\n\n\n\n\n\nJupyter running locally. A new notebook\n\n\n\n\n\nJupyter running locally. Always have a Python kernel selected.\n\n\n\n\n\nJupyter running locally. Some useful UI elements.\n\n\n\n\nWhen to Use Which Tool\nUse uv when:\n\nStarting a new project\nYou want the fastest tool\nYou’re working on modern Python projects\nYou want the latest features\n\nUse conda when:\n\nWorking with data science packages (it handles dependencies well)\nYou need packages that aren’t on PyPI (Python Package Index)\nYou’re following tutorials that use conda\nYou need packages that require complex non-Python dependencies\n\nUse Colab when:\n\nLearning and experimenting\nYou don’t have Python installed locally\nYou need quick access from any computer\nYou’re sharing code with others who may not have Python\n\n\n\n\n\n\n\nTip💡 Tip\n\n\n\nMany people use multiple tools: Colab for quick experiments and learning, uv for new projects, and conda for data science work. You don’t have to choose just one!\n\n\n\n\nExercise 10.1\nThis exercise requires setting up on your local computer (optional):\n\nInstall uv following the instructions for your operating system\nCreate a new project called “python-practice”\nAdd the pandas package to your project\nCreate a file called test.py with this code:\n\nimport pandas as pd\nprint(\"Pandas version:\", pd.__version__)\n\nPandas version: 2.3.3\n\n\nRun the file using uv run python test.py\n\nAlternative exercise with conda:\n\nInstall Miniconda following the instructions for your operating system\nCreate a new environment called “test-env” with Python 3.11\nActivate the environment\nInstall pandas\nRun Python and import pandas to verify it works"
  },
  {
    "objectID": "basic-python.html#conclusion",
    "href": "basic-python.html#conclusion",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "11. Conclusion",
    "text": "11. Conclusion\nCongratulations! You’ve completed this introduction to Python for text and data analysis.\n\nWhat You’ve Learned\nYou can now:\n\n✓ Run Python code in Google Colab\n✓ Read and understand Python code by recognizing variables, data types, functions, and control flow\n✓ Work with strings using methods like .lower(), .split(), .replace(), and string formatting\n✓ Use data structures (lists and dictionaries) to organize data\n✓ Control program flow with if statements and loops\n✓ Create functions to organize reusable code\n✓ Read and write files for data persistence\n✓ Use packages like pandas for data analysis\n✓ Find help using help(), documentation, and online resources\n✓ Understand the difference between cloud (Colab) and local Python environments\n✓ Set up local Python using uv or conda (optional)\n\n\n\nNext Steps\n\nPractice regularly - The best way to learn programming is by doing. Try writing small programs to solve problems you encounter.\nWork with real data - Apply these skills to actual text or datasets that interest you.\nLearn more packages - Explore packages like:\n\nnltk or spaCy for natural language processing\nmatplotlib or seaborn for data visualization\nnumpy for numerical computing\nscikit-learn for machine learning\n\nRead other people’s code - Since you’re learning to read code, study examples from tutorials and open-source projects.\nDebug and experiment - Don’t be afraid of errors! They’re a natural part of programming. See the optional debugging section for help.\nJoin communities - Consider joining Python forums, Reddit’s r/learnpython, or Stack Overflow to ask questions and learn from others.\n\n\n\nResources for Continued Learning\n\nOfficial Python Tutorial: https://docs.python.org/3/tutorial/\nPython for Data Analysis by Wes McKinney (pandas creator)\nReal Python: https://realpython.com - excellent tutorials\nAutomate the Boring Stuff with Python: Free online book at https://automatetheboringstuff.com\n\n\n\nKeep Learning!\nRemember: everyone who programs started as a beginner. The key is persistence and practice. You’ve taken the first important steps, and you have all the foundational knowledge you need to continue learning.\nGood luck with your Python journey!"
  },
  {
    "objectID": "basic-python.html#appendix-a-debugging-and-error-messages-optional",
    "href": "basic-python.html#appendix-a-debugging-and-error-messages-optional",
    "title": "Introduction to Python for Text and Data Analysis",
    "section": "Appendix A: Debugging and Error Messages (Optional)",
    "text": "Appendix A: Debugging and Error Messages (Optional)\nThis section is optional but important. Understanding errors will help you fix problems faster and become more independent in your learning.\n\nWhy Errors Are Helpful\nErrors are not failures - they’re feedback! Python is telling you exactly what went wrong. Learning to read error messages is a crucial skill.\n\n\nCommon Error Types\n\nSyntaxError\nWhat it means: You wrote code that doesn’t follow Python’s grammar rules.\n\nprint(\"Hello\"\n\n\n  Cell In[85], line 1\n    print(\"Hello\"\n                 ^\n_IncompleteInputError: incomplete input\n\n\n\n\nError:\nSyntaxError: unexpected EOF while parsing\nWhat went wrong: Missing closing parenthesis.\nHow to fix: Add the closing ):\n\nprint(\"Hello\")\n\nHello\n\n\nAnother example:\n\nif x &gt; 5\n    print(\"Big\")\n\n\n  Cell In[87], line 1\n    if x &gt; 5\n            ^\nSyntaxError: expected ':'\n\n\n\n\nError:\nSyntaxError: invalid syntax\nWhat went wrong: Missing colon after if statement.\nHow to fix:\n\nif x &gt; 5:\n    print(\"Big\")\n\nBig\n\n\n\n\nNameError\nWhat it means: You’re trying to use a variable or function that doesn’t exist.\n\nprint(message)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[90], line 1\n----&gt; 1 print(message)\n\nNameError: name 'message' is not defined\n\n\n\nError:\nNameError: name 'message' is not defined\nWhat went wrong: The variable message was never created.\nHow to fix: Define the variable first:\n\nmessage = \"Hello\"\nprint(message)\n\nHello\n\n\nCommon cause: Typos in variable names\n\nmy_name = \"Alice\"\nprint(my_nane)  # Typo: 'nane' instead of 'name'\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[92], line 2\n      1 my_name = \"Alice\"\n----&gt; 2 print(my_nane)  # Typo: 'nane' instead of 'name'\n\nNameError: name 'my_nane' is not defined\n\n\n\n\n\nTypeError\nWhat it means: You’re using a value in a way that doesn’t work with its type.\n\nage = 25\nmessage = \"I am \" + age\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[93], line 2\n      1 age = 25\n----&gt; 2 message = \"I am \" + age\n\nTypeError: can only concatenate str (not \"int\") to str\n\n\n\nError:\nTypeError: can only concatenate str (not \"int\") to str\nWhat went wrong: Can’t directly add a string and a number.\nHow to fix: Convert the number to a string:\n\nage = 25\nmessage = \"I am \" + str(age)\n# OR use an f-string:\nmessage = f\"I am {age}\"\n\nAnother example:\n\ntext = \"Python\"\nprint(text[0:3])\ntext[0] = \"p\"  # Trying to change a character\n\nPyt\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[96], line 3\n      1 text = \"Python\"\n      2 print(text[0:3])\n----&gt; 3 text[0] = \"p\"  # Trying to change a character\n\nTypeError: 'str' object does not support item assignment\n\n\n\nError:\nTypeError: 'str' object does not support item assignment\nWhat went wrong: Strings are immutable (can’t be changed).\nHow to fix: Create a new string:\n\ntext = \"Python\"\ntext = \"p\" + text[1:]\n\n\n\nIndexError\nWhat it means: You’re trying to access a list/string position that doesn’t exist.\n\nwords = [\"apple\", \"banana\", \"cherry\"]\nprint(words[5])\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[98], line 2\n      1 words = [\"apple\", \"banana\", \"cherry\"]\n----&gt; 2 print(words[5])\n\nIndexError: list index out of range\n\n\n\nError:\nIndexError: list index out of range\nWhat went wrong: The list only has indices 0, 1, 2 (three items), but we tried to access index 5.\nHow to fix: Use a valid index:\n\nwords = [\"apple\", \"banana\", \"cherry\"]\nprint(words[2])  # The last item\n\ncherry\n\n\n\n\nKeyError\nWhat it means: You’re trying to access a dictionary key that doesn’t exist.\n\nbook = {\"title\": \"1984\", \"author\": \"George Orwell\"}\nprint(book[\"year\"])\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[100], line 2\n      1 book = {\"title\": \"1984\", \"author\": \"George Orwell\"}\n----&gt; 2 print(book[\"year\"])\n\nKeyError: 'year'\n\n\n\nError:\nKeyError: 'year'\nWhat went wrong: The dictionary doesn’t have a “year” key.\nHow to fix: Use an existing key or check if the key exists first:\n\n# Option 1: Use get() method (returns None if key doesn't exist)\nprint(book.get(\"year\"))\n\n# Option 2: Check if key exists\nif \"year\" in book:\n    print(book[\"year\"])\nelse:\n    print(\"Year not found\")\n\nNone\nYear not found\n\n\n\n\nIndentationError\nWhat it means: Your code indentation is inconsistent or incorrect.\n\ndef greet():\nprint(\"Hello\")\n\n\n  Cell In[102], line 2\n    print(\"Hello\")\n    ^\nIndentationError: expected an indented block after function definition on line 1\n\n\n\n\nError:\nIndentationError: expected an indented block\nWhat went wrong: The function body needs to be indented.\nHow to fix:\n\ndef greet():\n    print(\"Hello\")\n\nAnother example (mixing tabs and spaces):\n\nif True:\n    print(\"First line\")\n        print(\"Second line\")  # Too much indentation\n\n\n  Cell In[104], line 3\n    print(\"Second line\")  # Too much indentation\n    ^\nIndentationError: unexpected indent\n\n\n\n\nError:\nIndentationError: unexpected indent\n\n\n\n\n\n\nWarning⚠️ Warning\n\n\n\nAlways use spaces for indentation in Python (4 spaces is the standard). Don’t mix tabs and spaces!\n\n\n\n\nValueError\nWhat it means: You passed a value of the right type but inappropriate value.\n\nnumber = int(\"hello\")\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[105], line 1\n----&gt; 1 number = int(\"hello\")\n\nValueError: invalid literal for int() with base 10: 'hello'\n\n\n\nError:\nValueError: invalid literal for int() with base 10: 'hello'\nWhat went wrong: “hello” can’t be converted to an integer.\nHow to fix: Use a string that represents a number:\n\nnumber = int(\"42\")\n\n\n\nAttributeError\nWhat it means: You’re trying to use a method or attribute that doesn’t exist for that type.\n\nnumber = 42\nresult = number.upper()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[107], line 2\n      1 number = 42\n----&gt; 2 result = number.upper()\n\nAttributeError: 'int' object has no attribute 'upper'\n\n\n\nError:\nAttributeError: 'int' object has no attribute 'upper'\nWhat went wrong: Numbers don’t have an .upper() method (only strings do).\nHow to fix: Use the correct type:\n\ntext = \"hello\"\nresult = text.upper()\n\n\n\n\nHow to Read Error Messages\nPython error messages have a standard format:\nTraceback (most recent call last):\n  File \"script.py\", line 3, in &lt;module&gt;\n    print(message)\nNameError: name 'message' is not defined\nReading from bottom to top:\n\nError type and description (NameError: name 'message' is not defined) - What went wrong\nLine number (line 3) - Where it happened\nCode snippet (print(message)) - The actual problematic line\nTraceback - The sequence of function calls leading to the error\n\n\n\n\n\n\n\nTip💡 Tip\n\n\n\nAlways read error messages from the bottom up. The last line tells you what went wrong, and the lines above show you where.\n\n\n\n\nCommon Beginner Mistakes\n\n1. Forgetting Colons\n\n# Wrong\nif x &gt; 5\n    print(\"Big\")\n\n# Right\nif x &gt; 5:\n    print(\"Big\")\n\n\n  Cell In[109], line 2\n    if x &gt; 5\n            ^\nSyntaxError: expected ':'\n\n\n\n\n\n\n2. Inconsistent Indentation\n\n# Wrong\ndef greet():\nprint(\"Hello\")\n    print(\"Welcome\")\n\n# Right\ndef greet():\n    print(\"Hello\")\n    print(\"Welcome\")\n\n\n  Cell In[110], line 3\n    print(\"Hello\")\n    ^\nIndentationError: expected an indented block after function definition on line 2\n\n\n\n\n\n\n3. Using = Instead of ==\n\n# Wrong (assignment, not comparison)\nif x = 5:\n    print(\"Five\")\n\n# Right\nif x == 5:\n    print(\"Five\")\n\n\n  Cell In[111], line 2\n    if x = 5:\n       ^\nSyntaxError: invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n\n\n\n\n\n\n4. Forgetting Quotes Around Strings\n\n# Wrong\nmessage = Hello\n\n# Right\nmessage = \"Hello\"\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[112], line 2\n      1 # Wrong\n----&gt; 2 message = Hello\n      4 # Right\n      5 message = \"Hello\"\n\nNameError: name 'Hello' is not defined\n\n\n\n\n\n5. Modifying a List While Iterating\n\n# Problematic\nnumbers = [1, 2, 3, 4, 5]\nfor num in numbers:\n    if num % 2 == 0:\n        numbers.remove(num)  # Modifying while iterating!\n\n# Better\nnumbers = [1, 2, 3, 4, 5]\nnumbers = [num for num in numbers if num % 2 != 0]\n\n\n\n6. Not Converting Types\n\n# Wrong\nage = input(\"Enter age: \")\nif age &gt; 18:  # Comparing string to number!\n    print(\"Adult\")\n\n# Right\nage = int(input(\"Enter age: \"))\nif age &gt; 18:\n    print(\"Adult\")\n\n\n\n\nDebugging Strategies\n\n1. Print Debugging\nAdd print statements to see what’s happening:\n\ndef calculate(x, y):\n    print(f\"x = {x}, y = {y}\")  # Debug print\n    result = x * y\n    print(f\"result = {result}\")  # Debug print\n    return result\n\n\n\n2. Check Variable Types\nUse type() to verify types:\n\nx = \"42\"\nprint(type(x))  # &lt;class 'str'&gt;\nx = int(x)\nprint(type(x))  # &lt;class 'int'&gt;\n\n&lt;class 'str'&gt;\n&lt;class 'int'&gt;\n\n\n\n\n3. Test Small Parts\nBreak your code into smaller pieces and test each part:\n# Instead of this all at once:\nresult = data.split()[0].upper().replace(\"X\", \"Y\")\n\n# Test step by step:\nstep1 = data.split()\nprint(step1)\nstep2 = step1[0]\nprint(step2)\nstep3 = step2.upper()\nprint(step3)\nresult = step3.replace(\"X\", \"Y\")\nprint(result)\n\n\n4. Use Try-Except (Advanced)\nHandle errors gracefully:\n\ntry:\n    number = int(input(\"Enter a number: \"))\n    result = 100 / number\n    print(result)\nexcept ValueError:\n    print(\"That's not a valid number!\")\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero!\")\n\n\n\n5. Read Documentation\nUse help() to understand how functions work:\n\nhelp(str.split)\n\n\n\n6. Search the Error\nCopy the error message (without your specific variable names) and search online:\n\nGood search: “python NameError name not defined”\nLess helpful: “my code doesn’t work”\n\n\n\n\nExercise A.1\nEach code snippet below has an error. Copy them into Colab one at a time and:\n\nRun the code and read the error message\nIdentify what type of error it is\nFix the error\n\nCode snippets:\n\n# Error 1\nprint(\"Hello World\"\n\n\n  Cell In[116], line 2\n    print(\"Hello World\"\n                       ^\n_IncompleteInputError: incomplete input\n\n\n\n\n\n# Error 2\nage = 25\nmessage = \"I am \" + age + \" years old\"\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[117], line 3\n      1 # Error 2\n      2 age = 25\n----&gt; 3 message = \"I am \" + age + \" years old\"\n\nTypeError: can only concatenate str (not \"int\") to str\n\n\n\n\n# Error 3\nwords = [\"apple\", \"banana\", \"cherry\"]\nprint(words[3])\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[118], line 3\n      1 # Error 3\n      2 words = [\"apple\", \"banana\", \"cherry\"]\n----&gt; 3 print(words[3])\n\nIndexError: list index out of range\n\n\n\n\n# Error 4\ndef greet():\nprint(\"Hello\")\n\n\n  Cell In[119], line 3\n    print(\"Hello\")\n    ^\nIndentationError: expected an indented block after function definition on line 2\n\n\n\n\n\n# Error 5\nx = 10\nif x &gt; 5\n    print(\"Big number\")\n\n\n  Cell In[120], line 3\n    if x &gt; 5\n            ^\nSyntaxError: expected ':'\n\n\n\n\n\n# Error 6\nbook = {\"title\": \"1984\", \"author\": \"Orwell\"}\nprint(book[\"year\"])\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[121], line 3\n      1 # Error 6\n      2 book = {\"title\": \"1984\", \"author\": \"Orwell\"}\n----&gt; 3 print(book[\"year\"])\n\nKeyError: 'year'\n\n\n\n\n# Error 7\ntext = \"Python\"\nresult = text.find()\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[122], line 3\n      1 # Error 7\n      2 text = \"Python\"\n----&gt; 3 result = text.find()\n\nTypeError: find expected at least 1 argument, got 0\n\n\n\n\n\nFinal Debugging Tips\n\n\n\n\n\n\nTip💡 Tips for Effective Debugging\n\n\n\n\nRead the error message carefully - Python tells you exactly what’s wrong\nCheck line numbers - But remember, the actual error might be on a previous line\nLook for typos - Variable names, function names, syntax\nVerify your assumptions - Use print() to check what values variables actually have\nSearch for help - You’re probably not the first person with this error\nTake breaks - Sometimes stepping away helps you see the problem fresh\nStart simple - Comment out code to isolate the problem\nDon’t panic - Every programmer deals with errors constantly. It’s normal!\n\n\n\n\nEnd of Tutorial\nYou now have a comprehensive reference for Python basics, from running your first code to debugging errors. Return to this document whenever you need to refresh your knowledge!"
  },
  {
    "objectID": "lab_01.html",
    "href": "lab_01.html",
    "title": "Lab 01: Words as data points I",
    "section": "",
    "text": "By the end of this lab, you will understand:\n\nHow to transform unstructured text into structured data\nWhat tokenization is and why it matters\nHow word frequencies reveal patterns in text\nZipf’s Law and its implications for text analysis\nWhat stop words are and when to remove them\nThe basics of text preprocessing\nWhat stylometry is and how function words reveal authorship\nHow to use Python and pandas for basic text analysis"
  },
  {
    "objectID": "lab_01.html#learning-objectives",
    "href": "lab_01.html#learning-objectives",
    "title": "Lab 01: Words as data points I",
    "section": "",
    "text": "By the end of this lab, you will understand:\n\nHow to transform unstructured text into structured data\nWhat tokenization is and why it matters\nHow word frequencies reveal patterns in text\nZipf’s Law and its implications for text analysis\nWhat stop words are and when to remove them\nThe basics of text preprocessing\nWhat stylometry is and how function words reveal authorship\nHow to use Python and pandas for basic text analysis"
  },
  {
    "objectID": "lab_01.html#introduction-why-text-as-data",
    "href": "lab_01.html#introduction-why-text-as-data",
    "title": "Lab 01: Words as data points I",
    "section": "2 Introduction: Why Text as Data?",
    "text": "2 Introduction: Why Text as Data?\nText is everywhere: social media posts, news articles, scientific papers, government documents, customer reviews. But text in its raw form is unstructured - it’s just sequences of characters. To analyze text computationally, we need to transform it into structured data that computers can process mathematically.\nThink of it this way: if you wanted to compare two novels, you could read both and form an impression. But what if you had 1,000 novels? Or 100,000 tweets? This is where computational text analysis becomes essential.\n\n2.1 Our Dataset: State of the Union Addresses\nToday we’ll work with a collection of State of the Union addresses - speeches given by US presidents from the 18th century to 2018. These speeches are:\n\nHistorical: Spanning over 200 years\nPolitical: Reflecting different eras and priorities\nComparable: Same genre, same occasion, different authors\n\nThis makes them perfect for learning text analysis techniques.\nData source: Brian Weinstein’s State of the Union corpus"
  },
  {
    "objectID": "lab_01.html#setup-loading-packages",
    "href": "lab_01.html#setup-loading-packages",
    "title": "Lab 01: Words as data points I",
    "section": "3 Setup: Loading Packages",
    "text": "3 Setup: Loading Packages\nFirst, let’s load the Python packages we’ll need:\n# you will need to run it once after installing spaCy\n!python -m spacy download en_core_web_sm\n\n# Data manipulation\nimport pandas as pd\n\n# Text processing\nimport spacy\nfrom collections import Counter\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# For word clouds\nfrom wordcloud import WordCloud\n\n# Set visualization style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\nprint(\"✓ Packages loaded successfully\")\n\n✓ Packages loaded successfully\n\n\n\n\n\n\n\n\nNote📌 About these packages\n\n\n\n\npandas: Works with tabular data (like spreadsheets)\nspaCy: Processes natural language text\nmatplotlib/seaborn: Create visualizations\nwordcloud: Generate word cloud visualizations"
  },
  {
    "objectID": "lab_01.html#loading-and-exploring-the-data",
    "href": "lab_01.html#loading-and-exploring-the-data",
    "title": "Lab 01: Words as data points I",
    "section": "4 Loading and Exploring the Data",
    "text": "4 Loading and Exploring the Data\nLet’s load our dataset:\n\n# Load the data\nspeeches = pd.read_csv(\"data/transcripts.csv\")\n\n# Display first few rows\nprint(\"Dataset shape:\", speeches.shape)\nprint(\"\\nFirst few rows:\")\nspeeches.head()\n\nDataset shape: (244, 5)\n\nFirst few rows:\n\n\n\n\n\n\n\n\n\ndate\npresident\ntitle\nurl\ntranscript\n\n\n\n\n0\n2018-01-30\nDonald J. Trump\nAddress Before a Joint Session of the Congress...\nhttps://www.cnn.com/2018/01/30/politics/2018-s...\n\\nMr. Speaker, Mr. Vice President, Members of ...\n\n\n1\n2017-02-28\nDonald J. Trump\nAddress Before a Joint Session of the Congress\nhttp://www.presidency.ucsb.edu/ws/index.php?pi...\nThank you very much. Mr. Speaker, Mr. Vice Pre...\n\n\n2\n2016-01-12\nBarack Obama\nAddress Before a Joint Session of the Congress...\nhttp://www.presidency.ucsb.edu/ws/index.php?pi...\nThank you. Mr. Speaker, Mr. Vice President, Me...\n\n\n3\n2015-01-20\nBarack Obama\nAddress Before a Joint Session of the Congress...\nhttp://www.presidency.ucsb.edu/ws/index.php?pi...\nThe President. Mr. Speaker, Mr. Vice President...\n\n\n4\n2014-01-28\nBarack Obama\nAddress Before a Joint Session of the Congress...\nhttp://www.presidency.ucsb.edu/ws/index.php?pi...\nThe President. Mr. Speaker, Mr. Vice President...\n\n\n\n\n\n\n\n\n4.1 Understanding the Data Structure\nOur data is in tabular format - like a spreadsheet with rows and columns:\n\n# Check column names and types\nprint(\"Columns:\")\nprint(speeches.dtypes)\n\n# Basic statistics\nprint(\"\\nNumber of speeches:\", len(speeches))\nprint(\"Number of presidents:\", speeches['president'].nunique())\nprint(\"\\nDate range:\")\nprint(\"  Earliest:\", speeches['date'].min())\nprint(\"  Latest:\", speeches['date'].max())\n\nColumns:\ndate          object\npresident     object\ntitle         object\nurl           object\ntranscript    object\ndtype: object\n\nNumber of speeches: 244\nNumber of presidents: 42\n\nDate range:\n  Earliest: 1790-01-08\n  Latest: 2018-01-30\n\n\nEach row represents one speech. The columns contain:\n\ndate: When the speech was given\npresident: Who gave it\ntitle: The speech’s official title\nurl: Where the transcript came from\ntranscript: The actual text of the speech\n\nLet’s look at a short excerpt:\n\n# Display a snippet of one speech\nsample_text = speeches.loc[0, 'transcript'][:500]  # First 500 characters\nprint(\"Sample from first speech:\")\nprint(sample_text)\nprint(\"...\")\n\nSample from first speech:\n\nMr. Speaker, Mr. Vice President, Members of Congress, the First Lady of the United States, and my fellow Americans:\nLess than 1 year has passed since I first stood at this podium, in this majestic chamber, to speak on behalf of the American People -- and to address their concerns, their hopes, and their dreams.  That night, our new Administration had already taken swift action.  A new tide of optimism was already sweeping across our land.\nEach day since, we have gone forward with a clear vision\n...\n\n\n\n\n\n\n\n\nTip💡 Why This Structure?\n\n\n\nText data commonly comes with metadata (data about data):\n\nAuthor, date, source, title, etc.\nThis metadata helps us compare and analyze texts\nWe keep text and metadata together in a table"
  },
  {
    "objectID": "lab_01.html#from-text-to-tokens---the-bag-of-words-model",
    "href": "lab_01.html#from-text-to-tokens---the-bag-of-words-model",
    "title": "Lab 01: Words as data points I",
    "section": "5 From Text to Tokens - The Bag of Words Model",
    "text": "5 From Text to Tokens - The Bag of Words Model\n\n5.1 The Challenge: Text is Unstructured\nRight now, our transcript column contains long strings of text. How do we measure or compare them? We can’t easily do math on text!\n\n\n5.2 The Solution: Tokenization\nTokenization is the process of splitting text into smaller units called tokens. Usually, tokens are words, but they could also be:\n\nCharacters (letters)\nSentences\nN-grams (sequences of N words)\n\nLet’s see this in action:\n\n# Simple example: split text into words\nexample = \"Python is great for text analysis\"\nwords = example.split()\nprint(\"Original text:\", example)\nprint(\"Tokens (words):\", words)\nprint(\"Number of tokens:\", len(words))\n\nOriginal text: Python is great for text analysis\nTokens (words): ['Python', 'is', 'great', 'for', 'text', 'analysis']\nNumber of tokens: 6\n\n\n\n\n5.3 Tokenizing Our Speeches\nNow let’s tokenize one full speech:\n\n# Get one speech\nspeech_text = speeches.loc[0, 'transcript']\n\n# Simple tokenization (just splitting on spaces)\nsimple_tokens = speech_text.split()\n\nprint(f\"Speech length: {len(speech_text)} characters\")\nprint(f\"Number of tokens: {len(simple_tokens)}\")\nprint(f\"\\nFirst 20 tokens:\")\nprint(simple_tokens[:20])\n\nSpeech length: 30354 characters\nNumber of tokens: 5190\n\nFirst 20 tokens:\n['Mr.', 'Speaker,', 'Mr.', 'Vice', 'President,', 'Members', 'of', 'Congress,', 'the', 'First', 'Lady', 'of', 'the', 'United', 'States,', 'and', 'my', 'fellow', 'Americans:', 'Less']\n\n\n\n\n5.4 The Bag of Words Model\nOnce we have tokens, we can treat text as a “bag of words” - we:\n\nIgnore word order (“cat dog” = “dog cat”)\nCount word frequencies\nRepresent text as numbers\n\nThis might seem like we’re throwing away information (word order matters!), but this simple model is surprisingly powerful for many tasks.\nLet’s create a bag of words for one speech:\n\n# Count word frequencies\nword_counts = Counter(simple_tokens)\n\n# Show most common words\nprint(\"Top 15 most frequent words:\")\nfor word, count in word_counts.most_common(15):\n    print(f\"  {word}: {count}\")\n\nTop 15 most frequent words:\n  the: 215\n  and: 184\n  to: 175\n  of: 121\n  our: 95\n  we: 88\n  a: 79\n  in: 72\n  is: 61\n  are: 48\n  that: 45\n  --: 44\n  have: 40\n  for: 34\n  will: 34\n\n\n\n\n5.5 Two Data Formats: Long vs Wide\nWe can represent tokenized text in two ways:\nLong format: One row per word occurrence\npresident    word\nTrump        the\nTrump        economy\nTrump        is\nTrump        strong\nWide format: One row per document, one column per unique word\npresident    the    economy    is    strong\nTrump        150    12         89    5\nFor now, we’ll work with long format because it’s easier to understand and manipulate."
  },
  {
    "objectID": "lab_01.html#tokenizing-all-speeches",
    "href": "lab_01.html#tokenizing-all-speeches",
    "title": "Lab 01: Words as data points I",
    "section": "6 Tokenizing All Speeches",
    "text": "6 Tokenizing All Speeches\nLet’s tokenize all speeches and create a long-format dataset:\n\n# Initialize empty list to store results\nall_tokens = []\n\n# Process each speech\nfor idx, row in speeches.iterrows():\n    president = row['president']\n    text = row['transcript']\n    \n    # Simple tokenization\n    tokens = text.lower().split()  # Convert to lowercase\n    \n    # Add each token to our list\n    for token in tokens:\n        all_tokens.append({\n            'president': president,\n            'word': token\n        })\n\n# Convert to DataFrame\ntokens_df = pd.DataFrame(all_tokens)\n\nprint(f\"Total number of tokens: {len(tokens_df):,}\")\nprint(f\"\\nFirst few rows:\")\ntokens_df.head(10)\n\nTotal number of tokens: 3,947,946\n\nFirst few rows:\n\n\n\n\n\n\n\n\n\npresident\nword\n\n\n\n\n0\nDonald J. Trump\nmr.\n\n\n1\nDonald J. Trump\nspeaker,\n\n\n2\nDonald J. Trump\nmr.\n\n\n3\nDonald J. Trump\nvice\n\n\n4\nDonald J. Trump\npresident,\n\n\n5\nDonald J. Trump\nmembers\n\n\n6\nDonald J. Trump\nof\n\n\n7\nDonald J. Trump\ncongress,\n\n\n8\nDonald J. Trump\nthe\n\n\n9\nDonald J. Trump\nfirst\n\n\n\n\n\n\n\n\n6.1 How many unique words?\n\nunique_words = tokens_df['word'].nunique()\nprint(f\"Number of unique words: {unique_words:,}\")\n\nNumber of unique words: 68,356\n\n\nThat’s a lot of unique words! But are they all useful?"
  },
  {
    "objectID": "lab_01.html#word-frequencies",
    "href": "lab_01.html#word-frequencies",
    "title": "Lab 01: Words as data points I",
    "section": "7 Word Frequencies",
    "text": "7 Word Frequencies\nNow let’s count how often each word appears:\n\n# Count word frequencies\nword_freq = tokens_df['word'].value_counts().reset_index()\nword_freq.columns = ['word', 'count']\n\nprint(\"Top 20 most frequent words:\")\nprint(word_freq.head(20))\n\nTop 20 most frequent words:\n     word   count\n0     the  326862\n1      of  212531\n2      to  135329\n3     and  133944\n4      in   85772\n5       a   61992\n6    that   47130\n7     for   43079\n8      be   40521\n9     our   38759\n10     is   36752\n11     by   32429\n12     it   29271\n13     we   27276\n14   have   26784\n15   this   26594\n16     as   26510\n17   with   26400\n18  which   26237\n19   will   21913\n\n\n\n7.1 Visualizing Frequency\n\n# Plot top 15 words\ntop_15 = word_freq.head(15)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(data=top_15, y='word', x='count', palette='viridis')\nplt.title('Top 15 Most Frequent Words in State of the Union Addresses', fontsize=14, fontweight='bold')\nplt.xlabel('Frequency (count)', fontsize=12)\nplt.ylabel('Word', fontsize=12)\nplt.tight_layout()\nplt.show()\n\n/tmp/ipykernel_1863094/2171022789.py:5: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=top_15, y='word', x='count', palette='viridis')\n\n\n\n\n\n\n\n\n\n\n\n7.2 What do we notice?\nThe most frequent words are:\n\nArticles: “the”, “a”, “an”\nPrepositions: “of”, “to”, “in”, “for”\nConjunctions: “and”, “but”, “or”\n\nThese are grammatical words that appear in almost any English text. They don’t tell us much about the content of the speeches!\nThis observation leads us to an important concept…"
  },
  {
    "objectID": "lab_01.html#zipfs-law---a-fundamental-pattern",
    "href": "lab_01.html#zipfs-law---a-fundamental-pattern",
    "title": "Lab 01: Words as data points I",
    "section": "8 Zipf’s Law - A Fundamental Pattern",
    "text": "8 Zipf’s Law - A Fundamental Pattern\n\n8.1 What is Zipf’s Law?\nIf you count word frequencies in any large collection of text and rank words by frequency, you’ll observe something remarkable:\nThe most common word appears roughly twice as often as the second most common word, three times as often as the third most common, and so on.\nThis is called Zipf’s Law (named after linguist George Kingsley Zipf).\n\n\n8.2 Why Does This Matter?\nZipf’s Law tells us that:\n\nA few words are very common (“the”, “of”, “and”)\nMost words are very rare (appear only once or twice)\nThis pattern is universal - it appears in all languages and genres\n\nThis has practical implications:\n\nMost words provide little statistical power (they’re too rare)\nA small vocabulary covers most of any text\nWe need strategies to deal with this imbalance\n\n\n\n8.3 Visualizing Zipf’s Law\nLet’s see if our data follows Zipf’s Law:\n\n# Add rank to our frequency table\nword_freq['rank'] = range(1, len(word_freq) + 1)\n\n# Create log-log plot\nplt.figure(figsize=(10, 6))\nplt.loglog(word_freq['rank'], word_freq['count'], 'b.')\nplt.xlabel('Rank (log scale)', fontsize=12)\nplt.ylabel('Frequency (log scale)', fontsize=12)\nplt.title(\"Zipf's Law in State of the Union Addresses\", fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe nearly straight line on a log-log plot confirms Zipf’s Law!\n\n\n8.4 The Long Tail\nLet’s look at how many words appear only once or twice:\n\n# Count words by frequency\nfreq_distribution = word_freq['count'].value_counts().sort_index()\n\nprint(\"Distribution of word frequencies:\")\nprint(f\"  Words appearing once: {freq_distribution.get(1, 0):,}\")\nprint(f\"  Words appearing twice: {freq_distribution.get(2, 0):,}\")\nprint(f\"  Words appearing 3-5 times: {freq_distribution.loc[3:5].sum():,}\")\nprint(f\"  Words appearing 6-10 times: {freq_distribution.loc[6:10].sum():,}\")\nprint(f\"  Words appearing &gt; 100 times: {(word_freq['count'] &gt; 100).sum():,}\")\n\nDistribution of word frequencies:\n  Words appearing once: 9,835\n  Words appearing twice: 27,310\n  Words appearing 3-5 times: 8,157\n  Words appearing 6-10 times: 7,957\n  Words appearing &gt; 100 times: 3,484\n\n\nThis is the “long tail” - many rare words, few common words.\n\n\n\n\n\n\nImportant🎯 Key Insight\n\n\n\nZipf’s Law means that word frequencies are highly skewed. This affects how we:\n\nBuild statistical models\nSelect features for machine learning\nPreprocess text\nInterpret results"
  },
  {
    "objectID": "lab_01.html#stop-words-and-preprocessing",
    "href": "lab_01.html#stop-words-and-preprocessing",
    "title": "Lab 01: Words as data points I",
    "section": "9 Stop Words and Preprocessing",
    "text": "9 Stop Words and Preprocessing\n\n9.1 What are Stop Words?\nStop words are extremely common words that appear frequently in almost any text:\n\nArticles: the, a, an\nPrepositions: in, on, at, to, from\nPronouns: I, you, he, she, it\nConjunctions: and, but, or\nAuxiliary verbs: is, are, was, were\n\n\n\n9.2 Why Remove Stop Words?\nThere are two main perspectives:\nReasons to remove stop words:\n\nContent analysis: They don’t tell us about the topic\nComputational efficiency: Fewer words = faster processing\nSignal-to-noise: They can overwhelm more informative words\n\nReasons to keep stop words:\n\nStylometry: They reveal personal writing style\nSyntax: Needed for parsing sentence structure\nMeaning: Sometimes they matter (“not good” ≠ “good”)\n\n\n\n9.3 Stop Words in Our Data\nLet’s see what Python’s spaCy library considers stop words:\n\n# Load English language model (small version)\ntry:\n    nlp = spacy.load(\"en_core_web_sm\")\nexcept:\n    # If not installed, show installation instructions\n    print(\"Please install spaCy model:\")\n    print(\"!python -m spacy download en_core_web_sm\")\n    # For now, use a simple stopword list\n    from spacy.lang.en.stop_words import STOP_WORDS\n    stopwords_set = STOP_WORDS\nelse:\n    stopwords_set = nlp.Defaults.stop_words\n\nprint(f\"Number of stop words: {len(stopwords_set)}\")\nprint(f\"\\nFirst 30 stop words (alphabetically):\")\nprint(sorted(list(stopwords_set))[:30])\n\nNumber of stop words: 326\n\nFirst 30 stop words (alphabetically):\n[\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amount', 'an', 'and', 'another', 'any']\n\n\n\n\n9.4 Comparing with Our Most Frequent Words\n\n# Check which top words are stop words\ntop_30 = word_freq.head(30).copy()\ntop_30['is_stopword'] = top_30['word'].isin(stopwords_set)\n\nprint(\"Top 30 words with stop word labels:\")\nprint(top_30[['word', 'count', 'is_stopword']])\n\nTop 30 words with stop word labels:\n     word   count  is_stopword\n0     the  326862         True\n1      of  212531         True\n2      to  135329         True\n3     and  133944         True\n4      in   85772         True\n5       a   61992         True\n6    that   47130         True\n7     for   43079         True\n8      be   40521         True\n9     our   38759         True\n10     is   36752         True\n11     by   32429         True\n12     it   29271         True\n13     we   27276         True\n14   have   26784         True\n15   this   26594         True\n16     as   26510         True\n17   with   26400         True\n18  which   26237         True\n19   will   21913         True\n20     on   20689         True\n21      i   20687         True\n22    has   19896         True\n23    are   19619         True\n24   been   19135         True\n25    not   18597         True\n26  their   16784         True\n27   from   16055         True\n28     at   14991         True\n29    all   13608         True\n\n\nNotice how most of the top frequent words are stop words!\n\n\n9.5 Removing Stop Words\n\n# Filter out stop words\ncontent_words = word_freq[~word_freq['word'].isin(stopwords_set)].copy()\n\nprint(f\"Words before removing stop words: {len(word_freq):,}\")\nprint(f\"Words after removing stop words: {len(content_words):,}\")\nprint(f\"\\nTop 30 content words:\")\nprint(content_words.head(30))\n\nWords before removing stop words: 68,356\nWords after removing stop words: 68,058\n\nTop 30 content words:\n            word  count  rank\n35    government  11209    36\n38        united  10158    39\n42        states   9524    43\n44      congress   8597    45\n52           new   7120    53\n56         great   6714    57\n59        public   6520    60\n62        people   6229    63\n66          year   5918    67\n69      american   5575    70\n72          time   5162    73\n76      national   4888    77\n81       country   4367    82\n82       present   4331    83\n88       federal   4093    89\n89         state   4073    90\n90         shall   4006    91\n91           war   3987    92\n99          work   3501   100\n100          act   3485   101\n102      foreign   3369   103\n103        years   3319   104\n105        power   3206   106\n106      general   3191   107\n107          law   3171   108\n108        world   3168   109\n112       system   2931   113\n116    necessary   2878   117\n117     increase   2818   118\n118  legislation   2790   119\n\n\nNow we see more content-bearing words: government, states, congress, country, people, etc.\n\n\n9.6 Visualizing Content Words\n\n# Create word cloud from content words\nwordcloud_dict = dict(zip(content_words['word'].head(100), \n                          content_words['count'].head(100)))\n\nwordcloud = WordCloud(width=800, height=400, \n                      background_color='white',\n                      colormap='viridis').generate_from_frequencies(wordcloud_dict)\n\nplt.figure(figsize=(14, 7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Most Frequent Content Words in State of the Union Addresses', \n          fontsize=16, fontweight='bold', pad=20)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip💡 Preprocessing is a Choice\n\n\n\nWhether to remove stop words depends on your research question:\n\nTopic modeling: Usually remove\nSentiment analysis: Keep (negations matter!)\nStylometry: Keep (they reveal style!)\nDocument classification: Test both"
  },
  {
    "objectID": "lab_01.html#introduction-to-stylometry",
    "href": "lab_01.html#introduction-to-stylometry",
    "title": "Lab 01: Words as data points I",
    "section": "10 Introduction to Stylometry",
    "text": "10 Introduction to Stylometry\n\n10.1 What is Stylometry?\nStylometry is the statistical analysis of writing style. It’s used to:\n\nAttribute authorship: Who wrote this anonymous text?\nDetect plagiarism: Did someone copy another’s style?\nStudy literary history: How did an author’s style evolve?\nForensics: Analyze threatening letters or disputed documents\n\n\n\n10.2 The Surprising Power of Function Words\nYou might think content words (nouns, verbs) reveal writing style. But actually, function words (the stop words we just removed!) are more revealing because:\n\nUnconscious use: Authors don’t think about them\nHigh frequency: Provide strong statistical signal\n\nStable patterns: Less affected by topic\nIndividual variation: People use them differently\n\n\n\n10.3 A Simple Example\nDifferent presidents might discuss the same topics but use different grammatical structures:\n\n“We must ensure…” vs “I believe we must…”\n“This is important” vs “This has been important”\n“The people” vs “Our people”\n\nThese tiny differences add up to distinctive “stylistic fingerprints.”\n\n\n10.4 Analyzing Presidential Style\nLet’s look at how often different presidents use personal pronouns:\n\n# Define personal pronouns\npersonal_pronouns = ['i', 'me', 'my', 'we', 'us', 'our', 'you', 'your']\n\n# Filter for pronouns only\npronoun_df = tokens_df[tokens_df['word'].isin(personal_pronouns)].copy()\n\n# Count by president\npronoun_by_president = (pronoun_df.groupby(['president', 'word'])\n                        .size()\n                        .reset_index(name='count'))\n\n# Calculate total words per president for normalization\nwords_per_president = tokens_df.groupby('president').size()\n\n# Normalize (calculate rate per 1000 words)\npronoun_by_president = pronoun_by_president.merge(\n    words_per_president.reset_index(name='total_words'),\n    on='president'\n)\npronoun_by_president['rate_per_1000'] = (\n    (pronoun_by_president['count'] / pronoun_by_president['total_words']) * 1000\n)\n\n# Show some examples\nprint(\"Sample of pronoun usage rates (per 1000 words):\")\nprint(pronoun_by_president[pronoun_by_president['president'].isin(\n    ['Donald J. Trump', 'Barack Obama', 'George W. Bush']\n)].head(15))\n\nSample of pronoun usage rates (per 1000 words):\n          president  word  count  total_words  rate_per_1000\n24     Barack Obama     i    857       106566       8.041965\n25     Barack Obama    me    120       106566       1.126063\n26     Barack Obama    my    180       106566       1.689094\n27     Barack Obama   our   1811       106566      16.994163\n28     Barack Obama    us    287       106566       2.693167\n29     Barack Obama    we   1927       106566      18.082691\n30     Barack Obama   you    349       106566       3.274966\n31     Barack Obama  your    113       106566       1.060376\n56  Donald J. Trump     i    105        15058       6.973038\n57  Donald J. Trump    me      5        15058       0.332049\n58  Donald J. Trump    my     37        15058       2.457166\n59  Donald J. Trump   our    331        15058      21.981671\n60  Donald J. Trump    us     55        15058       3.652543\n61  Donald J. Trump    we    319        15058      21.184752\n62  Donald J. Trump   you     35        15058       2.324346\n\n\n\n\n10.5 Visualizing Style Differences\nLet’s compare how different presidents use “I” vs “we”:\n\n# Focus on 'I' and 'we'\ni_we_df = pronoun_by_president[pronoun_by_president['word'].isin(['i', 'we'])].copy()\n\n# Pivot for easier plotting\ni_we_pivot = i_we_df.pivot(index='president', columns='word', values='rate_per_1000').fillna(0)\n\n# Get presidents with most speeches for clearer visualization\ntop_presidents = (tokens_df['president']\n                  .value_counts()\n                  .head(10)\n                  .index)\n\ni_we_plot = i_we_pivot.loc[i_we_pivot.index.isin(top_presidents)]\n\n# Create scatter plot\nplt.figure(figsize=(10, 8))\nplt.scatter(i_we_plot['i'], i_we_plot['we'], s=100, alpha=0.6)\n\n# Label points\nfor idx, row in i_we_plot.iterrows():\n    # Shorten long names\n    name = idx.split()[-1]  # Just last name\n    plt.annotate(name, (row['i'], row['we']), \n                xytext=(5, 5), textcoords='offset points', fontsize=9)\n\nplt.xlabel('Rate of \"I\" (per 1000 words)', fontsize=12)\nplt.ylabel('Rate of \"we\" (per 1000 words)', fontsize=12)\nplt.title('Presidential Pronoun Usage: \"I\" vs \"We\"', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote📌 What This Tells Us\n\n\n\nPresidents who use “I” more often might be:\n\nSpeaking in a more personal style\nTaking individual responsibility\nModern era (contemporary style)\n\nPresidents who use “we” more might be:\n\nEmphasizing collective action\nSpeaking for the nation\nEarlier era (formal style)\n\nThese patterns can distinguish authors even when topics overlap!\n\n\n\n\n10.6 The Role of PCA (Principal Component Analysis)\nIn a full stylometric analysis, we would:\n\nCount many function words (not just pronouns)\nHave dozens or hundreds of features\nNeed to reduce complexity to visualize patterns\n\nPCA (Principal Component Analysis) helps by:\n\nFinding the main “directions” of variation\nReducing many features to 2-3 dimensions\nAllowing us to plot and compare texts\n\nWhat PCA gives us (practically):\n\nA 2D plot where similar authors cluster together\nAbility to spot outliers or disputed authorship\nQuantified measure of stylistic distance\n\nWe won’t dive into the mathematics here, but know that PCA is a standard tool for reducing complex data to interpretable patterns."
  },
  {
    "objectID": "lab_01.html#summary-and-key-takeaways",
    "href": "lab_01.html#summary-and-key-takeaways",
    "title": "Lab 01: Words as data points I",
    "section": "11 Summary and Key Takeaways",
    "text": "11 Summary and Key Takeaways\n\n11.1 What We Learned\nToday we covered the fundamental workflow of computational text analysis:\n\nLoad text data → Working with structured formats (CSV, DataFrame)\nTokenize → Split text into words (or other units)\nCount frequencies → Transform text into numbers\nExplore patterns → Zipf’s Law, frequency distributions\nPreprocess → Remove stop words (when appropriate)\nAnalyze style → Use function words for stylometry\n\n\n\n11.2 Key Concepts\n\nTokenization\n\nSplitting text into units (words, characters, sentences)\n\nBag of Words\n\nTreating text as unordered collection of words\n\nZipf’s Law\n\nWord frequency follows power law distribution (few common, many rare)\n\nStop Words\n\nHigh-frequency grammatical words with little content\n\nStylometry\n\nStatistical analysis of writing style using function words\n\nPreprocessing\n\nTransforming raw text for analysis (lowercase, remove punctuation, etc.)\n\n\n\n\n11.3 Tools in Your Toolkit\n\n\n\nTask\nPython Tool\n\n\n\n\nLoad data\npandas.read_csv()\n\n\nTokenize\nstr.split() or spaCy\n\n\nCount frequencies\nCounter() or value_counts()\n\n\nRemove stop words\nspaCy stop word list\n\n\nVisualize\nmatplotlib, seaborn, wordcloud\n\n\n\n\n\n11.4 Next Steps\nIn future labs, we’ll explore:\n\nMore sophisticated tokenization (handling punctuation, contractions)\nN-grams (sequences of words)\nTF-IDF weighting (smarter than raw counts)"
  },
  {
    "objectID": "lab_01.html#exercises",
    "href": "lab_01.html#exercises",
    "title": "Lab 01: Words as data points I",
    "section": "12 Exercises",
    "text": "12 Exercises\n\n12.1 Exercise 1: Basic Frequency Analysis\nPick any president from the dataset and:\n\nExtract all their speeches\nTokenize and count word frequencies\nCreate a bar plot of their top 20 words\nCreate a word cloud\n\n\n\n12.2 Exercise 2: Stop Word Impact\nCompare word frequencies with and without stop words:\n\nCalculate top 50 words with stop words\nCalculate top 50 words without stop words\nHow many overlap? What changes?\n\n\n\n12.3 Exercise 3: Pronoun Style\nChoose three presidents and compare their use of:\n\nFirst person singular (“I”, “me”, “my”)\nFirst person plural (“we”, “us”, “our”)\n\nCreate a visualization showing the differences.\n\n\n12.4 Exercise 4: Historical Change (Advanced)\nCompare speeches before and after 1950:\n\nSplit the dataset into two time periods\nCalculate word frequencies for each period\nIdentify words that became more/less common\nCreate a Zipf’s Law plot for each period"
  },
  {
    "objectID": "lab_01.html#references-and-further-reading",
    "href": "lab_01.html#references-and-further-reading",
    "title": "Lab 01: Words as data points I",
    "section": "13 References and Further Reading",
    "text": "13 References and Further Reading\n\n13.1 Academic Papers\n\nPiantadosi, S. T. (2014). Zipf’s word frequency law in natural language: A critical review and future directions. Psychonomic Bulletin & Review, 21(5), 1112–1130. https://doi.org/10.3758/s13423-014-0585-6\nJurafsky, D., & Martin, J. H. (2023). Speech and Language Processing (3rd ed., draft) https://web.stanford.edu/~jurafsky/slp3/\n\n\n\n13.2 Tutorials and Books\n\nSilge, J., & Robinson, D. (2017). Text Mining with R (applicable concepts) https://www.tidytextmining.com/\nBird, S., Klein, E., & Loper, E. (2009). Natural Language Processing with Python. https://www.nltk.org/book/ (a bit dated, but good)\n\n\n\n13.3 Documentation\n\npandas documentation\nspaCy documentation\nseaborn documentation\n\n\nEnd of Lab 01"
  }
]